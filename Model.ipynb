{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter nbconvert --to script Model.ipynb\n",
    "\n",
    "\n",
    "#TODO: Añadir las métricas que voy a usar finales\n",
    "#TODO: Platt scaling\n",
    "#TODO: Hyperband\n",
    "#TODO: Corregir los logs\n",
    "#TODO: Conformal prediction\n",
    "\n",
    "# Optional with Shapely or sth like that?\n",
    "#TODO: Estudiar relevancia de las features para cada modelo y cada grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any existing log files\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "# Reset logger to avoid any issues with permissions\n",
    "logging.shutdown()\n",
    "# Remove loggers\n",
    "for log_file in glob.glob(\"*.log\"):\n",
    "    os.remove(log_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star-Galaxy Classification using ALHAMBRA Photometry\n",
    "\n",
    "This notebook implements and evaluates several machine learning models for classifying astronomical objects as stars or galaxies based on multi-band photometric data from the ALHAMBRA survey, using labels derived from higher-resolution COSMOS2020 data.\n",
    "\n",
    "**Target Variable:** `acs_mu_class` (from COSMOS2020)\n",
    " - Which is 1 for Galaxy and 2 for Star. We will remap this to 0 (Galaxy, majority class) and 1 (Star, minority class).\n",
    "\n",
    "**Features:** Selected columns from the ALHAMBRA survey data.\n",
    "\n",
    "**Models:**\n",
    "1. Support Vector Machine (SVM)\n",
    "2. Decision Tree (CART)\n",
    "3. Random Forest\n",
    "4. XGBoost\n",
    "5. LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import joblib # For saving/loading models efficiently\n",
    "import glob\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV,  ParameterSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import loguniform # For hyperparameter distributions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support, roc_auc_score,\n",
    "    brier_score_loss, precision_recall_curve, auc\n",
    ")   \n",
    "import seaborn as sns # For confusion matrix heatmap\n",
    "\n",
    "# Boosting models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.shutdown()\n",
    "logging.basicConfig(\n",
    "    filename=f'models_{datetime.now().strftime(\"%d_%H-%M-%S\")}.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True\n",
    ")\n",
    "# Prevent logs from being printed to console\n",
    "logging.getLogger().handlers = [h for h in logging.getLogger().handlers if isinstance(h, logging.FileHandler)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Dataset & Feature Selection\n",
    "\n",
    "**Interesting Feature Combinations for Modeling:**\n",
    " \n",
    " The feature groups are defined as follows:\n",
    " - Group 1: Morphology features and their uncertainties\n",
    " - Group 2: Photometry magnitudes\n",
    " - Group 3: Photometry magnitude and errors\n",
    " - Group 4: Redshift features and their uncertainties\n",
    " - Group 5: Combination of photometry magnitude errors and morphology features (including uncertainties)\n",
    " - Group 6: Combination of photometry magnitude errors, morphology features (including uncertainties), and redshift features (including uncertainties)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the df\n",
    "df = pd.read_csv('data/match_alhambra_cosmos2020_ACS_class_0.8arcsec.csv')\n",
    "logging.info(f\"DataFrame created with shape: {df.shape}\")\n",
    "# Map ACS classification: 1 (Galaxy, Majority) -> 0, 2 (Star, minority) -> 1, 3 (Fake) -> drop\n",
    "logging.info(\"Original class counts:\")\n",
    "logging.info(df['acs_mu_class'].value_counts().to_string())\n",
    "\n",
    "# Drop fake detections (class 3)\n",
    "# Drop fake detections\n",
    "n_fakes = (df['acs_mu_class'] == 3).sum()\n",
    "logging.info(f\"Number of fake detections (class 3): {n_fakes}\")\n",
    "df = df[df['acs_mu_class'] != 3]\n",
    "\n",
    "# Map classifications\n",
    "df['acs_mu_class'] = df['acs_mu_class'].map({1: 0, 2: 1})\n",
    "\n",
    "logging.info(\"After dropping fakes and mapping classes (0: Galaxy, 1: Star):\")\n",
    "logging.info(df['acs_mu_class'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features\n",
    "\n",
    "# --- Define feature categories based on ALHAMBRA data using exact names ---\n",
    "\n",
    "# 1. ALHAMBRA Morphology Features (SExtractor-based)\n",
    "morphology_features = [\n",
    "    'area', 'fwhm', 'stell', 'ell', 'a', 'b', 'theta', 'rk', 'rf'\n",
    "]\n",
    "\n",
    "morphology_err = [\n",
    "    's2n'\n",
    "]\n",
    "\n",
    "morphology_mags_errors = morphology_features + morphology_err\n",
    "\n",
    "# 2. ALHAMBRA Photometry Magnitudes (Optical + NIR + Synthetic)\n",
    "OPTICAL_MAG_COLS = [\n",
    "    'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W',\n",
    "    'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W',\n",
    "    'F799W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W'\n",
    "]\n",
    "photometry_magnitudes = (\n",
    "    OPTICAL_MAG_COLS +\n",
    "    ['J', 'H', 'KS', 'F814W']\n",
    ")\n",
    "\n",
    "# 3. ALHAMBRA Photometry Uncertainties\n",
    "OPTICAL_ERR_COLS = [\n",
    "    'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W',\n",
    "    'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W',\n",
    "    'dF799W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W'\n",
    "]\n",
    "photometry_uncertainties = (\n",
    "    OPTICAL_ERR_COLS +\n",
    "    ['dJ', 'dH', 'dKS', 'dF814W']\n",
    ")\n",
    "\n",
    "photometry_mags_errors = photometry_magnitudes + photometry_uncertainties\n",
    "\n",
    "# 4. ALHAMBRA Photometric Redshift & Derived Features (BPZ-based)\n",
    "redshift_features = [\n",
    "    'zb_1', 'zb_Min_1', 'zb_Max_1', 'Tb_1',\n",
    "    'z_ml', 't_ml',\n",
    "    'Stell_Mass_1', 'M_Abs_1', 'MagPrior'\n",
    "]\n",
    "\n",
    "redshift_uncertainties = [\n",
    "    'Odds_1', 'Chi2'\n",
    "]\n",
    "\n",
    "\n",
    "redshift_mags_errors = redshift_features + redshift_uncertainties\n",
    "\n",
    "# 5. ALHAMBRA Quality/Auxiliary Features (per-band quality etc.)\n",
    "OPTICAL_IRMS_COLS = [\n",
    "    'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W',\n",
    "    'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W',\n",
    "    'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W',\n",
    "    'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W'\n",
    "]\n",
    "quality_aux_features = (\n",
    "    ['nfobs'] +\n",
    "    OPTICAL_IRMS_COLS +\n",
    "    ['irms_J', 'irms_H', 'irms_KS', 'irms_F814W']\n",
    ")\n",
    "\n",
    "# --- Define lists of features NOT used for modeling ---\n",
    "\n",
    "non_modeling_identifiers = ['ID_1', 'id_2'] # ALHAMBRA ID, COSMOS ID\n",
    "\n",
    "non_modeling_astrometry = [\n",
    "    'RA_1', 'Dec_1', 'x', 'y', # ALHAMBRA Astrometry\n",
    "    'ra_2', 'dec_2',          # COSMOS Astrometry\n",
    "    'Separation'              # Matching Quality\n",
    "]\n",
    "\n",
    "non_modeling_flags = [\n",
    "    'photoflag', 'xray', 'PercW', 'Satur_Flag', # ALHAMBRA Object/Photometry Flags\n",
    "    'irms_OPT_Flag', 'irms_NIR_Flag'           # ALHAMBRA Overall Quality Flags\n",
    "]\n",
    "\n",
    "alhambra_prediction = ['Stellar_Flag'] # ALHAMBRA's own classification\n",
    "\n",
    "non_modeling_aperture_mags = [ # Specific aperture mags, usually use total mags\n",
    "    'F814W_3arcs', 'dF814W_3arcs', 'F814W_3arcs_corr'\n",
    "]\n",
    "\n",
    "non_modeling_cosmos_features = [ # Measurements/flags derived from COSMOS data (HST, HSC, VISTA...)\n",
    "    'model_flag',\n",
    "    'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista',\n",
    "    'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid',\n",
    "    'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid',\n",
    "    'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid',\n",
    "    'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid',\n",
    "    'acs_f814w_mag', 'acs_f814w_magerr',\n",
    "    'acs_fwhm_world', 'acs_mu_max',\n",
    "    'solution_model' # This is categorical, but still COSMOS-derived info\n",
    "]\n",
    "\n",
    "target_variable = ['acs_mu_class'] # The COSMOS classification label to predict\n",
    "\n",
    "##########################################################################################\n",
    "#! --- Consolidate into the main dictionary for easy access ---\n",
    "##########################################################################################\n",
    "\n",
    "feature_sets = {\n",
    "        # --- Potential Input Feature Sets ---\n",
    "        'morphology_only': morphology_mags_errors,\n",
    "        'photometry_magnitudes_only': photometry_magnitudes,\n",
    "        'photometry_mags_errors': photometry_mags_errors,\n",
    "        'photometry_plus_morphology': photometry_mags_errors + morphology_mags_errors,\n",
    "        'photometry_no_redshift': photometry_mags_errors + morphology_mags_errors + quality_aux_features,\n",
    "        'redshift_only': redshift_mags_errors,\n",
    "        'full_alhambra_all': (morphology_mags_errors +\n",
    "                            photometry_mags_errors +\n",
    "                            redshift_mags_errors + \n",
    "                            quality_aux_features),\n",
    "\n",
    "        # --- Excluded Feature Sets ---\n",
    "        'non_modeling_identifiers': non_modeling_identifiers,\n",
    "        'non_modeling_astrometry': non_modeling_astrometry,\n",
    "        'non_modeling_flags': non_modeling_flags,\n",
    "        'non_modeling_aperture_mags': non_modeling_aperture_mags,\n",
    "        'non_modeling_cosmos_features': non_modeling_cosmos_features,\n",
    "        'alhambra_prediction': alhambra_prediction,\n",
    "        'target_variable': target_variable\n",
    "    }\n",
    "\n",
    "#! This is excluding the quality aux.\n",
    "# Include target_variable in each group by appending it to the feature list\n",
    "groups = {\n",
    "        'group_1': feature_sets.get('morphology_only', []) + feature_sets.get('target_variable', []),\n",
    "        'group_2': feature_sets.get('photometry_magnitudes_only', []) + feature_sets.get('target_variable', []),\n",
    "        'group_3': feature_sets.get('photometry_mags_errors', []) + feature_sets.get('target_variable', []),\n",
    "        'group_4': feature_sets.get('redshift_only', []) + feature_sets.get('target_variable', []),\n",
    "        'group_5': feature_sets.get('photometry_plus_morphology', []) + feature_sets.get('target_variable', []),\n",
    "        'group_6': (feature_sets.get('photometry_mags_errors', []) +\n",
    "                   feature_sets.get('morphology_only', []) +\n",
    "                   feature_sets.get('redshift_only', []) +\n",
    "                   feature_sets.get('target_variable', [])),\n",
    "        'group_7': feature_sets.get('full_alhambra_all', []) + feature_sets.get('target_variable', [])\n",
    "    }\n",
    "\n",
    "# --- Function to get a specific feature set (Unchanged from before) ---\n",
    "\n",
    "def get_feature_set(df, set_name, groups = groups):\n",
    "    \"\"\"\n",
    "    Selects columns from a DataFrame based on a predefined feature set name,\n",
    "    including six specific groups defined by combinations of morphology,\n",
    "    photometry magnitudes, uncertainties, and redshift features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        set_name (str): The name of the desired feature set group:\n",
    "                        'group_1' to 'group_6' as defined below.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing only the columns\n",
    "                      belonging to the specified feature set group.\n",
    "                      Returns an empty DataFrame if no columns are found.\n",
    "    \"\"\"\n",
    "\n",
    "    if set_name not in groups:\n",
    "        raise ValueError(f\"Feature set group '{set_name}' not defined. \"\n",
    "                         f\"Available groups: {list(groups.keys())}\")\n",
    "\n",
    "    required_cols_in_set = groups[set_name]\n",
    "\n",
    "    # Find which of these columns actually exist in the DataFrame\n",
    "    available_cols = [col for col in required_cols_in_set if col in df.columns]\n",
    "\n",
    "    # Warn if some columns from the set definition are missing\n",
    "    missing_cols = [col for col in required_cols_in_set if col not in available_cols]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: The following columns defined for feature set group '{set_name}'\"\n",
    "              f\" were not found in the DataFrame and will be excluded: {missing_cols}\")\n",
    "\n",
    "    if not available_cols:\n",
    "        print(f\"Warning: No columns for feature set group '{set_name}' found in the DataFrame.\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "\n",
    "    print(f\"Selecting feature set group '{set_name}' with {len(available_cols)} columns.\")\n",
    "    return df[available_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All df columns are included in feature_sets.\n",
      "\n",
      "=== group_1 ===\n",
      "Selecting feature set group 'group_1' with 11 columns.\n",
      "\n",
      "Features present (11 columns):\n",
      "['a', 'acs_mu_class', 'area', 'b', 'ell', 'fwhm', 'rf', 'rk', 's2n', 'stell', 'theta']\n",
      "\n",
      "Features missing (125 columns):\n",
      "['Chi2', 'Dec_1', 'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F814W_3arcs', 'F814W_3arcs_corr', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'ID_1', 'J', 'KS', 'M_Abs_1', 'MagPrior', 'Odds_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stell_Mass_1', 'Stellar_Flag', 'Tb_1', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF814W_3arcs', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'dec_2', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'solution_model', 't_ml', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "=== group_2 ===\n",
      "Selecting feature set group 'group_2' with 25 columns.\n",
      "\n",
      "Features present (25 columns):\n",
      "['F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'acs_mu_class']\n",
      "\n",
      "Features missing (111 columns):\n",
      "['Chi2', 'Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'M_Abs_1', 'MagPrior', 'Odds_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stell_Mass_1', 'Stellar_Flag', 'Tb_1', 'a', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF814W_3arcs', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'dec_2', 'ell', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'fwhm', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'rf', 'rk', 's2n', 'solution_model', 'stell', 't_ml', 'theta', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "=== group_3 ===\n",
      "Selecting feature set group 'group_3' with 49 columns.\n",
      "\n",
      "Features present (49 columns):\n",
      "['F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'acs_mu_class', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS']\n",
      "\n",
      "Features missing (87 columns):\n",
      "['Chi2', 'Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'M_Abs_1', 'MagPrior', 'Odds_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stell_Mass_1', 'Stellar_Flag', 'Tb_1', 'a', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'area', 'b', 'dF814W_3arcs', 'dec_2', 'ell', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'fwhm', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'rf', 'rk', 's2n', 'solution_model', 'stell', 't_ml', 'theta', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "=== group_4 ===\n",
      "Selecting feature set group 'group_4' with 12 columns.\n",
      "\n",
      "Features present (12 columns):\n",
      "['Chi2', 'M_Abs_1', 'MagPrior', 'Odds_1', 'Stell_Mass_1', 'Tb_1', 'acs_mu_class', 't_ml', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "Features missing (124 columns):\n",
      "['Dec_1', 'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F814W_3arcs', 'F814W_3arcs_corr', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'ID_1', 'J', 'KS', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stellar_Flag', 'a', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF814W_3arcs', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'dec_2', 'ell', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'fwhm', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'rf', 'rk', 's2n', 'solution_model', 'stell', 'theta', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y']\n",
      "\n",
      "=== group_5 ===\n",
      "Selecting feature set group 'group_5' with 59 columns.\n",
      "\n",
      "Features present (59 columns):\n",
      "['F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'a', 'acs_mu_class', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'ell', 'fwhm', 'rf', 'rk', 's2n', 'stell', 'theta']\n",
      "\n",
      "Features missing (77 columns):\n",
      "['Chi2', 'Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'M_Abs_1', 'MagPrior', 'Odds_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stell_Mass_1', 'Stellar_Flag', 'Tb_1', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'dF814W_3arcs', 'dec_2', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'solution_model', 't_ml', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "=== group_6 ===\n",
      "Selecting feature set group 'group_6' with 70 columns.\n",
      "\n",
      "Features present (70 columns):\n",
      "['Chi2', 'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'M_Abs_1', 'MagPrior', 'Odds_1', 'Stell_Mass_1', 'Tb_1', 'a', 'acs_mu_class', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'ell', 'fwhm', 'rf', 'rk', 's2n', 'stell', 't_ml', 'theta', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "Features missing (66 columns):\n",
      "['Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stellar_Flag', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'dF814W_3arcs', 'dec_2', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'solution_model', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y']\n",
      "\n",
      "=== group_7 ===\n",
      "Selecting feature set group 'group_7' with 95 columns.\n",
      "\n",
      "Features present (95 columns):\n",
      "['Chi2', 'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'M_Abs_1', 'MagPrior', 'Odds_1', 'Stell_Mass_1', 'Tb_1', 'a', 'acs_mu_class', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'ell', 'fwhm', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'nfobs', 'rf', 'rk', 's2n', 'stell', 't_ml', 'theta', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "Features missing (41 columns):\n",
      "['Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stellar_Flag', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'dF814W_3arcs', 'dec_2', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'photoflag', 'ra_2', 'solution_model', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y']\n"
     ]
    }
   ],
   "source": [
    "# Quality check to see which cols are excluded and contained in each group\n",
    "all_feature_cols = set()\n",
    "for cols in feature_sets.values():\n",
    "    all_feature_cols.update(cols)\n",
    "\n",
    "df_cols_set = set(df.columns)\n",
    "not_in_feature_sets = df_cols_set - all_feature_cols\n",
    "\n",
    "if not_in_feature_sets:\n",
    "    print(f\"Columns in df not included in any feature_sets: {sorted(not_in_feature_sets)}\")\n",
    "else:\n",
    "    print(\"All df columns are included in feature_sets.\")\n",
    "\n",
    "\n",
    "# Check which columns are in each feature group\n",
    "for group_name in ['group_1', 'group_2', 'group_3', 'group_4', 'group_5', 'group_6', 'group_7']:\n",
    "    print(f\"\\n=== {group_name} ===\")\n",
    "    \n",
    "    # Get the feature set definition\n",
    "    feature_set = groups[group_name]\n",
    "    \n",
    "    # Get the actual columns that exist in the data\n",
    "    group_df = get_feature_set(df, group_name)\n",
    "    \n",
    "\n",
    "    available_cols = list(group_df.columns)\n",
    "    \n",
    "    # Find columns that are defined but not in the data\n",
    "    missing_cols = [col for col in list(df.columns) if col not in feature_set]\n",
    "    \n",
    "    print(f\"\\nFeatures present ({len(available_cols)} columns):\")\n",
    "    print(list(sorted(available_cols)))\n",
    "    \n",
    "    print(f\"\\nFeatures missing ({len(missing_cols)} columns):\")\n",
    "    print(list(sorted(missing_cols)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting parameters\n",
    "TEST_SIZE = 0.20 # Test set proportion\n",
    "VAL_SIZE = 0.00 # Validation set proportion\n",
    "CAL_SIZE = 0.10 # Calibration set proportion\n",
    "# Train size will be 1 - (TEST_SIZE + VAL_SIZE + CAL_SIZE) = 0.70\n",
    "\n",
    "TARGET_COLUMN = feature_sets.get('target_variable', [])[0]\n",
    "RANDOM_SEED = 33 # For reproducibility\n",
    "\n",
    "# Model saving directory\n",
    "MODEL_DIR = \"trained_models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Data splitting strategy ('stratified' or 'random')\n",
    "SPLIT_STRATEGY = 'stratified' # Recommended for imbalanced datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting feature set group 'group_7' with 95 columns.\n"
     ]
    }
   ],
   "source": [
    "# --- Data Cleaning ---\n",
    "logging.info(f\"Original dataset size: {df.shape}\")\n",
    "\n",
    "# Choose the feature group to use (e.g., 'group_1', 'group_2', etc.)\n",
    "FEATURE_GROUP = 'group_7'  # Change this to select a different group\n",
    "\n",
    "# Get the feature columns for the selected group using get_feature_set\n",
    "df_clean = get_feature_set(df, FEATURE_GROUP).dropna().copy()\n",
    "logging.info(f\"Dataset size after dropping NaNs: {df_clean.shape}\")\n",
    "\n",
    "# Ensure TARGET_COLUMN is defined correctly\n",
    "if TARGET_COLUMN not in df_clean.columns:\n",
    "    raise KeyError(f\"Target column '{TARGET_COLUMN}' not found in the cleaned DataFrame columns: {df_clean.columns.tolist()}\")\n",
    "\n",
    "# Log value counts for target\n",
    "logging.info(f\"Value counts for target:\\n1 (Star): {(df_clean[TARGET_COLUMN] == 1).sum()}\\n0 (Galaxy): {(df_clean[TARGET_COLUMN] == 0).sum()}\")\n",
    "\n",
    "# Separate features (X) and target (y) for the cleaned DataFrame\n",
    "X = df_clean.drop(columns=[TARGET_COLUMN])\n",
    "y = df_clean[TARGET_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Splitting ---\n",
    "import numpy as np # Ensure numpy is imported\n",
    "from sklearn.model_selection import train_test_split # Ensure train_test_split is imported\n",
    "\n",
    "logging.info(f\"Splitting data using '{SPLIT_STRATEGY}' strategy...\")\n",
    "\n",
    "# --- Validate Proportions ---\n",
    "if not (0 <= TEST_SIZE <= 1 and 0 <= VAL_SIZE <= 1 and 0 <= CAL_SIZE <= 1):\n",
    "     raise ValueError(\"Split proportions (TEST_SIZE, VAL_SIZE, CAL_SIZE) must be between 0 and 1.\")\n",
    "\n",
    "TRAIN_SIZE = 1.0 - TEST_SIZE - VAL_SIZE - CAL_SIZE\n",
    "if not (0 <= TRAIN_SIZE <= 1):\n",
    "     raise ValueError(f\"Calculated TRAIN_SIZE ({TRAIN_SIZE:.3f}) is invalid. Sum of TEST_SIZE, VAL_SIZE, and CAL_SIZE must be between 0 and 1.\")\n",
    "\n",
    "if not np.isclose(TRAIN_SIZE + TEST_SIZE + VAL_SIZE + CAL_SIZE, 1.0):\n",
    "    # This check might be redundant given the calculation of TRAIN_SIZE, but good for safety.\n",
    "    raise ValueError(\"Sum of split proportions must be equal to 1.\")\n",
    "\n",
    "if np.isclose(TRAIN_SIZE, 0) and (np.isclose(VAL_SIZE, 0) or np.isclose(TEST_SIZE, 0) or np.isclose(CAL_SIZE, 0)):\n",
    "     # Avoid scenarios where train is 0 but other splits are also 0, leading to ambiguity.\n",
    "     # If only train is 0, it might be valid in some rare cases, but usually requires at least one other non-zero split.\n",
    "     # Let's enforce Train > 0 for typical ML workflows.\n",
    "     # If you need zero training data, adjust this check.\n",
    "     logging.warning(\"TRAIN_SIZE is zero or near zero. Ensure this is intended.\")\n",
    "     if TRAIN_SIZE < 0: # Definitely an error\n",
    "         raise ValueError(\"TRAIN_SIZE cannot be negative.\")\n",
    "     # Allow TRAIN_SIZE = 0 only if explicitly handled later, otherwise raise error?\n",
    "     # For now, let's proceed but log a warning. If TRAIN_SIZE must be > 0, uncomment the raise below.\n",
    "     # raise ValueError(\"TRAIN_SIZE must be greater than 0 for typical model training.\")\n",
    "\n",
    "\n",
    "logging.info(f\"Target split ratios: Train={TRAIN_SIZE:.2f}, Val={VAL_SIZE:.2f}, Test={TEST_SIZE:.2f}, Cal={CAL_SIZE:.2f}\")\n",
    "\n",
    "# --- Initialize Splits ---\n",
    "# Use iloc[0:0] to create empty DataFrames/Series with the same columns/dtype\n",
    "empty_X = X.iloc[0:0]\n",
    "empty_y = y.iloc[0:0]\n",
    "X_train, y_train = empty_X.copy(), empty_y.copy()\n",
    "X_val, y_val = empty_X.copy(), empty_y.copy()\n",
    "X_test, y_test = empty_X.copy(), empty_y.copy()\n",
    "X_cal, y_cal = empty_X.copy(), empty_y.copy()\n",
    "\n",
    "# Temporary variables for sequential splitting\n",
    "X_remaining, y_remaining = X.copy(), y.copy() # Use copies to avoid modifying original X, y\n",
    "\n",
    "# --- Stratification Option ---\n",
    "# Define stratify_func only once\n",
    "def get_stratify_array(y_arr):\n",
    "    return y_arr if SPLIT_STRATEGY == 'stratified' and not y_arr.empty else None\n",
    "\n",
    "# --- First Split: Train vs. Remainder (Val + Test + Cal) ---\n",
    "val_test_cal_size = VAL_SIZE + TEST_SIZE + CAL_SIZE\n",
    "\n",
    "if np.isclose(val_test_cal_size, 0): # Only Train set needed\n",
    "    X_train, y_train = X_remaining, y_remaining\n",
    "    logging.info(\"All data assigned to Train set (Val, Test, Cal sizes are 0).\")\n",
    "    X_remaining, y_remaining = empty_X.copy(), empty_y.copy() # No remainder\n",
    "elif np.isclose(TRAIN_SIZE, 0): # No Train set needed\n",
    "    logging.info(\"Train set is empty (TRAIN_SIZE=0). Remainder passed to next splits.\")\n",
    "    # X_remaining, y_remaining already hold all data\n",
    "else: # Split Train vs Remainder\n",
    "    split_test_size = val_test_cal_size # Proportion of remainder relative to total (1.0)\n",
    "    X_train, X_remaining, y_train, y_remaining = train_test_split(\n",
    "        X_remaining, y_remaining,\n",
    "        test_size=split_test_size,\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=get_stratify_array(y_remaining)\n",
    "    )\n",
    "logging.info(f\"Train set shape: {X_train.shape}\")\n",
    "\n",
    "\n",
    "# --- Second Split: Val vs. Remainder (Test + Cal) ---\n",
    "if not X_remaining.empty:\n",
    "    test_cal_size = TEST_SIZE + CAL_SIZE\n",
    "    # Denominator for relative size calculation: size of the current remaining pool\n",
    "    current_remaining_size_frac = VAL_SIZE + test_cal_size # = val_test_cal_size\n",
    "\n",
    "    if np.isclose(VAL_SIZE, 0): # No Val set, pass remainder to next stage\n",
    "        X_temp2, y_temp2 = X_remaining, y_remaining # Remainder is Test + Cal\n",
    "        logging.info(\"Validation set is empty (VAL_SIZE=0).\")\n",
    "    elif np.isclose(test_cal_size, 0): # Only Val set left in remainder\n",
    "        X_val, y_val = X_remaining, y_remaining\n",
    "        X_temp2, y_temp2 = empty_X.copy(), empty_y.copy() # No data left for Test/Cal\n",
    "        logging.info(f\"Validation set shape: {X_val.shape}\")\n",
    "    else: # Split Val vs (Test + Cal)\n",
    "        # Proportion of (Test + Cal) relative to (Val + Test + Cal)\n",
    "        split_test_size = test_cal_size / current_remaining_size_frac\n",
    "        X_val, X_temp2, y_val, y_temp2 = train_test_split(\n",
    "            X_remaining, y_remaining,\n",
    "            test_size=split_test_size,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=get_stratify_array(y_remaining)\n",
    "        )\n",
    "        logging.info(f\"Validation set shape: {X_val.shape}\")\n",
    "else: # No data remaining after train split\n",
    "    X_temp2, y_temp2 = empty_X.copy(), empty_y.copy()\n",
    "    if not np.isclose(VAL_SIZE, 0): # Log only if Val set was expected\n",
    "       logging.info(\"Validation set is empty (no data remaining after train split).\")\n",
    "\n",
    "\n",
    "# --- Third Split: Test vs. Cal ---\n",
    "if not X_temp2.empty:\n",
    "    # Denominator for relative size calculation: size of the current remaining pool\n",
    "    current_remaining_size_frac = TEST_SIZE + CAL_SIZE # = test_cal_size\n",
    "\n",
    "    if np.isclose(CAL_SIZE, 0): # No Cal set, remainder is Test\n",
    "        X_test, y_test = X_temp2, y_temp2\n",
    "        logging.info(\"Calibration set is empty (CAL_SIZE=0).\")\n",
    "    elif np.isclose(TEST_SIZE, 0): # Only Cal set left in remainder\n",
    "        X_cal, y_cal = X_temp2, y_temp2\n",
    "        logging.info(\"Test set is empty (TEST_SIZE=0).\")\n",
    "    else: # Split Test vs Cal\n",
    "        # Proportion of Cal relative to (Test + Cal)\n",
    "        split_test_size = CAL_SIZE / current_remaining_size_frac\n",
    "        X_test, X_cal, y_test, y_cal = train_test_split(\n",
    "            X_temp2, y_temp2,\n",
    "            test_size=split_test_size,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=get_stratify_array(y_temp2)\n",
    "        )\n",
    "        # Logging shapes done after the if/else block\n",
    "else: # No data remaining for Test/Cal split\n",
    "    if not (np.isclose(TEST_SIZE, 0) and np.isclose(CAL_SIZE, 0)): # Log only if Test or Cal were expected\n",
    "        logging.info(\"Test and Calibration sets are empty (no data remaining for final split).\")\n",
    "\n",
    "# Log final shapes for Test and Cal\n",
    "logging.info(f\"Test set shape: {X_test.shape}\")\n",
    "logging.info(f\"Calibration set shape: {X_cal.shape}\")\n",
    "\n",
    "\n",
    "# --- Verification and Final Logging ---\n",
    "total_len = len(X_train) + len(X_val) + len(X_test) + len(X_cal)\n",
    "original_len = len(X)\n",
    "\n",
    "if total_len != original_len:\n",
    "     # Calculate actual proportions based on lengths\n",
    "     actual_train = len(X_train) / original_len if original_len > 0 else 0\n",
    "     actual_val = len(X_val) / original_len if original_len > 0 else 0\n",
    "     actual_test = len(X_test) / original_len if original_len > 0 else 0\n",
    "     actual_cal = len(X_cal) / original_len if original_len > 0 else 0\n",
    "     logging.warning(f\"Total split length ({total_len}) does not exactly match original length ({original_len}). \"\n",
    "                     f\"This can happen with stratification or rounding. \"\n",
    "                     f\"Target proportions: Train={TRAIN_SIZE:.3f}, Val={VAL_SIZE:.3f}, Test={TEST_SIZE:.3f}, Cal={CAL_SIZE:.3f}. \"\n",
    "                     f\"Actual proportions: Train={actual_train:.3f}, Val={actual_val:.3f}, Test={actual_test:.3f}, Cal={actual_cal:.3f}\")\n",
    "else:\n",
    "    logging.info(\"Split lengths verification successful.\")\n",
    "\n",
    "logging.info(\"Data splitting complete.\")\n",
    "\n",
    "# Log distributions, handling empty sets\n",
    "def log_distribution(name, y_set):\n",
    "    if y_set.empty:\n",
    "        logging.info(f\"{name} target distribution: Set is empty.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Use normalize=True, handle potential division by zero if counts are zero (though unlikely if not empty)\n",
    "            counts = y_set.value_counts()\n",
    "            dist = counts / counts.sum() if counts.sum() > 0 else counts\n",
    "            logging.info(f\"{name} target distribution:\\n{dist}\")\n",
    "            # Log absolute counts as well for clarity\n",
    "            logging.info(f\"{name} target counts:\\n{counts}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Could not calculate distribution for {name}: {e}\")\n",
    "            # Attempt to log raw value counts even if normalization fails\n",
    "            try:\n",
    "                logging.info(f\"{name} raw value counts:\\n{y_set.value_counts()}\")\n",
    "            except Exception as e_raw:\n",
    "                 logging.error(f\"Could not get raw value counts for {name}: {e_raw}\")\n",
    "\n",
    "\n",
    "log_distribution(\"Train\", y_train)\n",
    "log_distribution(\"Validation\", y_val)\n",
    "log_distribution(\"Test\", y_test)\n",
    "log_distribution(\"Calibration\", y_cal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Scaling ---\n",
    "# Important for SVM, can be beneficial for others too.\n",
    "# Fit scaler ONLY on training data, then transform all sets.\n",
    "\n",
    "# Check if training set and other datasets are non-empty before scaling\n",
    "if len(X_train) > 0 and TRAIN_SIZE > 0:\n",
    "    logging.info(\"Applying StandardScaler to features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "else:\n",
    "    logging.info(\"Empty training set, NOT able to apply StandardScaler!\")\n",
    "    X_train_scaled = X_train\n",
    "\n",
    "if len(X_val) > 0 and VAL_SIZE > 0:\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "else:\n",
    "    X_val_scaled = X_val\n",
    "\n",
    "if len(X_test) > 0 and TEST_SIZE > 0:\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "else:\n",
    "    X_test_scaled = X_test\n",
    "\n",
    "if len(X_cal) > 0 and CAL_SIZE > 0:\n",
    "    X_cal_scaled = scaler.transform(X_cal)\n",
    "else:\n",
    "    X_cal_scaled = X_cal\n",
    "\n",
    "# Save the scaler if it was fitted\n",
    "if 'scaler' in locals():\n",
    "    scaler_filename = os.path.join(MODEL_DIR, f\"scaler_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib\")\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    logging.info(f\"Scaler saved to {scaler_filename}\")\n",
    "\n",
    "# Use scaled data for models sensitive to scale (like SVM)\n",
    "# For tree-based models, scaling is not strictly necessary, but using scaled\n",
    "# data consistently here won't hurt and simplifies the workflow.\n",
    "X_train_processed = X_train_scaled\n",
    "X_val_processed = X_val_scaled\n",
    "X_test_processed = X_test_scaled\n",
    "X_cal_processed = X_cal_scaled\n",
    "\n",
    "# (Optional) Convert back to DataFrames for easier column inspection if needed\n",
    "# X_train_processed = pd.DataFrame(X_train_scaled, columns=FEATURE_COLUMNS, index=X_train.index)\n",
    "# X_val_processed = pd.DataFrame(X_val_scaled, columns=FEATURE_COLUMNS, index=X_val.index)\n",
    "# X_test_processed = pd.DataFrame(X_test_scaled, columns=FEATURE_COLUMNS, index=X_test.index)\n",
    "# X_cal_processed = pd.DataFrame(X_cal_scaled, columns=FEATURE_COLUMNS, index=X_cal.index)\n",
    "\n",
    "logging.info(\"Feature scaling complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platt Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_platt_scaler(base_estimator_class, best_params, X_train, y_train, n_splits=5, random_state=None):\n",
    "    \"\"\"\n",
    "    Trains a base estimator and calibrates its outputs using Platt scaling\n",
    "    with k-fold cross-validation to obtain out-of-fold decision scores.\n",
    "\n",
    "    Args:\n",
    "        base_estimator_class: The class of the base estimator (e.g., SVC).\n",
    "        best_params (dict): Dictionary of best hyperparameters for the base estimator.\n",
    "        X_train (np.ndarray): Training features.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        n_splits (int): Number of folds for cross-validation.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (fitted_base_estimator, fitted_platt_scaler)\n",
    "               Returns (None, None) if an error occurs.\n",
    "    \"\"\"\n",
    "    logging.info(\"--- Starting Platt Scaling Training ---\")\n",
    "    try:\n",
    "        # 1. Train the final base model on the entire training set\n",
    "        logging.info(\"Training final base model on full training data...\")\n",
    "        final_base_estimator = base_estimator_class(**best_params)\n",
    "        final_base_estimator.fit(X_train, y_train)\n",
    "        logging.info(\"Final base model trained.\")\n",
    "\n",
    "        # 2. Get out-of-fold decision scores using k-fold CV\n",
    "        logging.info(f\"Performing {n_splits}-fold CV to get out-of-fold decision scores...\")\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        oof_decision_scores = np.zeros_like(y_train, dtype=float)\n",
    "        oof_true_labels = np.zeros_like(y_train, dtype=int)\n",
    "        fold_indices = np.zeros_like(y_train, dtype=int) # To store which fold each sample was held out from\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "            logging.info(f\"Processing Fold {fold+1}/{n_splits}...\")\n",
    "            X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx] # Use iloc for Series indexing\n",
    "\n",
    "            # Clone and train estimator on the fold's training data\n",
    "            estimator_fold = base_estimator_class(**best_params) # Instantiate fresh estimator\n",
    "            estimator_fold.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "            # Get decision function scores for the validation fold\n",
    "            decision_scores_fold = estimator_fold.decision_function(X_val_fold)\n",
    "            \n",
    "            # Store results\n",
    "            oof_decision_scores[val_idx] = decision_scores_fold\n",
    "            oof_true_labels[val_idx] = y_val_fold.values # Get numpy array from Series\n",
    "            fold_indices[val_idx] = fold + 1\n",
    "\n",
    "\n",
    "        logging.info(\"Out-of-fold decision scores collected.\")\n",
    "        \n",
    "        # Ensure correct shapes and types\n",
    "        oof_decision_scores = oof_decision_scores.reshape(-1, 1)\n",
    "\n",
    "        # 3. Train the Logistic Regression scaler\n",
    "        logging.info(\"Training Logistic Regression (Platt) scaler...\")\n",
    "        # Use high C to approximate original Platt scaling (low regularization)\n",
    "        platt_scaler = LogisticRegression(C=1e10, solver='liblinear', random_state=random_state)\n",
    "        platt_scaler.fit(oof_decision_scores, oof_true_labels)\n",
    "        logging.info(\"Platt scaler trained.\")\n",
    "\n",
    "        # Verify shapes one last time\n",
    "        if oof_decision_scores.shape[0] != len(oof_true_labels):\n",
    "            raise ValueError(f\"Shape mismatch after collecting OOF scores: scores {oof_decision_scores.shape[0]}, labels {len(oof_true_labels)}\")\n",
    "\n",
    "\n",
    "        logging.info(\"--- Platt Scaling Training Complete ---\")\n",
    "        return final_base_estimator, platt_scaler\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during Platt scaling: {e}\", exc_info=True)\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ncm_scores(calibrated_probs, true_labels):\n",
    "    \"\"\"Calculates non-conformity scores (1 - probability of true class).\"\"\"\n",
    "    if not isinstance(calibrated_probs, np.ndarray) or not isinstance(true_labels, np.ndarray):\n",
    "         # Ensure inputs are numpy arrays for proper indexing\n",
    "        calibrated_probs = np.asarray(calibrated_probs)\n",
    "        true_labels = np.asarray(true_labels)\n",
    "        \n",
    "    if calibrated_probs.shape[0] != true_labels.shape[0]:\n",
    "         raise ValueError(\"Probs and labels must have the same number of samples.\")\n",
    "    if calibrated_probs.shape[1] < np.max(true_labels) + 1:\n",
    "        raise ValueError(\"Probs array has fewer columns than needed for max label index.\")\n",
    "\n",
    "    # Get probability of the true class for each sample\n",
    "    # Using np.take_along_axis for efficient indexing\n",
    "    true_class_probs = np.take_along_axis(calibrated_probs, true_labels[:, np.newaxis], axis=1).squeeze()\n",
    "\n",
    "    return 1.0 - true_class_probs\n",
    "\n",
    "\n",
    "def calibrate_conformal_threshold(ncm_scores, alpha):\n",
    "    \"\"\"Calculates the quantile threshold q for ICP.\"\"\"\n",
    "    n = len(ncm_scores)\n",
    "    q_level = np.ceil((n + 1) * (1 - alpha)) / n\n",
    "    q_threshold = np.quantile(ncm_scores, q_level, method='higher') # Use 'higher' to ensure coverage\n",
    "    logging.info(f\"Calibrated CP: n={n}, alpha={alpha}, q_level={q_level:.4f}, q_threshold={q_threshold:.6f}\")\n",
    "    return q_threshold\n",
    "\n",
    "def predict_conformal_sets(calibrated_probs, q_threshold):\n",
    "    \"\"\"Generates prediction sets based on calibrated probabilities and threshold.\"\"\"\n",
    "    prediction_sets = []\n",
    "    ncm_scores_per_class = 1.0 - calibrated_probs # NCM score for *each* potential class\n",
    "\n",
    "    for scores in ncm_scores_per_class:\n",
    "        set_for_sample = [i for i, score in enumerate(scores) if score <= q_threshold]\n",
    "        # Handle empty sets - should be rare with correct quantile calculation\n",
    "        # but as a fallback, could include the most likely class\n",
    "        if not set_for_sample:\n",
    "             set_for_sample = [np.argmin(scores)] # Index of lowest NCM score == highest prob\n",
    "             logging.warning(f\"Empty prediction set generated, falling back to most likely class: {set_for_sample}\")\n",
    "        prediction_sets.append(set(set_for_sample)) # Store as sets\n",
    "    return prediction_sets\n",
    "\n",
    "def evaluate_conformal_prediction(y_true, prediction_sets, alpha, model_name=\"Model\"):\n",
    "    \"\"\"Evaluates the performance of conformal prediction sets.\"\"\"\n",
    "    if not isinstance(y_true, (pd.Series, np.ndarray)):\n",
    "        y_true = np.asarray(y_true) # Ensure y_true is indexable\n",
    "\n",
    "    n_samples = len(y_true)\n",
    "    if n_samples == 0:\n",
    "        logging.warning(f\"[{model_name} CP Eval] Empty y_true provided.\")\n",
    "        return None, None\n",
    "\n",
    "    # Empirical Coverage\n",
    "    coverage_count = sum(y_true.iloc[i] in prediction_sets[i] for i in range(n_samples))\n",
    "    empirical_coverage = coverage_count / n_samples\n",
    "\n",
    "    # Average Set Size\n",
    "    average_set_size = np.mean([len(s) for s in prediction_sets])\n",
    "\n",
    "    logging.info(f\"--- {model_name} Conformal Prediction Evaluation (alpha={alpha}) ---\")\n",
    "    logging.info(f\"Target Coverage: {1 - alpha:.2f}\")\n",
    "    logging.info(f\"Empirical Coverage: {empirical_coverage:.4f} ({coverage_count}/{n_samples})\")\n",
    "    logging.info(f\"Average Prediction Set Size: {average_set_size:.4f}\")\n",
    "\n",
    "    return empirical_coverage, average_set_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Comprehensive Metrics ---\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_proba, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Calculates a comprehensive set of classification metrics.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Ground truth labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        y_proba (array-like): Predicted probabilities for the positive class (class 1).\n",
    "        model_name (str): Name of the model for logging.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing calculated metrics.\n",
    "              Returns None if input arrays are empty or invalid.\n",
    "    \"\"\"\n",
    "    if len(y_true) == 0 or len(y_pred) == 0 or len(y_proba) == 0:\n",
    "        logging.error(f\"[{model_name}] Empty input arrays provided for metric calculation.\")\n",
    "        return None\n",
    "    if len(y_true) != len(y_pred) or len(y_true) != len(y_proba):\n",
    "        logging.error(f\"[{model_name}] Mismatched lengths in input arrays for metric calculation.\")\n",
    "        return None\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # --- Threshold-based Metrics (using y_pred) ---\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['precision'] = precision\n",
    "    metrics['recall_tpr'] = recall # True Positive Rate (Sensitivity)\n",
    "    metrics['f1_score'] = f1\n",
    "\n",
    "    # Specificity (True Negative Rate)\n",
    "    metrics['specificity_tnr'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "    # Geometric Mean\n",
    "    metrics['g_mean'] = np.sqrt(metrics['recall_tpr'] * metrics['specificity_tnr'])\n",
    "\n",
    "    # Confusion Matrix\n",
    "    metrics['confusion_matrix'] = {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp}\n",
    "\n",
    "    # --- Ranking/Probabilistic Metrics (using y_proba) ---\n",
    "    try:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_true, y_proba)\n",
    "    except ValueError as e:\n",
    "        logging.warning(f\"[{model_name}] Could not calculate ROC AUC: {e}. Setting to 0.0.\")\n",
    "        metrics['roc_auc'] = 0.0 # Handle cases with only one class present\n",
    "\n",
    "    # PR AUC\n",
    "    pr_curve_precision, pr_curve_recall, _ = precision_recall_curve(y_true, y_proba)\n",
    "    metrics['pr_auc'] = auc(pr_curve_recall, pr_curve_precision) # Note order: recall is x, precision is y\n",
    "\n",
    "    # Brier Score\n",
    "    metrics['brier_score'] = brier_score_loss(y_true, y_proba)\n",
    "\n",
    "    logging.info(f\"--- {model_name} Metrics ---\")\n",
    "    logging.info(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    logging.info(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    logging.info(f\"Recall (TPR): {metrics['recall_tpr']:.4f}\")\n",
    "    logging.info(f\"Specificity (TNR): {metrics['specificity_tnr']:.4f}\")\n",
    "    logging.info(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    logging.info(f\"G-Mean: {metrics['g_mean']:.4f}\")\n",
    "    logging.info(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    logging.info(f\"PR AUC: {metrics['pr_auc']:.4f}\")\n",
    "    logging.info(f\"Brier Score: {metrics['brier_score']:.4f}\")\n",
    "    logging.info(f\"Confusion Matrix (TN, FP, FN, TP): ({tn}, {fp}, {fn}, {tp})\")\n",
    "\n",
    "    # Optional: Plot Confusion Matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap([[tn, fp], [fn, tp]], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted Galaxy (0)', 'Predicted Star (1)'],\n",
    "                yticklabels=['Actual Galaxy (0)', 'Actual Star (1)'])\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    cm_filename = os.path.join(MODEL_DIR, f\"{model_name}_confusion_matrix_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\")\n",
    "    plt.savefig(cm_filename)\n",
    "    plt.close()\n",
    "    logging.info(f\"Confusion matrix plot saved to {cm_filename}\")\n",
    "\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    }
   ],
   "source": [
    "# --- 3.1 SVM: Define Base Model and Hyperparameter Search ---\n",
    "model_name_svm = \"svm_rbf_tuned\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") # Timestamp for this run\n",
    "\n",
    "# Base SVM instance (untrained)\n",
    "# Ensure probability=False initially, as we handle calibration separately.\n",
    "# decision_function is needed for independent Platt scaling.\n",
    "base_svm = SVC(\n",
    "    kernel='rbf',\n",
    "    random_state=RANDOM_SEED,\n",
    "    class_weight='balanced',\n",
    "    # probability=False # Not needed if using decision_function + external Platt\n",
    ")\n",
    "\n",
    "# Hyperparameter distribution for Randomized Search\n",
    "# Use loguniform for C and gamma as they often span orders of magnitude\n",
    "param_dist_svm = {\n",
    "    'C': loguniform(1e-2, 1e3),       # Regularization parameter (e.g., 0.01 to 1000)\n",
    "    'gamma': loguniform(1e-4, 1e1)   # Kernel coefficient for 'rbf' (e.g., 0.0001 to 10)\n",
    "    # 'kernel': ['rbf'] # Keep kernel fixed or add 'linear' etc. if desired\n",
    "}\n",
    "\n",
    "# Randomized Search Cross-Validation\n",
    "N_ITER_SEARCH = 5 # Number of parameter settings sampled (adjust as needed)\n",
    "CV_FOLDS_SEARCH = 5 # Number of folds for cross-validation during search\n",
    "\n",
    "logging.info(f\"--- [{model_name_svm}] Setting up RandomizedSearchCV ---\")\n",
    "logging.info(f\"Parameter distributions: {param_dist_svm}\")\n",
    "logging.info(f\"Number of iterations: {N_ITER_SEARCH}\")\n",
    "logging.info(f\"CV folds: {CV_FOLDS_SEARCH}\")\n",
    "\n",
    "random_search_svm = RandomizedSearchCV(\n",
    "    estimator=base_svm,\n",
    "    param_distributions=param_dist_svm,\n",
    "    n_iter=N_ITER_SEARCH,\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS_SEARCH, shuffle=True, random_state=RANDOM_SEED), # Explicit StratifiedKFold\n",
    "    scoring='f1', # Optimize for F1\n",
    "    n_jobs=-1,         # Use all available CPU cores\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=1          # Set verbosity level for RandomizedSearchCV\n",
    ")\n",
    "\n",
    "logging.info(f\"Starting hyperparameter search for {model_name_svm}...\")\n",
    "start_time_hpo = datetime.now()\n",
    "\n",
    "# Perform the search on the scaled training data\n",
    "random_search_svm.fit(X_train_processed, y_train)\n",
    "\n",
    "end_time_hpo = datetime.now()\n",
    "hpo_duration = end_time_hpo - start_time_hpo\n",
    "logging.info(f\"Hyperparameter search finished in: {hpo_duration}\")\n",
    "\n",
    "# Get best parameters\n",
    "best_params_svm = random_search_svm.best_params_\n",
    "best_score_svm = random_search_svm.best_score_\n",
    "logging.info(f\"Best parameters found: {best_params_svm}\")\n",
    "logging.info(f\"Best cross-validation score ({random_search_svm.scoring}): {best_score_svm:.4f}\")\n",
    "\n",
    "# Add required parameters if not tuned but needed by SVC\n",
    "best_params_svm['random_state'] = RANDOM_SEED\n",
    "best_params_svm['class_weight'] = 'balanced'\n",
    "# best_params_svm['probability'] = False # Ensure this is set if needed by base class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.2 SVM: Train Final Model and Platt Scaler ---\n",
    "# Now train the final base SVM with best params and the Platt scaler\n",
    "logging.info(f\"--- [{model_name_svm}] Training final model and Platt scaler ---\")\n",
    "\n",
    "CV_FOLDS_PLATT = 5 # Folds for Platt scaling CV\n",
    "\n",
    "# Ensure y_train is passed as a Series/DataFrame if X_train_processed is NumPy\n",
    "# Ensure that the base estimator class SVC is passed, not an instance\n",
    "fitted_svm_base, platt_scaler_svm = train_platt_scaler(\n",
    "    base_estimator_class=SVC, # Pass the class itself\n",
    "    best_params=best_params_svm,\n",
    "    X_train=X_train_processed, # Use scaled data\n",
    "    y_train=y_train,           # Pass original Series/DF for StratifiedKFold indexing\n",
    "    n_splits=CV_FOLDS_PLATT,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Save the final base model and the scaler\n",
    "if fitted_svm_base and platt_scaler_svm:\n",
    "    base_model_filename_svm = os.path.join(MODEL_DIR, f\"{model_name_svm}_base_{timestamp}.joblib\")\n",
    "    scaler_filename_svm = os.path.join(MODEL_DIR, f\"{model_name_svm}_platt_scaler_{timestamp}.joblib\")\n",
    "\n",
    "    logging.info(f\"Saving final base SVM model to {base_model_filename_svm}\")\n",
    "    joblib.dump(fitted_svm_base, base_model_filename_svm)\n",
    "    logging.info(f\"Saving Platt scaler to {scaler_filename_svm}\")\n",
    "    joblib.dump(platt_scaler_svm, scaler_filename_svm)\n",
    "    logging.info(\"Base model and Platt scaler saved successfully.\")\n",
    "else:\n",
    "    logging.error(f\"[{model_name_svm}] Failed to train base model or Platt scaler. Aborting further steps for this model.\")\n",
    "    # Set models to None to prevent errors in subsequent steps\n",
    "    fitted_svm_base = None\n",
    "    platt_scaler_svm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (Optional) Load Models if already trained ---\n",
    "# This section is useful if you want to run evaluation separately\n",
    "\n",
    "LOAD_EXISTING = False # Set to True to load previously saved models/scalers\n",
    "target_timestamp = \"YYYYMMDD_HHMMSS\" # Replace with the actual timestamp if LOAD_EXISTING is True\n",
    "\n",
    "if LOAD_EXISTING:\n",
    "    base_model_filename_svm_load = os.path.join(MODEL_DIR, f\"{model_name_svm}_base_{target_timestamp}.joblib\")\n",
    "    scaler_filename_svm_load = os.path.join(MODEL_DIR, f\"{model_name_svm}_platt_scaler_{target_timestamp}.joblib\")\n",
    "\n",
    "    if os.path.exists(base_model_filename_svm_load) and os.path.exists(scaler_filename_svm_load):\n",
    "        logging.info(f\"Loading existing base model: {base_model_filename_svm_load}\")\n",
    "        fitted_svm_base = joblib.load(base_model_filename_svm_load)\n",
    "        logging.info(f\"Loading existing Platt scaler: {scaler_filename_svm_load}\")\n",
    "        platt_scaler_svm = joblib.load(scaler_filename_svm_load)\n",
    "        logging.info(\"Models loaded successfully.\")\n",
    "    else:\n",
    "        logging.error(\"Load existing set to True, but model/scaler files not found. Cannot proceed.\")\n",
    "        fitted_svm_base = None\n",
    "        platt_scaler_svm = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.3 SVM: Calibrate Conformal Prediction Threshold ---\n",
    "ALPHA = 0.1 # Miscoverage rate (e.g., 0.1 for 90% coverage)\n",
    "q_threshold_svm = None # Initialize\n",
    "\n",
    "if fitted_svm_base and platt_scaler_svm:\n",
    "    logging.info(f\"--- [{model_name_svm}] Calibrating Conformal Prediction (alpha={ALPHA}) ---\")\n",
    "\n",
    "    # 1. Get calibrated probabilities for the calibration set\n",
    "    logging.info(\"Getting decision scores for calibration set...\")\n",
    "    decision_scores_cal_svm = fitted_svm_base.decision_function(X_cal_processed) # Use scaled cal data\n",
    "    logging.info(\"Predicting calibrated probabilities for calibration set...\")\n",
    "    # Platt scaler expects 2D input\n",
    "    calibrated_probs_cal_svm = platt_scaler_svm.predict_proba(decision_scores_cal_svm.reshape(-1, 1))\n",
    "\n",
    "    # 2. Calculate NCM scores for the calibration set\n",
    "    logging.info(\"Calculating NCM scores for calibration set...\")\n",
    "    ncm_scores_cal_svm = calculate_ncm_scores(calibrated_probs_cal_svm, y_cal.values) # Use .values for numpy array\n",
    "\n",
    "    # 3. Determine the quantile threshold q\n",
    "    logging.info(\"Determining CP threshold q...\")\n",
    "    q_threshold_svm = calibrate_conformal_threshold(ncm_scores_cal_svm, ALPHA)\n",
    "\n",
    "    # Save the threshold\n",
    "    cp_threshold_filename_svm = os.path.join(MODEL_DIR, f\"{model_name_svm}_cp_threshold_alpha{ALPHA}_{timestamp}.joblib\")\n",
    "    logging.info(f\"Saving CP threshold ({q_threshold_svm:.6f}) to {cp_threshold_filename_svm}\")\n",
    "    joblib.dump(q_threshold_svm, cp_threshold_filename_svm)\n",
    "\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_svm}] Base model or Platt scaler not available. Skipping CP calibration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (Optional) Load CP Threshold ---\n",
    "if LOAD_EXISTING and q_threshold_svm is None: # Only load if not calculated above\n",
    "    cp_threshold_filename_svm_load = os.path.join(MODEL_DIR, f\"{model_name_svm}_cp_threshold_alpha{ALPHA}_{target_timestamp}.joblib\")\n",
    "    if os.path.exists(cp_threshold_filename_svm_load):\n",
    "        logging.info(f\"Loading existing CP threshold: {cp_threshold_filename_svm_load}\")\n",
    "        q_threshold_svm = joblib.load(cp_threshold_filename_svm_load)\n",
    "        logging.info(f\"CP threshold ({q_threshold_svm:.6f}) loaded.\")\n",
    "    else:\n",
    "        logging.error(\"Load existing set to True, but CP threshold file not found.\")\n",
    "        q_threshold_svm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.4 SVM: Final Evaluation on Test Set ---\n",
    "if fitted_svm_base and platt_scaler_svm:\n",
    "    logging.info(f\"--- [{model_name_svm}] Final Evaluation on Test Set ---\")\n",
    "\n",
    "    # 1. Get calibrated probabilities for the test set\n",
    "    logging.info(\"Getting decision scores for test set...\")\n",
    "    decision_scores_test_svm = fitted_svm_base.decision_function(X_test_processed) # Use scaled test data\n",
    "    logging.info(\"Predicting calibrated probabilities for test set...\")\n",
    "    calibrated_probs_test_svm = platt_scaler_svm.predict_proba(decision_scores_test_svm.reshape(-1, 1))\n",
    "    \n",
    "    # Get probabilities for the positive class (Star=1) for standard metrics\n",
    "    y_proba_test_svm = calibrated_probs_test_svm[:, 1]\n",
    "\n",
    "    # 2. Get point predictions (e.g., using 0.5 threshold on calibrated probs)\n",
    "    y_pred_test_svm = (y_proba_test_svm >= 0.5).astype(int)\n",
    "\n",
    "    # 3. Calculate standard performance metrics\n",
    "    logging.info(\"Calculating standard performance metrics...\")\n",
    "    metrics_svm = calculate_metrics(y_test, y_pred_test_svm, y_proba_test_svm, model_name=model_name_svm)\n",
    "\n",
    "    # 4. Evaluate Conformal Prediction (if threshold is available)\n",
    "    if q_threshold_svm is not None:\n",
    "        logging.info(\"Generating and evaluating conformal prediction sets...\")\n",
    "        prediction_sets_test_svm = predict_conformal_sets(calibrated_probs_test_svm, q_threshold_svm)\n",
    "        cp_coverage_svm, cp_avg_set_size_svm = evaluate_conformal_prediction(\n",
    "            y_test, prediction_sets_test_svm, ALPHA, model_name=model_name_svm\n",
    "        )\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_svm}] CP threshold not available. Skipping CP evaluation.\")\n",
    "        cp_coverage_svm, cp_avg_set_size_svm = None, None\n",
    "\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_svm}] Base model or Platt scaler not loaded/trained. Skipping final evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Implementation: Decision Tree (CART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.1 CART: Define Model ---\n",
    "model_name_cart = \"cart\"\n",
    "model_cart = None\n",
    "\n",
    "# Hyperparameters (Defaults tend to overfit, apply some basic constraints)\n",
    "cart_params = {\n",
    "    'criterion': 'gini',        # Split quality measure ('gini' or 'entropy')\n",
    "    'max_depth': 15,            # Max depth to prevent overfitting (tune later)\n",
    "    'min_samples_split': 10,    # Min samples required to split an internal node (tune later)\n",
    "    'min_samples_leaf': 5,      # Min samples required at a leaf node (tune later)\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'class_weight': 'balanced' # Handle imbalance\n",
    "}\n",
    "\n",
    "model_cart = DecisionTreeClassifier(**cart_params)\n",
    "logging.info(f\"Defined CART model '{model_name_cart}' with params: {cart_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.2 CART: Train Model ---\n",
    "logging.info(f\"--- Training {model_name_cart} ---\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_filename_cart = os.path.join(MODEL_DIR, f\"{model_name_cart}_{timestamp}.joblib\")\n",
    "\n",
    "if os.path.exists(model_filename_cart):\n",
    "    logging.warning(f\"Model file {model_filename_cart} already exists. Skipping training.\")\n",
    "else:\n",
    "    logging.info(f\"Starting training for {model_name_cart}...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Tree models don't strictly need scaling, but we use processed data for consistency\n",
    "    model_cart.fit(X_train_processed, y_train)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    training_duration = end_time - start_time\n",
    "    logging.info(f\"Training finished in: {training_duration}\")\n",
    "    logging.info(f\"Saving trained model to {model_filename_cart}\")\n",
    "    joblib.dump(model_cart, model_filename_cart)\n",
    "    logging.info(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.3 CART: Load Model ---\n",
    "list_of_cart_files = glob.glob(os.path.join(MODEL_DIR, f\"{model_name_cart}_*.joblib\"))\n",
    "if list_of_cart_files:\n",
    "    latest_cart_file = max(list_of_cart_files, key=os.path.getctime)\n",
    "    logging.info(f\"Loading latest CART model: {latest_cart_file}\")\n",
    "    try:\n",
    "        model_cart_loaded = joblib.load(latest_cart_file)\n",
    "        logging.info(\"CART model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading CART model: {e}\")\n",
    "        model_cart_loaded = None\n",
    "else:\n",
    "    logging.warning(f\"No saved models found for {model_name_cart} in {MODEL_DIR}\")\n",
    "    model_cart_loaded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.4 CART: Test Model ---\n",
    "if model_cart_loaded:\n",
    "    logging.info(f\"--- Testing {model_name_cart} ---\")\n",
    "    y_pred_cart = model_cart_loaded.predict(X_test_processed)\n",
    "    y_proba_cart = model_cart_loaded.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy_cart = accuracy_score(y_test, y_pred_cart)\n",
    "    roc_auc_cart = roc_auc_score(y_test, y_proba_cart)\n",
    "    report_cart = classification_report(y_test, y_pred_cart, target_names=['Galaxy (0)', 'Star (1)'])\n",
    "    cm_cart = confusion_matrix(y_test, y_pred_cart)\n",
    "\n",
    "    logging.info(f\"CART Test Accuracy: {accuracy_cart:.4f}\")\n",
    "    logging.info(f\"CART Test ROC AUC: {roc_auc_cart:.4f}\")\n",
    "    logging.info(f\"CART Classification Report:\\n{report_cart}\")\n",
    "    logging.info(f\"CART Confusion Matrix:\\n{cm_cart}\")\n",
    "else:\n",
    "    logging.warning(\"CART model not loaded. Skipping testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Implementation: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1 RF: Define Model ---\n",
    "model_name_rf = \"random_forest\"\n",
    "model_rf = None\n",
    "\n",
    "# Hyperparameters (Good starting point)\n",
    "rf_params = {\n",
    "    'n_estimators': 200,        # Number of trees in the forest\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': None,          # Grow trees fully (or set a limit like CART)\n",
    "    'min_samples_split': 2,     # Default\n",
    "    'min_samples_leaf': 1,      # Default (can increase for regularization)\n",
    "    'max_features': 'sqrt',     # Number of features to consider for best split ('sqrt', 'log2', or int/float)\n",
    "    'bootstrap': True,          # Use bootstrap samples\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'n_jobs': -1,               # Use all available CPU cores\n",
    "    'class_weight': 'balanced'  # Handle imbalance\n",
    "}\n",
    "\n",
    "model_rf = RandomForestClassifier(**rf_params)\n",
    "logging.info(f\"Defined RF model '{model_name_rf}' with params: {rf_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.2 RF: Train Model ---\n",
    "logging.info(f\"--- Training {model_name_rf} ---\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_filename_rf = os.path.join(MODEL_DIR, f\"{model_name_rf}_{timestamp}.joblib\")\n",
    "\n",
    "if os.path.exists(model_filename_rf):\n",
    "    logging.warning(f\"Model file {model_filename_rf} already exists. Skipping training.\")\n",
    "else:\n",
    "    logging.info(f\"Starting training for {model_name_rf}...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    model_rf.fit(X_train_processed, y_train)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    training_duration = end_time - start_time\n",
    "    logging.info(f\"Training finished in: {training_duration}\")\n",
    "    logging.info(f\"Saving trained model to {model_filename_rf}\")\n",
    "    joblib.dump(model_rf, model_filename_rf)\n",
    "    logging.info(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.3 RF: Load Model ---\n",
    "list_of_rf_files = glob.glob(os.path.join(MODEL_DIR, f\"{model_name_rf}_*.joblib\"))\n",
    "if list_of_rf_files:\n",
    "    latest_rf_file = max(list_of_rf_files, key=os.path.getctime)\n",
    "    logging.info(f\"Loading latest RF model: {latest_rf_file}\")\n",
    "    try:\n",
    "        model_rf_loaded = joblib.load(latest_rf_file)\n",
    "        logging.info(\"RF model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading RF model: {e}\")\n",
    "        model_rf_loaded = None\n",
    "else:\n",
    "    logging.warning(f\"No saved models found for {model_name_rf} in {MODEL_DIR}\")\n",
    "    model_rf_loaded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.4 RF: Test Model ---\n",
    "if model_rf_loaded:\n",
    "    logging.info(f\"--- Testing {model_name_rf} ---\")\n",
    "    y_pred_rf = model_rf_loaded.predict(X_test_processed)\n",
    "    y_proba_rf = model_rf_loaded.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "    roc_auc_rf = roc_auc_score(y_test, y_proba_rf)\n",
    "    report_rf = classification_report(y_test, y_pred_rf, target_names=['Galaxy (0)', 'Star (1)'])\n",
    "    cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "    logging.info(f\"RF Test Accuracy: {accuracy_rf:.4f}\")\n",
    "    logging.info(f\"RF Test ROC AUC: {roc_auc_rf:.4f}\")\n",
    "    logging.info(f\"RF Classification Report:\\n{report_rf}\")\n",
    "    logging.info(f\"RF Confusion Matrix:\\n{cm_rf}\")\n",
    "else:\n",
    "    logging.warning(\"RF model not loaded. Skipping testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6.1 XGB: Define Model ---\n",
    "model_name_xgb = \"xgboost\"\n",
    "model_xgb = None\n",
    "\n",
    "# Hyperparameters\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic', # Objective function for binary classification\n",
    "    'eval_metric': 'auc',           # Evaluation metric ('logloss', 'auc', 'error')\n",
    "    'n_estimators': 200,            # Number of boosting rounds/trees\n",
    "    'learning_rate': 0.1,           # Step size shrinkage\n",
    "    'max_depth': 5,                 # Maximum tree depth\n",
    "    'subsample': 0.8,               # Fraction of samples used per tree\n",
    "    'colsample_bytree': 0.8,        # Fraction of features used per tree\n",
    "    'gamma': 0,                     # Minimum loss reduction required to make a split\n",
    "    'reg_alpha': 0,                 # L1 regularization\n",
    "    'reg_lambda': 1,                # L2 regularization (default)\n",
    "    'use_label_encoder': False,     # Recommended setting for recent versions\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'n_jobs': -1\n",
    "    # scale_pos_weight can be used for imbalance, but often handled by eval_metric='auc' and tuning\n",
    "}\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(**xgb_params)\n",
    "logging.info(f\"Defined XGBoost model '{model_name_xgb}' with params: {xgb_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m eval_set \u001b[38;5;241m=\u001b[39m [(X_val_processed, y_val)]\n\u001b[0;32m     15\u001b[0m early_stopping_rounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m \u001b[38;5;66;03m# Stop if no improvement on eval_set for 15 rounds\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mmodel_xgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m              \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m              \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m              \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Set verbose=True or integer for progress logs\u001b[39;00m\n\u001b[0;32m     22\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     23\u001b[0m training_duration \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\javym\\miniconda3\\envs\\Lab\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "# --- 6.2 XGB: Train Model ---\n",
    "logging.info(f\"--- Training {model_name_xgb} ---\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# XGBoost has its own save method, but joblib works too for consistency here\n",
    "model_filename_xgb = os.path.join(MODEL_DIR, f\"{model_name_xgb}_{timestamp}.joblib\")\n",
    "\n",
    "if os.path.exists(model_filename_xgb):\n",
    "    logging.warning(f\"Model file {model_filename_xgb} already exists. Skipping training.\")\n",
    "else:\n",
    "    logging.info(f\"Starting training for {model_name_xgb}...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Use validation set for early stopping\n",
    "    eval_set = [(X_val_processed, y_val)]\n",
    "    early_stopping_rounds = 15 # Stop if no improvement on eval_set for 15 rounds\n",
    "\n",
    "    model_xgb.fit(X_train_processed, y_train,\n",
    "                  eval_set=eval_set,\n",
    "                  early_stopping_rounds=early_stopping_rounds,\n",
    "                  verbose=False) # Set verbose=True or integer for progress logs\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    training_duration = end_time - start_time\n",
    "    logging.info(f\"Training finished in: {training_duration}\")\n",
    "    logging.info(f\"Best iteration: {model_xgb.best_iteration}, Best score ({xgb_params['eval_metric']}): {model_xgb.best_score:.4f}\")\n",
    "    logging.info(f\"Saving trained model to {model_filename_xgb}\")\n",
    "    joblib.dump(model_xgb, model_filename_xgb)\n",
    "    # Alternatively: model_xgb.save_model(model_filename_xgb.replace('.joblib', '.xgb'))\n",
    "    logging.info(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6.3 XGB: Load Model ---\n",
    "list_of_xgb_files = glob.glob(os.path.join(MODEL_DIR, f\"{model_name_xgb}_*.joblib\"))\n",
    "if list_of_xgb_files:\n",
    "    latest_xgb_file = max(list_of_xgb_files, key=os.path.getctime)\n",
    "    logging.info(f\"Loading latest XGBoost model: {latest_xgb_file}\")\n",
    "    try:\n",
    "        model_xgb_loaded = joblib.load(latest_xgb_file)\n",
    "        # If using native save:\n",
    "        # model_xgb_loaded = xgb.XGBClassifier()\n",
    "        # model_xgb_loaded.load_model(latest_xgb_file.replace('.joblib', '.xgb'))\n",
    "        logging.info(\"XGBoost model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading XGBoost model: {e}\")\n",
    "        model_xgb_loaded = None\n",
    "else:\n",
    "    logging.warning(f\"No saved models found for {model_name_xgb} in {MODEL_DIR}\")\n",
    "    model_xgb_loaded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6.4 XGB: Test Model ---\n",
    "if model_xgb_loaded:\n",
    "    logging.info(f\"--- Testing {model_name_xgb} ---\")\n",
    "    y_pred_xgb = model_xgb_loaded.predict(X_test_processed)\n",
    "    y_proba_xgb = model_xgb_loaded.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "    roc_auc_xgb = roc_auc_score(y_test, y_proba_xgb)\n",
    "    report_xgb = classification_report(y_test, y_pred_xgb, target_names=['Galaxy (0)', 'Star (1)'])\n",
    "    cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "\n",
    "    logging.info(f\"XGBoost Test Accuracy: {accuracy_xgb:.4f}\")\n",
    "    logging.info(f\"XGBoost Test ROC AUC: {roc_auc_xgb:.4f}\")\n",
    "    logging.info(f\"XGBoost Classification Report:\\n{report_xgb}\")\n",
    "    logging.info(f\"XGBoost Confusion Matrix:\\n{cm_xgb}\")\n",
    "else:\n",
    "    logging.warning(\"XGBoost model not loaded. Skipping testing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Implementation: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7.1 LGBM: Define Model ---\n",
    "model_name_lgbm = \"lightgbm\"\n",
    "model_lgbm = None\n",
    "\n",
    "# Hyperparameters\n",
    "lgbm_params = {\n",
    "    'objective': 'binary',          # Binary classification\n",
    "    'metric': 'auc',                # Evaluation metric ('auc', 'binary_logloss')\n",
    "    'n_estimators': 200,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,               # Default, main parameter to control complexity\n",
    "    'max_depth': -1,                # Default: no limit (num_leaves is often preferred)\n",
    "    'feature_fraction': 0.8,        # Equivalent to colsample_bytree\n",
    "    'bagging_fraction': 0.8,        # Equivalent to subsample\n",
    "    'bagging_freq': 1,              # Perform bagging at every iteration\n",
    "    'reg_alpha': 0,\n",
    "    'reg_lambda': 0,                # Default: 0 for LightGBM\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'n_jobs': -1,\n",
    "    'class_weight': 'balanced'      # Handle imbalance\n",
    "}\n",
    "\n",
    "model_lgbm = lgb.LGBMClassifier(**lgbm_params)\n",
    "logging.info(f\"Defined LightGBM model '{model_name_lgbm}' with params: {lgbm_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7.2 LGBM: Train Model ---\n",
    "logging.info(f\"--- Training {model_name_lgbm} ---\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# LightGBM also has native save, using joblib here\n",
    "model_filename_lgbm = os.path.join(MODEL_DIR, f\"{model_name_lgbm}_{timestamp}.joblib\")\n",
    "\n",
    "if os.path.exists(model_filename_lgbm):\n",
    "    logging.warning(f\"Model file {model_filename_lgbm} already exists. Skipping training.\")\n",
    "else:\n",
    "    logging.info(f\"Starting training for {model_name_lgbm}...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Use validation set for early stopping\n",
    "    eval_set = [(X_val_processed, y_val)]\n",
    "    early_stopping_rounds = 15\n",
    "\n",
    "    # Need callbacks for early stopping in older versions, but newer ones have direct param\n",
    "    # from lightgbm import early_stopping\n",
    "    # callbacks = [early_stopping(stopping_rounds=early_stopping_rounds, verbose=1)]\n",
    "\n",
    "    model_lgbm.fit(X_train_processed, y_train,\n",
    "                   eval_set=eval_set,\n",
    "                   eval_metric=lgbm_params['metric'],\n",
    "                   # callbacks=callbacks # Use this if early_stopping_rounds is not in fit()\n",
    "                   # For newer versions:\n",
    "                   early_stopping_rounds=early_stopping_rounds\n",
    "                   )\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    training_duration = end_time - start_time\n",
    "    logging.info(f\"Training finished in: {training_duration}\")\n",
    "    logging.info(f\"Best iteration: {model_lgbm.best_iteration_}, Best score ({lgbm_params['metric']}): {model_lgbm.best_score_['valid_0'][lgbm_params['metric']]:.4f}\")\n",
    "    logging.info(f\"Saving trained model to {model_filename_lgbm}\")\n",
    "    joblib.dump(model_lgbm, model_filename_lgbm)\n",
    "    # Alternatively: model_lgbm.booster_.save_model(model_filename_lgbm.replace('.joblib', '.txt'))\n",
    "    logging.info(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7.3 LGBM: Load Model ---\n",
    "list_of_lgbm_files = glob.glob(os.path.join(MODEL_DIR, f\"{model_name_lgbm}_*.joblib\"))\n",
    "if list_of_lgbm_files:\n",
    "    latest_lgbm_file = max(list_of_lgbm_files, key=os.path.getctime)\n",
    "    logging.info(f\"Loading latest LightGBM model: {latest_lgbm_file}\")\n",
    "    try:\n",
    "        model_lgbm_loaded = joblib.load(latest_lgbm_file)\n",
    "        # If using native save:\n",
    "        # model_lgbm_loaded = lgb.Booster(model_file=latest_lgbm_file.replace('.joblib', '.txt')) # Note: loads Booster, need wrapper for predict\n",
    "        # model_lgbm_loaded_clf = lgb.LGBMClassifier()\n",
    "        # model_lgbm_loaded_clf.booster_ = model_lgbm_loaded\n",
    "        logging.info(\"LightGBM model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading LightGBM model: {e}\")\n",
    "        model_lgbm_loaded = None\n",
    "else:\n",
    "    logging.warning(f\"No saved models found for {model_name_lgbm} in {MODEL_DIR}\")\n",
    "    model_lgbm_loaded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7.4 LGBM: Test Model ---\n",
    "if model_lgbm_loaded:\n",
    "    logging.info(f\"--- Testing {model_name_lgbm} ---\")\n",
    "    # If loaded Booster directly, need wrapper or use booster_.predict\n",
    "    # y_pred_lgbm = (model_lgbm_loaded.predict(X_test_processed) > 0.5).astype(int) # Booster predicts scores\n",
    "    # y_proba_lgbm = model_lgbm_loaded.predict(X_test_processed)\n",
    "    # If loaded via joblib (as LGBMClassifier):\n",
    "    y_pred_lgbm = model_lgbm_loaded.predict(X_test_processed)\n",
    "    y_proba_lgbm = model_lgbm_loaded.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy_lgbm = accuracy_score(y_test, y_pred_lgbm)\n",
    "    roc_auc_lgbm = roc_auc_score(y_test, y_proba_lgbm)\n",
    "    report_lgbm = classification_report(y_test, y_pred_lgbm, target_names=['Galaxy (0)', 'Star (1)'])\n",
    "    cm_lgbm = confusion_matrix(y_test, y_pred_lgbm)\n",
    "\n",
    "    logging.info(f\"LightGBM Test Accuracy: {accuracy_lgbm:.4f}\")\n",
    "    logging.info(f\"LightGBM Test ROC AUC: {roc_auc_lgbm:.4f}\")\n",
    "    logging.info(f\"LightGBM Classification Report:\\n{report_lgbm}\")\n",
    "    logging.info(f\"LightGBM Confusion Matrix:\\n{cm_lgbm}\")\n",
    "else:\n",
    "    logging.warning(\"LightGBM model not loaded. Skipping testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 9. Next Steps\n",
    "#\n",
    "# - **Hyperparameter Tuning:** Use the validation set (`X_val_processed`, `y_val`) with techniques like `GridSearchCV` or `RandomizedSearchCV` to find optimal hyperparameters for each model.\n",
    "# - **Feature Engineering:** Create new features (e.g., colors like F365W - F814W) and evaluate their impact.\n",
    "# - **Feature Importance:** Analyze feature importance plots (especially for tree-based models) to understand which ALHAMBRA measurements are most predictive.\n",
    "# - **Calibration:** Implement calibration methods using the calibration set.\n",
    "# - **Error Analysis:** Investigate misclassified examples in the test set to understand model weaknesses.\n",
    "# - **Comparison:** Systematically compare the performance metrics of all tuned and calibrated models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
