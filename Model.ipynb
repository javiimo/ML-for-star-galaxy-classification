{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter nbconvert --to script Model.ipynb\n",
    "\n",
    "\n",
    "#TODO: CalibraciÃ³n con Conformal Prediction.\n",
    "#TODO: Hyperparameter selection con el validation dataset. `GridSearchCV` or `RandomizedSearchCV`\n",
    "#TODO: Undersampling/weighted algorithms.\n",
    "#TODO: Usar metricas para estudiar la performance.\n",
    "#TODO: Estudiar relevancia de las features para cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any existing log files\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "# Reset logger to avoid any issues with permissions\n",
    "logging.shutdown()\n",
    "# Remove loggers\n",
    "for log_file in glob.glob(\"*.log\"):\n",
    "    os.remove(log_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star-Galaxy Classification using ALHAMBRA Photometry\n",
    "\n",
    "This notebook implements and evaluates several machine learning models for classifying astronomical objects as stars or galaxies based on multi-band photometric data from the ALHAMBRA survey, using labels derived from higher-resolution COSMOS2020 data.\n",
    "\n",
    "**Target Variable:** `acs_mu_class` (from COSMOS2020)\n",
    " - Which is 1 for Galaxy and 2 for Star. We will remap this to 0 (Galaxy, majority class) and 1 (Star, minority class).\n",
    "\n",
    "**Features:** Selected columns from the ALHAMBRA survey data.\n",
    "\n",
    "**Models:**\n",
    "1. Support Vector Machine (SVM)\n",
    "2. Decision Tree (CART)\n",
    "3. Random Forest\n",
    "4. XGBoost\n",
    "5. LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import joblib # For saving/loading models efficiently\n",
    "import glob\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support, roc_auc_score\n",
    ")\n",
    "\n",
    "# Boosting models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.shutdown()\n",
    "logging.basicConfig(\n",
    "    filename=f'models_{datetime.now().strftime(\"%d_%H-%M-%S\")}.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True\n",
    ")\n",
    "# Prevent logs from being printed to console\n",
    "logging.getLogger().handlers = [h for h in logging.getLogger().handlers if isinstance(h, logging.FileHandler)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Dataset & Feature Selection\n",
    "\n",
    "**Interesting Feature Combinations for Modeling:**\n",
    "\n",
    "1.  `morphology_only`: Use only features from category 1.\n",
    "2.  `photometry_magnitudes_only`: Use only features from category 2.\n",
    "3.  `photometry_mags_errors`: Use features from categories 2 and 3. (Recommended over magnitudes alone).\n",
    "4.  `photometry_colors_only`: *Requires deriving colors first* (e.g., F457W - F519W, J - KS). Not directly listed but a very common and powerful approach.\n",
    "5.  `photometry_plus_morphology`: Use features from categories 1, 2, and 3.\n",
    "6.  `photometry_no_redshift`: Use features from categories 1, 2, 3, and 5. Excludes all BPZ-derived outputs. Tests classification based purely on observed shape and flux.\n",
    "7.  `redshift_related_only`: Use only features from category 4. Tests how much classification info is contained *just* in the photo-z code's output (which is itself based on photometry).\n",
    "8.  `full_alhambra_photometry_based`: Use features from categories 2, 3, 4, and 5. All information derivable from ALHAMBRA photometry, including photo-z results and per-band quality.\n",
    "9.  `full_alhambra_all`: Use features from categories 1, 2, 3, 4, and 5. The most comprehensive set using only ALHAMBRA-derived features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the df\n",
    "df = pd.read_csv('data/match_alhambra_cosmos2020_ACS_class_0.8arcsec.csv')\n",
    "logging.info(f\"DataFrame created with shape: {df.shape}\")\n",
    "# Map ACS classification: 1 (Galaxy, Majority) -> 0, 2 (Star, minority) -> 1, 3 (Fake) -> drop\n",
    "logging.info(\"Original class counts:\")\n",
    "logging.info(df['acs_mu_class'].value_counts().to_string())\n",
    "\n",
    "# Drop fake detections (class 3)\n",
    "# Drop fake detections\n",
    "n_fakes = (df['acs_mu_class'] == 3).sum()\n",
    "logging.info(f\"Number of fake detections (class 3): {n_fakes}\")\n",
    "df = df[df['acs_mu_class'] != 3]\n",
    "\n",
    "# Map classifications\n",
    "df['acs_mu_class'] = df['acs_mu_class'].map({1: 0, 2: 1})\n",
    "\n",
    "logging.info(\"After dropping fakes and mapping classes (0: Galaxy, 1: Star):\")\n",
    "logging.info(df['acs_mu_class'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features\n",
    "\n",
    "# --- Define feature categories based on ALHAMBRA data using exact names ---\n",
    "\n",
    "# 1. ALHAMBRA Morphology Features (SExtractor-based)\n",
    "morphology_features = [\n",
    "    'area', 'fwhm', 'stell', 'ell', 'a', 'b', 'theta', 'rk', 'rf', 's2n'\n",
    "]\n",
    "\n",
    "# 2. ALHAMBRA Photometry Magnitudes (Optical + NIR + Synthetic)\n",
    "OPTICAL_MAG_COLS = [\n",
    "    'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W',\n",
    "    'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W',\n",
    "    'F799W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W'\n",
    "]\n",
    "photometry_magnitudes = (\n",
    "    OPTICAL_MAG_COLS +\n",
    "    ['J', 'H', 'KS', 'F814W']\n",
    ")\n",
    "\n",
    "# 3. ALHAMBRA Photometry Uncertainties\n",
    "OPTICAL_ERR_COLS = [\n",
    "    'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W',\n",
    "    'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W',\n",
    "    'dF799W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W'\n",
    "]\n",
    "photometry_uncertainties = (\n",
    "    OPTICAL_ERR_COLS +\n",
    "    ['dJ', 'dH', 'dKS', 'dF814W']\n",
    ")\n",
    "\n",
    "# Combine Mags and Errors - often used together\n",
    "photometry_mags_errors = photometry_magnitudes + photometry_uncertainties\n",
    "\n",
    "# 4. ALHAMBRA Photometric Redshift & Derived Features (BPZ-based)\n",
    "redshift_derived_features = [\n",
    "    'zb_1', 'zb_Min_1', 'zb_Max_1', 'Tb_1', 'Odds_1',\n",
    "    'z_ml', 't_ml', 'Chi2',\n",
    "    'Stell_Mass_1', 'M_Abs_1', 'MagPrior'\n",
    "]\n",
    "\n",
    "# 5. ALHAMBRA Quality/Auxiliary Features (per-band quality etc.)\n",
    "OPTICAL_IRMS_COLS = [\n",
    "    'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W',\n",
    "    'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W',\n",
    "    'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W',\n",
    "    'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W'\n",
    "]\n",
    "quality_aux_features = (\n",
    "    ['nfobs'] +\n",
    "    OPTICAL_IRMS_COLS +\n",
    "    ['irms_J', 'irms_H', 'irms_KS', 'irms_F814W']\n",
    ")\n",
    "\n",
    "# --- Define lists of features NOT used for modeling ---\n",
    "\n",
    "non_modeling_identifiers = ['ID_1', 'id_2'] # ALHAMBRA ID, COSMOS ID\n",
    "\n",
    "non_modeling_astrometry = [\n",
    "    'RA_1', 'Dec_1', 'x', 'y', # ALHAMBRA Astrometry\n",
    "    'ra_2', 'dec_2',          # COSMOS Astrometry\n",
    "    'Separation'              # Matching Quality\n",
    "]\n",
    "\n",
    "non_modeling_flags = [\n",
    "    'photoflag', 'xray', 'PercW', 'Satur_Flag', # ALHAMBRA Object/Photometry Flags\n",
    "    'irms_OPT_Flag', 'irms_NIR_Flag'           # ALHAMBRA Overall Quality Flags\n",
    "]\n",
    "\n",
    "non_modeling_alhambra_prediction = ['Stellar_Flag'] # ALHAMBRA's own classification\n",
    "\n",
    "non_modeling_aperture_mags = [ # Specific aperture mags, usually use total mags\n",
    "    'F814W_3arcs', 'dF814W_3arcs', 'F814W_3arcs_corr'\n",
    "]\n",
    "\n",
    "non_modeling_cosmos_features = [ # Measurements/flags derived from COSMOS data (HST, HSC, VISTA...)\n",
    "    'model_flag',\n",
    "    'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista',\n",
    "    'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid',\n",
    "    'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid',\n",
    "    'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid',\n",
    "    'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid',\n",
    "    'acs_f814w_mag', 'acs_f814w_magerr',\n",
    "    'acs_fwhm_world', 'acs_mu_max',\n",
    "    'solution_model' # This is categorical, but still COSMOS-derived info\n",
    "]\n",
    "\n",
    "target_variable = ['acs_mu_class'] # The COSMOS classification label to predict\n",
    "\n",
    "# --- Consolidate into the main dictionary for easy access ---\n",
    "\n",
    "feature_sets = {\n",
    "    # --- Potential Input Feature Sets ---\n",
    "    'morphology_only': morphology_features,\n",
    "    'photometry_magnitudes_only': photometry_magnitudes,\n",
    "    'photometry_mags_errors': photometry_mags_errors,\n",
    "    'photometry_plus_morphology': photometry_mags_errors + morphology_features,\n",
    "    'photometry_no_redshift': photometry_mags_errors + morphology_features + quality_aux_features,\n",
    "    'redshift_related_only': redshift_derived_features,\n",
    "    'full_alhambra_photometry_based': photometry_mags_errors + redshift_derived_features + quality_aux_features,\n",
    "    'full_alhambra_all': (morphology_features +\n",
    "                          photometry_mags_errors +\n",
    "                          redshift_derived_features +\n",
    "                          quality_aux_features),\n",
    "\n",
    "    # --- Excluded Feature Sets ---\n",
    "    'non_modeling_identifiers': non_modeling_identifiers,\n",
    "    'non_modeling_astrometry': non_modeling_astrometry,\n",
    "    'non_modeling_flags': non_modeling_flags,\n",
    "    'non_modeling_alhambra_prediction': non_modeling_alhambra_prediction,\n",
    "    'non_modeling_aperture_mags': non_modeling_aperture_mags,\n",
    "    'non_modeling_cosmos_features': non_modeling_cosmos_features,\n",
    "    'target_variable': target_variable\n",
    "}\n",
    "\n",
    "# --- Function to get a specific feature set (Unchanged from before) ---\n",
    "\n",
    "def get_feature_set(df, set_name, feature_sets_dict):\n",
    "    \"\"\"\n",
    "    Selects columns from a DataFrame based on a predefined feature set name.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        set_name (str): The name of the desired feature set\n",
    "                        (must be a key in feature_sets_dict).\n",
    "        feature_sets_dict (dict): Dictionary containing feature set names\n",
    "                                  as keys and lists of column names as values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing only the columns\n",
    "                      belonging to the specified feature set.\n",
    "                      Returns an empty DataFrame if no columns are found.\n",
    "    \"\"\"\n",
    "    if set_name not in feature_sets_dict:\n",
    "        raise ValueError(f\"Feature set '{set_name}' not defined. \"\n",
    "                         f\"Available sets: {list(feature_sets_dict.keys())}\")\n",
    "\n",
    "    # Get the list of columns for the requested set\n",
    "    required_cols_in_set = feature_sets_dict[set_name]\n",
    "\n",
    "    # Find which of these columns actually exist in the DataFrame\n",
    "    available_cols = [col for col in required_cols_in_set if col in df.columns]\n",
    "\n",
    "    # Warn if some columns from the set definition are missing\n",
    "    missing_cols = [col for col in required_cols_in_set if col not in available_cols]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: The following columns defined for feature set '{set_name}'\"\n",
    "              f\" were not found in the DataFrame and will be excluded: {missing_cols}\")\n",
    "\n",
    "    if not available_cols:\n",
    "        print(f\"Warning: No columns for feature set '{set_name}' found in the DataFrame.\")\n",
    "        return pd.DataFrame() # Return empty DataFrame\n",
    "\n",
    "    print(f\"Selecting feature set '{set_name}' with {len(available_cols)} columns.\")\n",
    "    return df[available_cols]\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assuming 'df' is your loaded DataFrame\n",
    "\n",
    "# Example: Get the DataFrame with only morphology features\n",
    "# df_morphology = get_feature_set(df, 'morphology_only', feature_sets)\n",
    "# print(f\"Morphology features shape: {df_morphology.shape}\")\n",
    "\n",
    "# Example: Get the DataFrame with photometry (mags+errors) and morphology\n",
    "# df_phot_morph = get_feature_set(df, 'photometry_plus_morphology', feature_sets)\n",
    "# print(f\"Photometry + Morphology features shape: {df_phot_morph.shape}\")\n",
    "\n",
    "# Example: Get the most comprehensive set of ALHAMBRA features\n",
    "# df_full = get_feature_set(df, 'full_alhambra_all', feature_sets)\n",
    "# print(f\"Full ALHAMBRA features shape: {df_full.shape}\")\n",
    "\n",
    "# Example: Get the target variable\n",
    "# target = df[feature_sets['target_variable'][0]]\n",
    "# print(f\"Target variable shape: {target.shape}\")\n",
    "\n",
    "# Example: See which COSMOS features were identified for exclusion\n",
    "# df_cosmos_excluded = get_feature_set(df, 'non_modeling_cosmos_features', feature_sets)\n",
    "# print(f\"COSMOS features to exclude: {df_cosmos_excluded.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting parameters\n",
    "TEST_SIZE = 0.10 # Test set proportion\n",
    "VAL_SIZE = 0.10 # Validation set proportion\n",
    "CAL_SIZE = 0.10 # Calibration set proportion\n",
    "# Train size will be 1 - (TEST_SIZE + VAL_SIZE + CAL_SIZE) = 0.70\n",
    "\n",
    "RANDOM_SEED = 42 # For reproducibility\n",
    "\n",
    "# Model saving directory\n",
    "MODEL_DIR = \"trained_models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Data splitting strategy ('stratified' or 'random')\n",
    "SPLIT_STRATEGY = 'stratified' # Recommended for imbalanced datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target column\n",
    "TARGET_COLUMN = target_variable[0]\n",
    "\n",
    "# Feature columns sets: \n",
    "    # 'morphology_only', 'photometry_magnitudes_only', 'photometry_mags_errors','photometry_plus_morphology','photometry_no_redshift','redshift_related_only','full_alhambra_photometry_based','full_alhambra_all'\n",
    "FEATURE_COLUMNS = 'full_alhambra_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting feature set 'full_alhambra_all' with 94 columns.\n"
     ]
    }
   ],
   "source": [
    "# --- Data Cleaning ---\n",
    "logging.info(f\"Original dataset size: {df.shape}\")\n",
    "\n",
    "# Handle missing values (simple strategy: drop rows with NaNs in target or features)\n",
    "# More sophisticated imputation could be used later.\n",
    "df_clean = df.dropna(subset=[TARGET_COLUMN] + feature_sets[FEATURE_COLUMNS]).copy()\n",
    "logging.info(f\"Dataset size after dropping NaNs: {df_clean.shape}\")\n",
    "\n",
    "logging.info(f\"Value counts for target:\\n1 (Star): {(df_clean[TARGET_COLUMN] == 1).sum()}\\n0 (Galaxy): {(df_clean[TARGET_COLUMN] == 0).sum()}\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = get_feature_set(df_clean, FEATURE_COLUMNS, feature_sets)\n",
    "y = df_clean[TARGET_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Splitting ---\n",
    "\n",
    "logging.info(f\"Splitting data using '{SPLIT_STRATEGY}' strategy...\")\n",
    "logging.info(f\"Split ratios: Train={1-TEST_SIZE-VAL_SIZE-CAL_SIZE:.2f}, Val={VAL_SIZE:.2f}, Test={TEST_SIZE:.2f}, Cal={CAL_SIZE:.2f}\")\n",
    "\n",
    "# Calculate intermediate split sizes\n",
    "val_test_cal_size = VAL_SIZE + TEST_SIZE + CAL_SIZE # 0.30\n",
    "val_rel_size = VAL_SIZE / val_test_cal_size         # 0.10 / 0.30 = 1/3\n",
    "test_cal_size = TEST_SIZE + CAL_SIZE               # 0.20\n",
    "test_rel_size = TEST_SIZE / test_cal_size            # 0.10 / 0.20 = 1/2\n",
    "\n",
    "stratify_option = y if SPLIT_STRATEGY == 'stratified' else None\n",
    "\n",
    "# First split: Train vs. Temp (Val + Test + Cal)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y,\n",
    "    test_size=val_test_cal_size,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=stratify_option\n",
    ")\n",
    "logging.info(f\"Train set shape: {X_train.shape}\")\n",
    "logging.info(f\"Train set class distribution:\\nStars (1): {(y_train == 1).mean():.2%}\\nGalaxies (0): {(y_train == 0).mean():.2%}\")\n",
    "\n",
    "# Second split: Temp -> Val vs. Temp2 (Test + Cal)\n",
    "stratify_option_temp = y_temp if SPLIT_STRATEGY == 'stratified' else None\n",
    "X_val, X_temp2, y_val, y_temp2 = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=(1 - val_rel_size), # Size of Temp2 relative to Temp\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=stratify_option_temp\n",
    ")\n",
    "logging.info(f\"Validation set shape: {X_val.shape}\")\n",
    "\n",
    "\n",
    "# Third split: Temp2 -> Test vs. Cal\n",
    "stratify_option_temp2 = y_temp2 if SPLIT_STRATEGY == 'stratified' else None\n",
    "X_test, X_cal, y_test, y_cal = train_test_split(\n",
    "    X_temp2, y_temp2,\n",
    "    test_size=test_rel_size, # Size of Cal relative to Temp2\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=stratify_option_temp2\n",
    ")\n",
    "logging.info(f\"Test set shape: {X_test.shape}\")\n",
    "logging.info(f\"Calibration set shape: {X_cal.shape}\")\n",
    "\n",
    "# Verify splits (approximate due to stratification)\n",
    "assert len(X_train) + len(X_val) + len(X_test) + len(X_cal) == len(X)\n",
    "logging.info(\"Data splitting complete.\")\n",
    "logging.info(f\"Train target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "logging.info(f\"Validation target distribution:\\n{y_val.value_counts(normalize=True)}\")\n",
    "logging.info(f\"Test target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
    "logging.info(f\"Calibration target distribution:\\n{y_cal.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Scaling ---\n",
    "# Important for SVM, can be beneficial for others too.\n",
    "# Fit scaler ONLY on training data, then transform all sets.\n",
    "\n",
    "logging.info(\"Applying StandardScaler to features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_cal_scaled = scaler.transform(X_cal)\n",
    "\n",
    "# Save the scaler\n",
    "scaler_filename = os.path.join(MODEL_DIR, f\"scaler_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib\")\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "logging.info(f\"Scaler saved to {scaler_filename}\")\n",
    "\n",
    "# Use scaled data for models sensitive to scale (like SVM)\n",
    "# For tree-based models, scaling is not strictly necessary, but using scaled\n",
    "# data consistently here won't hurt and simplifies the workflow.\n",
    "X_train_processed = X_train_scaled\n",
    "X_val_processed = X_val_scaled\n",
    "X_test_processed = X_test_scaled\n",
    "X_cal_processed = X_cal_scaled\n",
    "\n",
    "# (Optional) Convert back to DataFrames for easier column inspection if needed\n",
    "# X_train_processed = pd.DataFrame(X_train_scaled, columns=FEATURE_COLUMNS, index=X_train.index)\n",
    "# X_val_processed = pd.DataFrame(X_val_scaled, columns=FEATURE_COLUMNS, index=X_val.index)\n",
    "# X_test_processed = pd.DataFrame(X_test_scaled, columns=FEATURE_COLUMNS, index=X_test.index)\n",
    "# X_cal_processed = pd.DataFrame(X_cal_scaled, columns=FEATURE_COLUMNS, index=X_cal.index)\n",
    "\n",
    "logging.info(\"Feature scaling complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.1 SVM: Define Model ---\n",
    "model_name_svm = \"svm_rbf\"\n",
    "model_svm = None # Initialize variable\n",
    "\n",
    "# Hyperparameters (Good defaults, tune using validation set later)\n",
    "svm_params = {\n",
    "    'C': 1.0,            # Regularization parameter\n",
    "    'kernel': 'rbf',       # Kernel type ('linear', 'poly', 'rbf', 'sigmoid')\n",
    "    'gamma': 'scale',    # Kernel coefficient for 'rbf', 'poly', 'sigmoid' ('scale' or 'auto' or float)\n",
    "    'probability': True, # Enable probability estimates (needed for ROC AUC, calibration)\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'class_weight': 'balanced' # Useful for imbalanced datasets\n",
    "}\n",
    "\n",
    "model_svm = SVC(**svm_params)\n",
    "logging.info(f\"Defined SVM model '{model_name_svm}' with params: {svm_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.2 SVM: Train Model ---\n",
    "logging.info(f\"--- Training {model_name_svm} ---\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_filename_svm = os.path.join(MODEL_DIR, f\"{model_name_svm}_{timestamp}.joblib\")\n",
    "\n",
    "if os.path.exists(model_filename_svm):\n",
    "    logging.warning(f\"Model file {model_filename_svm} already exists. Skipping training.\")\n",
    "    # Optionally load the existing one here if needed immediately\n",
    "    # model_svm = joblib.load(model_filename_svm)\n",
    "else:\n",
    "    logging.info(f\"Starting training for {model_name_svm}...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Use scaled data for SVM\n",
    "    model_svm.fit(X_train_processed, y_train)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    training_duration = end_time - start_time\n",
    "    logging.info(f\"Training finished in: {training_duration}\")\n",
    "    logging.info(f\"Saving trained model to {model_filename_svm}\")\n",
    "    joblib.dump(model_svm, model_filename_svm)\n",
    "    logging.info(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.3 SVM: Load Model ---\n",
    "# Find the latest SVM model file\n",
    "list_of_svm_files = glob.glob(os.path.join(MODEL_DIR, f\"{model_name_svm}_*.joblib\"))\n",
    "if list_of_svm_files:\n",
    "    latest_svm_file = max(list_of_svm_files, key=os.path.getctime)\n",
    "    logging.info(f\"Loading latest SVM model: {latest_svm_file}\")\n",
    "    try:\n",
    "        model_svm_loaded = joblib.load(latest_svm_file)\n",
    "        logging.info(\"SVM model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading SVM model: {e}\")\n",
    "        model_svm_loaded = None\n",
    "else:\n",
    "    logging.warning(f\"No saved models found for {model_name_svm} in {MODEL_DIR}\")\n",
    "    model_svm_loaded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.4 SVM: Test Model ---\n",
    "if model_svm_loaded:\n",
    "    logging.info(f\"--- Testing {model_name_svm} ---\")\n",
    "    # Use scaled test data\n",
    "    y_pred_svm = model_svm_loaded.predict(X_test_processed)\n",
    "    y_proba_svm = model_svm_loaded.predict_proba(X_test_processed)[:, 1] # Probability of class 1 (Star)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "    roc_auc_svm = roc_auc_score(y_test, y_proba_svm)\n",
    "    report_svm = classification_report(y_test, y_pred_svm, target_names=['Galaxy (0)', 'Star (1)'])\n",
    "    cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "    logging.info(f\"SVM Test Accuracy: {accuracy_svm:.4f}\")\n",
    "    logging.info(f\"SVM Test ROC AUC: {roc_auc_svm:.4f}\")\n",
    "    logging.info(f\"SVM Classification Report:\\n{report_svm}\")\n",
    "    logging.info(f\"SVM Confusion Matrix:\\n{cm_svm}\")\n",
    "else:\n",
    "    logging.warning(\"SVM model not loaded. Skipping testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Implementation: Decision Tree (CART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.1 CART: Define Model ---\n",
    "model_name_cart = \"cart\"\n",
    "model_cart = None\n",
    "\n",
    "# Hyperparameters (Defaults tend to overfit, apply some basic constraints)\n",
    "cart_params = {\n",
    "    'criterion': 'gini',        # Split quality measure ('gini' or 'entropy')\n",
    "    'max_depth': 15,            # Max depth to prevent overfitting (tune later)\n",
    "    'min_samples_split': 10,    # Min samples required to split an internal node (tune later)\n",
    "    'min_samples_leaf': 5,      # Min samples required at a leaf node (tune later)\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'class_weight': 'balanced' # Handle imbalance\n",
    "}\n",
    "\n",
    "model_cart = DecisionTreeClassifier(**cart_params)\n",
    "logging.info(f\"Defined CART model '{model_name_cart}' with params: {cart_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.2 CART: Train Model ---\n",
    "logging.info(f\"--- Training {model_name_cart} ---\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_filename_cart = os.path.join(MODEL_DIR, f\"{model_name_cart}_{timestamp}.joblib\")\n",
    "\n",
    "if os.path.exists(model_filename_cart):\n",
    "    logging.warning(f\"Model file {model_filename_cart} already exists. Skipping training.\")\n",
    "else:\n",
    "    logging.info(f\"Starting training for {model_name_cart}...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Tree models don't strictly need scaling, but we use processed data for consistency\n",
    "    model_cart.fit(X_train_processed, y_train)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    training_duration = end_time - start_time\n",
    "    logging.info(f\"Training finished in: {training_duration}\")\n",
    "    logging.info(f\"Saving trained model to {model_filename_cart}\")\n",
    "    joblib.dump(model_cart, model_filename_cart)\n",
    "    logging.info(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.3 CART: Load Model ---\n",
    "list_of_cart_files = glob.glob(os.path.join(MODEL_DIR, f\"{model_name_cart}_*.joblib\"))\n",
    "if list_of_cart_files:\n",
    "    latest_cart_file = max(list_of_cart_files, key=os.path.getctime)\n",
    "    logging.info(f\"Loading latest CART model: {latest_cart_file}\")\n",
    "    try:\n",
    "        model_cart_loaded = joblib.load(latest_cart_file)\n",
    "        logging.info(\"CART model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading CART model: {e}\")\n",
    "        model_cart_loaded = None\n",
    "else:\n",
    "    logging.warning(f\"No saved models found for {model_name_cart} in {MODEL_DIR}\")\n",
    "    model_cart_loaded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.4 CART: Test Model ---\n",
    "if model_cart_loaded:\n",
    "    logging.info(f\"--- Testing {model_name_cart} ---\")\n",
    "    y_pred_cart = model_cart_loaded.predict(X_test_processed)\n",
    "    y_proba_cart = model_cart_loaded.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy_cart = accuracy_score(y_test, y_pred_cart)\n",
    "    roc_auc_cart = roc_auc_score(y_test, y_proba_cart)\n",
    "    report_cart = classification_report(y_test, y_pred_cart, target_names=['Galaxy (0)', 'Star (1)'])\n",
    "    cm_cart = confusion_matrix(y_test, y_pred_cart)\n",
    "\n",
    "    logging.info(f\"CART Test Accuracy: {accuracy_cart:.4f}\")\n",
    "    logging.info(f\"CART Test ROC AUC: {roc_auc_cart:.4f}\")\n",
    "    logging.info(f\"CART Classification Report:\\n{report_cart}\")\n",
    "    logging.info(f\"CART Confusion Matrix:\\n{cm_cart}\")\n",
    "else:\n",
    "    logging.warning(\"CART model not loaded. Skipping testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Implementation: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1 RF: Define Model ---\n",
    "model_name_rf = \"random_forest\"\n",
    "model_rf = None\n",
    "\n",
    "# Hyperparameters (Good starting point)\n",
    "rf_params = {\n",
    "    'n_estimators': 200,        # Number of trees in the forest\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': None,          # Grow trees fully (or set a limit like CART)\n",
    "    'min_samples_split': 2,     # Default\n",
    "    'min_samples_leaf': 1,      # Default (can increase for regularization)\n",
    "    'max_features': 'sqrt',     # Number of features to consider for best split ('sqrt', 'log2', or int/float)\n",
    "    'bootstrap': True,          # Use bootstrap samples\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'n_jobs': -1,               # Use all available CPU cores\n",
    "    'class_weight': 'balanced'  # Handle imbalance\n",
    "}\n",
    "\n",
    "model_rf = RandomForestClassifier(**rf_params)\n",
    "logging.info(f\"Defined RF model '{model_name_rf}' with params: {rf_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.2 RF: Train Model ---\n",
    "logging.info(f\"--- Training {model_name_rf} ---\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_filename_rf = os.path.join(MODEL_DIR, f\"{model_name_rf}_{timestamp}.joblib\")\n",
    "\n",
    "if os.path.exists(model_filename_rf):\n",
    "    logging.warning(f\"Model file {model_filename_rf} already exists. Skipping training.\")\n",
    "else:\n",
    "    logging.info(f\"Starting training for {model_name_rf}...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    model_rf.fit(X_train_processed, y_train)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    training_duration = end_time - start_time\n",
    "    logging.info(f\"Training finished in: {training_duration}\")\n",
    "    logging.info(f\"Saving trained model to {model_filename_rf}\")\n",
    "    joblib.dump(model_rf, model_filename_rf)\n",
    "    logging.info(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.3 RF: Load Model ---\n",
    "list_of_rf_files = glob.glob(os.path.join(MODEL_DIR, f\"{model_name_rf}_*.joblib\"))\n",
    "if list_of_rf_files:\n",
    "    latest_rf_file = max(list_of_rf_files, key=os.path.getctime)\n",
    "    logging.info(f\"Loading latest RF model: {latest_rf_file}\")\n",
    "    try:\n",
    "        model_rf_loaded = joblib.load(latest_rf_file)\n",
    "        logging.info(\"RF model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading RF model: {e}\")\n",
    "        model_rf_loaded = None\n",
    "else:\n",
    "    logging.warning(f\"No saved models found for {model_name_rf} in {MODEL_DIR}\")\n",
    "    model_rf_loaded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.4 RF: Test Model ---\n",
    "if model_rf_loaded:\n",
    "    logging.info(f\"--- Testing {model_name_rf} ---\")\n",
    "    y_pred_rf = model_rf_loaded.predict(X_test_processed)\n",
    "    y_proba_rf = model_rf_loaded.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "    roc_auc_rf = roc_auc_score(y_test, y_proba_rf)\n",
    "    report_rf = classification_report(y_test, y_pred_rf, target_names=['Galaxy (0)', 'Star (1)'])\n",
    "    cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "    logging.info(f\"RF Test Accuracy: {accuracy_rf:.4f}\")\n",
    "    logging.info(f\"RF Test ROC AUC: {roc_auc_rf:.4f}\")\n",
    "    logging.info(f\"RF Classification Report:\\n{report_rf}\")\n",
    "    logging.info(f\"RF Confusion Matrix:\\n{cm_rf}\")\n",
    "else:\n",
    "    logging.warning(\"RF model not loaded. Skipping testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6.1 XGB: Define Model ---\n",
    "model_name_xgb = \"xgboost\"\n",
    "model_xgb = None\n",
    "\n",
    "# Hyperparameters\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic', # Objective function for binary classification\n",
    "    'eval_metric': 'auc',           # Evaluation metric ('logloss', 'auc', 'error')\n",
    "    'n_estimators': 200,            # Number of boosting rounds/trees\n",
    "    'learning_rate': 0.1,           # Step size shrinkage\n",
    "    'max_depth': 5,                 # Maximum tree depth\n",
    "    'subsample': 0.8,               # Fraction of samples used per tree\n",
    "    'colsample_bytree': 0.8,        # Fraction of features used per tree\n",
    "    'gamma': 0,                     # Minimum loss reduction required to make a split\n",
    "    'reg_alpha': 0,                 # L1 regularization\n",
    "    'reg_lambda': 1,                # L2 regularization (default)\n",
    "    'use_label_encoder': False,     # Recommended setting for recent versions\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'n_jobs': -1\n",
    "    # scale_pos_weight can be used for imbalance, but often handled by eval_metric='auc' and tuning\n",
    "}\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(**xgb_params)\n",
    "logging.info(f\"Defined XGBoost model '{model_name_xgb}' with params: {xgb_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m eval_set \u001b[38;5;241m=\u001b[39m [(X_val_processed, y_val)]\n\u001b[0;32m     15\u001b[0m early_stopping_rounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m \u001b[38;5;66;03m# Stop if no improvement on eval_set for 15 rounds\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mmodel_xgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m              \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m              \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m              \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Set verbose=True or integer for progress logs\u001b[39;00m\n\u001b[0;32m     22\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     23\u001b[0m training_duration \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\javym\\miniconda3\\envs\\Lab\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "# --- 6.2 XGB: Train Model ---\n",
    "logging.info(f\"--- Training {model_name_xgb} ---\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# XGBoost has its own save method, but joblib works too for consistency here\n",
    "model_filename_xgb = os.path.join(MODEL_DIR, f\"{model_name_xgb}_{timestamp}.joblib\")\n",
    "\n",
    "if os.path.exists(model_filename_xgb):\n",
    "    logging.warning(f\"Model file {model_filename_xgb} already exists. Skipping training.\")\n",
    "else:\n",
    "    logging.info(f\"Starting training for {model_name_xgb}...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Use validation set for early stopping\n",
    "    eval_set = [(X_val_processed, y_val)]\n",
    "    early_stopping_rounds = 15 # Stop if no improvement on eval_set for 15 rounds\n",
    "\n",
    "    model_xgb.fit(X_train_processed, y_train,\n",
    "                  eval_set=eval_set,\n",
    "                  early_stopping_rounds=early_stopping_rounds,\n",
    "                  verbose=False) # Set verbose=True or integer for progress logs\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    training_duration = end_time - start_time\n",
    "    logging.info(f\"Training finished in: {training_duration}\")\n",
    "    logging.info(f\"Best iteration: {model_xgb.best_iteration}, Best score ({xgb_params['eval_metric']}): {model_xgb.best_score:.4f}\")\n",
    "    logging.info(f\"Saving trained model to {model_filename_xgb}\")\n",
    "    joblib.dump(model_xgb, model_filename_xgb)\n",
    "    # Alternatively: model_xgb.save_model(model_filename_xgb.replace('.joblib', '.xgb'))\n",
    "    logging.info(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6.3 XGB: Load Model ---\n",
    "list_of_xgb_files = glob.glob(os.path.join(MODEL_DIR, f\"{model_name_xgb}_*.joblib\"))\n",
    "if list_of_xgb_files:\n",
    "    latest_xgb_file = max(list_of_xgb_files, key=os.path.getctime)\n",
    "    logging.info(f\"Loading latest XGBoost model: {latest_xgb_file}\")\n",
    "    try:\n",
    "        model_xgb_loaded = joblib.load(latest_xgb_file)\n",
    "        # If using native save:\n",
    "        # model_xgb_loaded = xgb.XGBClassifier()\n",
    "        # model_xgb_loaded.load_model(latest_xgb_file.replace('.joblib', '.xgb'))\n",
    "        logging.info(\"XGBoost model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading XGBoost model: {e}\")\n",
    "        model_xgb_loaded = None\n",
    "else:\n",
    "    logging.warning(f\"No saved models found for {model_name_xgb} in {MODEL_DIR}\")\n",
    "    model_xgb_loaded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6.4 XGB: Test Model ---\n",
    "if model_xgb_loaded:\n",
    "    logging.info(f\"--- Testing {model_name_xgb} ---\")\n",
    "    y_pred_xgb = model_xgb_loaded.predict(X_test_processed)\n",
    "    y_proba_xgb = model_xgb_loaded.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "    roc_auc_xgb = roc_auc_score(y_test, y_proba_xgb)\n",
    "    report_xgb = classification_report(y_test, y_pred_xgb, target_names=['Galaxy (0)', 'Star (1)'])\n",
    "    cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "\n",
    "    logging.info(f\"XGBoost Test Accuracy: {accuracy_xgb:.4f}\")\n",
    "    logging.info(f\"XGBoost Test ROC AUC: {roc_auc_xgb:.4f}\")\n",
    "    logging.info(f\"XGBoost Classification Report:\\n{report_xgb}\")\n",
    "    logging.info(f\"XGBoost Confusion Matrix:\\n{cm_xgb}\")\n",
    "else:\n",
    "    logging.warning(\"XGBoost model not loaded. Skipping testing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Implementation: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7.1 LGBM: Define Model ---\n",
    "model_name_lgbm = \"lightgbm\"\n",
    "model_lgbm = None\n",
    "\n",
    "# Hyperparameters\n",
    "lgbm_params = {\n",
    "    'objective': 'binary',          # Binary classification\n",
    "    'metric': 'auc',                # Evaluation metric ('auc', 'binary_logloss')\n",
    "    'n_estimators': 200,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,               # Default, main parameter to control complexity\n",
    "    'max_depth': -1,                # Default: no limit (num_leaves is often preferred)\n",
    "    'feature_fraction': 0.8,        # Equivalent to colsample_bytree\n",
    "    'bagging_fraction': 0.8,        # Equivalent to subsample\n",
    "    'bagging_freq': 1,              # Perform bagging at every iteration\n",
    "    'reg_alpha': 0,\n",
    "    'reg_lambda': 0,                # Default: 0 for LightGBM\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'n_jobs': -1,\n",
    "    'class_weight': 'balanced'      # Handle imbalance\n",
    "}\n",
    "\n",
    "model_lgbm = lgb.LGBMClassifier(**lgbm_params)\n",
    "logging.info(f\"Defined LightGBM model '{model_name_lgbm}' with params: {lgbm_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7.2 LGBM: Train Model ---\n",
    "logging.info(f\"--- Training {model_name_lgbm} ---\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# LightGBM also has native save, using joblib here\n",
    "model_filename_lgbm = os.path.join(MODEL_DIR, f\"{model_name_lgbm}_{timestamp}.joblib\")\n",
    "\n",
    "if os.path.exists(model_filename_lgbm):\n",
    "    logging.warning(f\"Model file {model_filename_lgbm} already exists. Skipping training.\")\n",
    "else:\n",
    "    logging.info(f\"Starting training for {model_name_lgbm}...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Use validation set for early stopping\n",
    "    eval_set = [(X_val_processed, y_val)]\n",
    "    early_stopping_rounds = 15\n",
    "\n",
    "    # Need callbacks for early stopping in older versions, but newer ones have direct param\n",
    "    # from lightgbm import early_stopping\n",
    "    # callbacks = [early_stopping(stopping_rounds=early_stopping_rounds, verbose=1)]\n",
    "\n",
    "    model_lgbm.fit(X_train_processed, y_train,\n",
    "                   eval_set=eval_set,\n",
    "                   eval_metric=lgbm_params['metric'],\n",
    "                   # callbacks=callbacks # Use this if early_stopping_rounds is not in fit()\n",
    "                   # For newer versions:\n",
    "                   early_stopping_rounds=early_stopping_rounds\n",
    "                   )\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    training_duration = end_time - start_time\n",
    "    logging.info(f\"Training finished in: {training_duration}\")\n",
    "    logging.info(f\"Best iteration: {model_lgbm.best_iteration_}, Best score ({lgbm_params['metric']}): {model_lgbm.best_score_['valid_0'][lgbm_params['metric']]:.4f}\")\n",
    "    logging.info(f\"Saving trained model to {model_filename_lgbm}\")\n",
    "    joblib.dump(model_lgbm, model_filename_lgbm)\n",
    "    # Alternatively: model_lgbm.booster_.save_model(model_filename_lgbm.replace('.joblib', '.txt'))\n",
    "    logging.info(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7.3 LGBM: Load Model ---\n",
    "list_of_lgbm_files = glob.glob(os.path.join(MODEL_DIR, f\"{model_name_lgbm}_*.joblib\"))\n",
    "if list_of_lgbm_files:\n",
    "    latest_lgbm_file = max(list_of_lgbm_files, key=os.path.getctime)\n",
    "    logging.info(f\"Loading latest LightGBM model: {latest_lgbm_file}\")\n",
    "    try:\n",
    "        model_lgbm_loaded = joblib.load(latest_lgbm_file)\n",
    "        # If using native save:\n",
    "        # model_lgbm_loaded = lgb.Booster(model_file=latest_lgbm_file.replace('.joblib', '.txt')) # Note: loads Booster, need wrapper for predict\n",
    "        # model_lgbm_loaded_clf = lgb.LGBMClassifier()\n",
    "        # model_lgbm_loaded_clf.booster_ = model_lgbm_loaded\n",
    "        logging.info(\"LightGBM model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading LightGBM model: {e}\")\n",
    "        model_lgbm_loaded = None\n",
    "else:\n",
    "    logging.warning(f\"No saved models found for {model_name_lgbm} in {MODEL_DIR}\")\n",
    "    model_lgbm_loaded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7.4 LGBM: Test Model ---\n",
    "if model_lgbm_loaded:\n",
    "    logging.info(f\"--- Testing {model_name_lgbm} ---\")\n",
    "    # If loaded Booster directly, need wrapper or use booster_.predict\n",
    "    # y_pred_lgbm = (model_lgbm_loaded.predict(X_test_processed) > 0.5).astype(int) # Booster predicts scores\n",
    "    # y_proba_lgbm = model_lgbm_loaded.predict(X_test_processed)\n",
    "    # If loaded via joblib (as LGBMClassifier):\n",
    "    y_pred_lgbm = model_lgbm_loaded.predict(X_test_processed)\n",
    "    y_proba_lgbm = model_lgbm_loaded.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy_lgbm = accuracy_score(y_test, y_pred_lgbm)\n",
    "    roc_auc_lgbm = roc_auc_score(y_test, y_proba_lgbm)\n",
    "    report_lgbm = classification_report(y_test, y_pred_lgbm, target_names=['Galaxy (0)', 'Star (1)'])\n",
    "    cm_lgbm = confusion_matrix(y_test, y_pred_lgbm)\n",
    "\n",
    "    logging.info(f\"LightGBM Test Accuracy: {accuracy_lgbm:.4f}\")\n",
    "    logging.info(f\"LightGBM Test ROC AUC: {roc_auc_lgbm:.4f}\")\n",
    "    logging.info(f\"LightGBM Classification Report:\\n{report_lgbm}\")\n",
    "    logging.info(f\"LightGBM Confusion Matrix:\\n{cm_lgbm}\")\n",
    "else:\n",
    "    logging.warning(\"LightGBM model not loaded. Skipping testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 9. Next Steps\n",
    "#\n",
    "# - **Hyperparameter Tuning:** Use the validation set (`X_val_processed`, `y_val`) with techniques like `GridSearchCV` or `RandomizedSearchCV` to find optimal hyperparameters for each model.\n",
    "# - **Feature Engineering:** Create new features (e.g., colors like F365W - F814W) and evaluate their impact.\n",
    "# - **Feature Importance:** Analyze feature importance plots (especially for tree-based models) to understand which ALHAMBRA measurements are most predictive.\n",
    "# - **Calibration:** Implement calibration methods using the calibration set.\n",
    "# - **Error Analysis:** Investigate misclassified examples in the test set to understand model weaknesses.\n",
    "# - **Comparison:** Systematically compare the performance metrics of all tuned and calibrated models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
