{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter nbconvert --to script Model.ipynb\n",
    "\n",
    "# Optional with Shapely or sth like that?\n",
    "#TODO: Estudiar relevancia de las features para cada modelo y cada grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any existing log files\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "# Reset logger to avoid any issues with permissions\n",
    "logging.shutdown()\n",
    "# Remove loggers\n",
    "for log_file in glob.glob(\"*.log\"):\n",
    "    os.remove(log_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star-Galaxy Classification using ALHAMBRA Photometry\n",
    "\n",
    "This notebook implements and evaluates several machine learning models for classifying astronomical objects as stars or galaxies based on multi-band photometric data from the ALHAMBRA survey, using labels derived from higher-resolution COSMOS2020 data.\n",
    "\n",
    "**Target Variable:** `acs_mu_class` (from COSMOS2020)\n",
    " - Which is 1 for Galaxy and 2 for Star. We will remap this to 0 (Galaxy, majority class) and 1 (Star, minority class).\n",
    "\n",
    "**Features:** Selected columns from the ALHAMBRA survey data.\n",
    "\n",
    "**Models:**\n",
    "1. Support Vector Machine (SVM)\n",
    "2. Decision Tree (CART)\n",
    "3. Random Forest\n",
    "4. XGBoost\n",
    "5. LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint, uniform, loguniform # Ensure loguniform is imported if used\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from tqdm import tqdm  \n",
    "#from tqdm.notebook import tqdm # Needs pip install ipywidgets\n",
    "from datetime import datetime\n",
    "import joblib # For saving/loading models efficiently\n",
    "import glob\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV,  ParameterSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import loguniform # For hyperparameter distributions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support, roc_auc_score,\n",
    "    brier_score_loss, precision_recall_curve, auc\n",
    ")   \n",
    "import seaborn as sns # For confusion matrix heatmap\n",
    "\n",
    "# Boosting models\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "\n",
    "# Mondrian ICP\n",
    "from crepes import ConformalClassifier\n",
    "from crepes.extras import margin, hinge\n",
    "from crepes import ConformalClassifier\n",
    "\n",
    "# Configure logging\n",
    "logging.shutdown()\n",
    "logging.basicConfig(\n",
    "    filename=f'models_{datetime.now().strftime(\"%d_%H-%M-%S\")}.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True\n",
    ")\n",
    "# Prevent logs from being printed to console\n",
    "logging.getLogger().handlers = [h for h in logging.getLogger().handlers if isinstance(h, logging.FileHandler)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Dataset & Feature Selection\n",
    "\n",
    "**Interesting Feature Combinations for Modeling:**\n",
    " \n",
    " The feature groups are defined as follows:\n",
    " - Group 1: Morphology features and their uncertainties\n",
    " - Group 2: Photometry magnitudes\n",
    " - Group 3: Photometry magnitude and errors\n",
    " - Group 4: Redshift features and their uncertainties\n",
    " - Group 5: Combination of photometry magnitude errors and morphology features (including uncertainties)\n",
    " - Group 6: Combination of photometry magnitude errors, morphology features (including uncertainties), and redshift features (including uncertainties)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the df\n",
    "df = pd.read_csv('data/match_alhambra_cosmos2020_ACS_class_0.8arcsec.csv')\n",
    "logging.info(f\"DataFrame created with shape: {df.shape}\")\n",
    "# Map ACS classification: 1 (Galaxy, Majority) -> 0, 2 (Star, minority) -> 1, 3 (Fake) -> drop\n",
    "logging.info(\"Original class counts:\")\n",
    "logging.info(df['acs_mu_class'].value_counts().to_string())\n",
    "\n",
    "# Drop fake detections (class 3)\n",
    "# Drop fake detections\n",
    "n_fakes = (df['acs_mu_class'] == 3).sum()\n",
    "logging.info(f\"Number of fake detections (class 3): {n_fakes}\")\n",
    "df = df[df['acs_mu_class'] != 3]\n",
    "\n",
    "# Map classifications\n",
    "df['acs_mu_class'] = df['acs_mu_class'].map({1: 0, 2: 1})\n",
    "\n",
    "logging.info(\"After dropping fakes and mapping classes (0: Galaxy, 1: Star):\")\n",
    "logging.info(df['acs_mu_class'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features\n",
    "\n",
    "# --- Define feature categories based on ALHAMBRA data using exact names ---\n",
    "\n",
    "# 1. ALHAMBRA Morphology Features (SExtractor-based)\n",
    "morphology_features = [\n",
    "    'area', 'fwhm', 'stell', 'ell', 'a', 'b', 'theta', 'rk', 'rf'\n",
    "]\n",
    "\n",
    "morphology_err = [\n",
    "    's2n'\n",
    "]\n",
    "\n",
    "morphology_mags_errors = morphology_features + morphology_err\n",
    "\n",
    "# 2. ALHAMBRA Photometry Magnitudes (Optical + NIR + Synthetic)\n",
    "OPTICAL_MAG_COLS = [\n",
    "    'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W',\n",
    "    'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W',\n",
    "    'F799W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W'\n",
    "]\n",
    "photometry_magnitudes = (\n",
    "    OPTICAL_MAG_COLS +\n",
    "    ['J', 'H', 'KS', 'F814W']\n",
    ")\n",
    "\n",
    "# 3. ALHAMBRA Photometry Uncertainties\n",
    "OPTICAL_ERR_COLS = [\n",
    "    'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W',\n",
    "    'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W',\n",
    "    'dF799W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W'\n",
    "]\n",
    "photometry_uncertainties = (\n",
    "    OPTICAL_ERR_COLS +\n",
    "    ['dJ', 'dH', 'dKS', 'dF814W']\n",
    ")\n",
    "\n",
    "photometry_mags_errors = photometry_magnitudes + photometry_uncertainties\n",
    "\n",
    "# 4. ALHAMBRA Photometric Redshift & Derived Features (BPZ-based)\n",
    "redshift_features = [\n",
    "    'zb_1', 'zb_Min_1', 'zb_Max_1', 'Tb_1',\n",
    "    'z_ml', 't_ml',\n",
    "    'Stell_Mass_1', 'M_Abs_1', 'MagPrior'\n",
    "]\n",
    "\n",
    "redshift_uncertainties = [\n",
    "    'Odds_1', 'Chi2'\n",
    "]\n",
    "\n",
    "\n",
    "redshift_mags_errors = redshift_features + redshift_uncertainties\n",
    "\n",
    "# 5. ALHAMBRA Quality/Auxiliary Features (per-band quality etc.)\n",
    "OPTICAL_IRMS_COLS = [\n",
    "    'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W',\n",
    "    'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W',\n",
    "    'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W',\n",
    "    'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W'\n",
    "]\n",
    "quality_aux_features = (\n",
    "    ['nfobs'] +\n",
    "    OPTICAL_IRMS_COLS +\n",
    "    ['irms_J', 'irms_H', 'irms_KS', 'irms_F814W']\n",
    ")\n",
    "\n",
    "# --- Define lists of features NOT used for modeling ---\n",
    "\n",
    "non_modeling_identifiers = ['ID_1', 'id_2'] # ALHAMBRA ID, COSMOS ID\n",
    "\n",
    "non_modeling_astrometry = [\n",
    "    'RA_1', 'Dec_1', 'x', 'y', # ALHAMBRA Astrometry\n",
    "    'ra_2', 'dec_2',          # COSMOS Astrometry\n",
    "    'Separation'              # Matching Quality\n",
    "]\n",
    "\n",
    "non_modeling_flags = [\n",
    "    'photoflag', 'xray', 'PercW', 'Satur_Flag', # ALHAMBRA Object/Photometry Flags\n",
    "    'irms_OPT_Flag', 'irms_NIR_Flag'           # ALHAMBRA Overall Quality Flags\n",
    "]\n",
    "\n",
    "alhambra_prediction = ['Stellar_Flag'] # ALHAMBRA's own classification\n",
    "\n",
    "non_modeling_aperture_mags = [ # Specific aperture mags, usually use total mags\n",
    "    'F814W_3arcs', 'dF814W_3arcs', 'F814W_3arcs_corr'\n",
    "]\n",
    "\n",
    "non_modeling_cosmos_features = [ # Measurements/flags derived from COSMOS data (HST, HSC, VISTA...)\n",
    "    'model_flag',\n",
    "    'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista',\n",
    "    'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid',\n",
    "    'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid',\n",
    "    'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid',\n",
    "    'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid',\n",
    "    'acs_f814w_mag', 'acs_f814w_magerr',\n",
    "    'acs_fwhm_world', 'acs_mu_max',\n",
    "    'solution_model' # This is categorical, but still COSMOS-derived info\n",
    "]\n",
    "\n",
    "target_variable = ['acs_mu_class'] # The COSMOS classification label to predict\n",
    "\n",
    "##########################################################################################\n",
    "#! --- Consolidate into the main dictionary for easy access ---\n",
    "##########################################################################################\n",
    "\n",
    "feature_sets = {\n",
    "        # --- Potential Input Feature Sets ---\n",
    "        'morphology_only': morphology_mags_errors,\n",
    "        'photometry_magnitudes_only': photometry_magnitudes,\n",
    "        'photometry_mags_errors': photometry_mags_errors,\n",
    "        'photometry_plus_morphology': photometry_mags_errors + morphology_mags_errors,\n",
    "        'photometry_no_redshift': photometry_mags_errors + morphology_mags_errors + quality_aux_features,\n",
    "        'redshift_only': redshift_mags_errors,\n",
    "        'full_alhambra_all': (morphology_mags_errors +\n",
    "                            photometry_mags_errors +\n",
    "                            redshift_mags_errors + \n",
    "                            quality_aux_features),\n",
    "\n",
    "        # --- Excluded Feature Sets ---\n",
    "        'non_modeling_identifiers': non_modeling_identifiers,\n",
    "        'non_modeling_astrometry': non_modeling_astrometry,\n",
    "        'non_modeling_flags': non_modeling_flags,\n",
    "        'non_modeling_aperture_mags': non_modeling_aperture_mags,\n",
    "        'non_modeling_cosmos_features': non_modeling_cosmos_features,\n",
    "        'alhambra_prediction': alhambra_prediction,\n",
    "        'target_variable': target_variable\n",
    "    }\n",
    "\n",
    "#! This is excluding the quality aux.\n",
    "# Include target_variable in each group by appending it to the feature list\n",
    "groups = {\n",
    "        'group_1': feature_sets.get('morphology_only', []) + feature_sets.get('target_variable', []),\n",
    "        'group_2': feature_sets.get('photometry_magnitudes_only', []) + feature_sets.get('target_variable', []),\n",
    "        'group_3': feature_sets.get('photometry_mags_errors', []) + feature_sets.get('target_variable', []),\n",
    "        'group_4': feature_sets.get('redshift_only', []) + feature_sets.get('target_variable', []),\n",
    "        'group_5': feature_sets.get('photometry_plus_morphology', []) + feature_sets.get('target_variable', []),\n",
    "        'group_6': (feature_sets.get('photometry_mags_errors', []) +\n",
    "                   feature_sets.get('morphology_only', []) +\n",
    "                   feature_sets.get('redshift_only', []) +\n",
    "                   feature_sets.get('target_variable', [])),\n",
    "        'group_7': feature_sets.get('full_alhambra_all', []) + feature_sets.get('target_variable', [])\n",
    "    }\n",
    "\n",
    "# --- Function to get a specific feature set (Unchanged from before) ---\n",
    "\n",
    "def get_feature_set(df, set_name, groups = groups):\n",
    "    \"\"\"\n",
    "    Selects columns from a DataFrame based on a predefined feature set name,\n",
    "    including six specific groups defined by combinations of morphology,\n",
    "    photometry magnitudes, uncertainties, and redshift features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        set_name (str): The name of the desired feature set group:\n",
    "                        'group_1' to 'group_6' as defined below.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing only the columns\n",
    "                      belonging to the specified feature set group.\n",
    "                      Returns an empty DataFrame if no columns are found.\n",
    "    \"\"\"\n",
    "\n",
    "    if set_name not in groups:\n",
    "        raise ValueError(f\"Feature set group '{set_name}' not defined. \"\n",
    "                         f\"Available groups: {list(groups.keys())}\")\n",
    "\n",
    "    required_cols_in_set = groups[set_name]\n",
    "\n",
    "    # Find which of these columns actually exist in the DataFrame\n",
    "    available_cols = [col for col in required_cols_in_set if col in df.columns]\n",
    "\n",
    "    # Warn if some columns from the set definition are missing\n",
    "    missing_cols = [col for col in required_cols_in_set if col not in available_cols]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: The following columns defined for feature set group '{set_name}'\"\n",
    "              f\" were not found in the DataFrame and will be excluded: {missing_cols}\")\n",
    "\n",
    "    if not available_cols:\n",
    "        print(f\"Warning: No columns for feature set group '{set_name}' found in the DataFrame.\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "\n",
    "    print(f\"Selecting feature set group '{set_name}' with {len(available_cols)} columns.\")\n",
    "    return df[available_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All df columns are included in feature_sets.\n",
      "\n",
      "=== group_1 ===\n",
      "Selecting feature set group 'group_1' with 11 columns.\n",
      "\n",
      "Features present (11 columns):\n",
      "['a', 'acs_mu_class', 'area', 'b', 'ell', 'fwhm', 'rf', 'rk', 's2n', 'stell', 'theta']\n",
      "\n",
      "Features missing (125 columns):\n",
      "['Chi2', 'Dec_1', 'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F814W_3arcs', 'F814W_3arcs_corr', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'ID_1', 'J', 'KS', 'M_Abs_1', 'MagPrior', 'Odds_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stell_Mass_1', 'Stellar_Flag', 'Tb_1', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF814W_3arcs', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'dec_2', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'solution_model', 't_ml', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "=== group_2 ===\n",
      "Selecting feature set group 'group_2' with 25 columns.\n",
      "\n",
      "Features present (25 columns):\n",
      "['F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'acs_mu_class']\n",
      "\n",
      "Features missing (111 columns):\n",
      "['Chi2', 'Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'M_Abs_1', 'MagPrior', 'Odds_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stell_Mass_1', 'Stellar_Flag', 'Tb_1', 'a', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF814W_3arcs', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'dec_2', 'ell', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'fwhm', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'rf', 'rk', 's2n', 'solution_model', 'stell', 't_ml', 'theta', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "=== group_3 ===\n",
      "Selecting feature set group 'group_3' with 49 columns.\n",
      "\n",
      "Features present (49 columns):\n",
      "['F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'acs_mu_class', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS']\n",
      "\n",
      "Features missing (87 columns):\n",
      "['Chi2', 'Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'M_Abs_1', 'MagPrior', 'Odds_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stell_Mass_1', 'Stellar_Flag', 'Tb_1', 'a', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'area', 'b', 'dF814W_3arcs', 'dec_2', 'ell', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'fwhm', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'rf', 'rk', 's2n', 'solution_model', 'stell', 't_ml', 'theta', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "=== group_4 ===\n",
      "Selecting feature set group 'group_4' with 12 columns.\n",
      "\n",
      "Features present (12 columns):\n",
      "['Chi2', 'M_Abs_1', 'MagPrior', 'Odds_1', 'Stell_Mass_1', 'Tb_1', 'acs_mu_class', 't_ml', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "Features missing (124 columns):\n",
      "['Dec_1', 'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F814W_3arcs', 'F814W_3arcs_corr', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'ID_1', 'J', 'KS', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stellar_Flag', 'a', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF814W_3arcs', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'dec_2', 'ell', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'fwhm', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'rf', 'rk', 's2n', 'solution_model', 'stell', 'theta', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y']\n",
      "\n",
      "=== group_5 ===\n",
      "Selecting feature set group 'group_5' with 59 columns.\n",
      "\n",
      "Features present (59 columns):\n",
      "['F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'a', 'acs_mu_class', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'ell', 'fwhm', 'rf', 'rk', 's2n', 'stell', 'theta']\n",
      "\n",
      "Features missing (77 columns):\n",
      "['Chi2', 'Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'M_Abs_1', 'MagPrior', 'Odds_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stell_Mass_1', 'Stellar_Flag', 'Tb_1', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'dF814W_3arcs', 'dec_2', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'solution_model', 't_ml', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "=== group_6 ===\n",
      "Selecting feature set group 'group_6' with 70 columns.\n",
      "\n",
      "Features present (70 columns):\n",
      "['Chi2', 'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'M_Abs_1', 'MagPrior', 'Odds_1', 'Stell_Mass_1', 'Tb_1', 'a', 'acs_mu_class', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'ell', 'fwhm', 'rf', 'rk', 's2n', 'stell', 't_ml', 'theta', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "Features missing (66 columns):\n",
      "['Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stellar_Flag', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'dF814W_3arcs', 'dec_2', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'solution_model', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y']\n",
      "\n",
      "=== group_7 ===\n",
      "Selecting feature set group 'group_7' with 95 columns.\n",
      "\n",
      "Features present (95 columns):\n",
      "['Chi2', 'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'M_Abs_1', 'MagPrior', 'Odds_1', 'Stell_Mass_1', 'Tb_1', 'a', 'acs_mu_class', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'ell', 'fwhm', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'nfobs', 'rf', 'rk', 's2n', 'stell', 't_ml', 'theta', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "Features missing (41 columns):\n",
      "['Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stellar_Flag', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'dF814W_3arcs', 'dec_2', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'photoflag', 'ra_2', 'solution_model', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y']\n"
     ]
    }
   ],
   "source": [
    "# Quality check to see which cols are excluded and contained in each group\n",
    "all_feature_cols = set()\n",
    "for cols in feature_sets.values():\n",
    "    all_feature_cols.update(cols)\n",
    "\n",
    "df_cols_set = set(df.columns)\n",
    "not_in_feature_sets = df_cols_set - all_feature_cols\n",
    "\n",
    "if not_in_feature_sets:\n",
    "    print(f\"Columns in df not included in any feature_sets: {sorted(not_in_feature_sets)}\")\n",
    "else:\n",
    "    print(\"All df columns are included in feature_sets.\")\n",
    "\n",
    "\n",
    "# Check which columns are in each feature group\n",
    "for group_name in ['group_1', 'group_2', 'group_3', 'group_4', 'group_5', 'group_6', 'group_7']:\n",
    "    print(f\"\\n=== {group_name} ===\")\n",
    "    \n",
    "    # Get the feature set definition\n",
    "    feature_set = groups[group_name]\n",
    "    \n",
    "    # Get the actual columns that exist in the data\n",
    "    group_df = get_feature_set(df, group_name)\n",
    "    \n",
    "\n",
    "    available_cols = list(group_df.columns)\n",
    "    \n",
    "    # Find columns that are defined but not in the data\n",
    "    missing_cols = [col for col in list(df.columns) if col not in feature_set]\n",
    "    \n",
    "    print(f\"\\nFeatures present ({len(available_cols)} columns):\")\n",
    "    print(list(sorted(available_cols)))\n",
    "    \n",
    "    print(f\"\\nFeatures missing ({len(missing_cols)} columns):\")\n",
    "    print(list(sorted(missing_cols)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting parameters\n",
    "TEST_SIZE = 0.20 # Test set proportion\n",
    "VAL_SIZE = 0.10 # Validation set proportion\n",
    "CAL_SIZE = 0.20 # Calibration set proportion\n",
    "# Train size will be 1 - (TEST_SIZE + VAL_SIZE + CAL_SIZE)\n",
    "\n",
    "TARGET_COLUMN = feature_sets.get('target_variable', [])[0]\n",
    "RANDOM_SEED = 33 # For reproducibility\n",
    "\n",
    "# Model saving directory\n",
    "MODEL_DIR = \"trained_models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Data splitting strategy ('stratified' or 'random')\n",
    "SPLIT_STRATEGY = 'stratified' # Recommended for imbalanced datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting feature set group 'group_7' with 95 columns.\n"
     ]
    }
   ],
   "source": [
    "# --- Data Cleaning ---\n",
    "logging.info(f\"Original dataset size: {df.shape}\")\n",
    "\n",
    "# Choose the feature group to use (e.g., 'group_1', 'group_2', etc.)\n",
    "FEATURE_GROUP = 'group_7'  # Change this to select a different group\n",
    "\n",
    "# Get the feature columns for the selected group using get_feature_set\n",
    "df_clean = get_feature_set(df, FEATURE_GROUP).dropna().copy()\n",
    "logging.info(f\"Dataset size after dropping NaNs: {df_clean.shape}\")\n",
    "\n",
    "# Ensure TARGET_COLUMN is defined correctly\n",
    "if TARGET_COLUMN not in df_clean.columns:\n",
    "    raise KeyError(f\"Target column '{TARGET_COLUMN}' not found in the cleaned DataFrame columns: {df_clean.columns.tolist()}\")\n",
    "\n",
    "# Log value counts for target\n",
    "logging.info(f\"Value counts for target:\\n1 (Star): {(df_clean[TARGET_COLUMN] == 1).sum()}\\n0 (Galaxy): {(df_clean[TARGET_COLUMN] == 0).sum()}\")\n",
    "\n",
    "# Separate features (X) and target (y) for the cleaned DataFrame\n",
    "X = df_clean.drop(columns=[TARGET_COLUMN])\n",
    "y = df_clean[TARGET_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Splitting ---\n",
    "import numpy as np # Ensure numpy is imported\n",
    "from sklearn.model_selection import train_test_split # Ensure train_test_split is imported\n",
    "\n",
    "logging.info(f\"Splitting data using '{SPLIT_STRATEGY}' strategy...\")\n",
    "\n",
    "# --- Validate Proportions ---\n",
    "if not (0 <= TEST_SIZE <= 1 and 0 <= VAL_SIZE <= 1 and 0 <= CAL_SIZE <= 1):\n",
    "     raise ValueError(\"Split proportions (TEST_SIZE, VAL_SIZE, CAL_SIZE) must be between 0 and 1.\")\n",
    "\n",
    "TRAIN_SIZE = 1.0 - TEST_SIZE - VAL_SIZE - CAL_SIZE\n",
    "if not (0 <= TRAIN_SIZE <= 1):\n",
    "     raise ValueError(f\"Calculated TRAIN_SIZE ({TRAIN_SIZE:.3f}) is invalid. Sum of TEST_SIZE, VAL_SIZE, and CAL_SIZE must be between 0 and 1.\")\n",
    "\n",
    "if not np.isclose(TRAIN_SIZE + TEST_SIZE + VAL_SIZE + CAL_SIZE, 1.0):\n",
    "    # This check might be redundant given the calculation of TRAIN_SIZE, but good for safety.\n",
    "    raise ValueError(\"Sum of split proportions must be equal to 1.\")\n",
    "\n",
    "if np.isclose(TRAIN_SIZE, 0) and (np.isclose(VAL_SIZE, 0) or np.isclose(TEST_SIZE, 0) or np.isclose(CAL_SIZE, 0)):\n",
    "     # Avoid scenarios where train is 0 but other splits are also 0, leading to ambiguity.\n",
    "     # If only train is 0, it might be valid in some rare cases, but usually requires at least one other non-zero split.\n",
    "     # Let's enforce Train > 0 for typical ML workflows.\n",
    "     # If you need zero training data, adjust this check.\n",
    "     logging.warning(\"TRAIN_SIZE is zero or near zero. Ensure this is intended.\")\n",
    "     if TRAIN_SIZE < 0: # Definitely an error\n",
    "         raise ValueError(\"TRAIN_SIZE cannot be negative.\")\n",
    "     # Allow TRAIN_SIZE = 0 only if explicitly handled later, otherwise raise error?\n",
    "     # For now, let's proceed but log a warning. If TRAIN_SIZE must be > 0, uncomment the raise below.\n",
    "     # raise ValueError(\"TRAIN_SIZE must be greater than 0 for typical model training.\")\n",
    "\n",
    "\n",
    "logging.info(f\"Target split ratios: Train={TRAIN_SIZE:.2f}, Val={VAL_SIZE:.2f}, Test={TEST_SIZE:.2f}, Cal={CAL_SIZE:.2f}\")\n",
    "\n",
    "# --- Initialize Splits ---\n",
    "# Use iloc[0:0] to create empty DataFrames/Series with the same columns/dtype\n",
    "empty_X = X.iloc[0:0]\n",
    "empty_y = y.iloc[0:0]\n",
    "X_train, y_train = empty_X.copy(), empty_y.copy()\n",
    "X_val, y_val = empty_X.copy(), empty_y.copy()\n",
    "X_test, y_test = empty_X.copy(), empty_y.copy()\n",
    "X_cal, y_cal = empty_X.copy(), empty_y.copy()\n",
    "\n",
    "# Temporary variables for sequential splitting\n",
    "X_remaining, y_remaining = X.copy(), y.copy() # Use copies to avoid modifying original X, y\n",
    "\n",
    "# --- Stratification Option ---\n",
    "# Define stratify_func only once\n",
    "def get_stratify_array(y_arr):\n",
    "    return y_arr if SPLIT_STRATEGY == 'stratified' and not y_arr.empty else None\n",
    "\n",
    "# --- First Split: Train vs. Remainder (Val + Test + Cal) ---\n",
    "val_test_cal_size = VAL_SIZE + TEST_SIZE + CAL_SIZE\n",
    "\n",
    "if np.isclose(val_test_cal_size, 0): # Only Train set needed\n",
    "    X_train, y_train = X_remaining, y_remaining\n",
    "    logging.info(\"All data assigned to Train set (Val, Test, Cal sizes are 0).\")\n",
    "    X_remaining, y_remaining = empty_X.copy(), empty_y.copy() # No remainder\n",
    "elif np.isclose(TRAIN_SIZE, 0): # No Train set needed\n",
    "    logging.info(\"Train set is empty (TRAIN_SIZE=0). Remainder passed to next splits.\")\n",
    "    # X_remaining, y_remaining already hold all data\n",
    "else: # Split Train vs Remainder\n",
    "    split_test_size = val_test_cal_size # Proportion of remainder relative to total (1.0)\n",
    "    X_train, X_remaining, y_train, y_remaining = train_test_split(\n",
    "        X_remaining, y_remaining,\n",
    "        test_size=split_test_size,\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=get_stratify_array(y_remaining)\n",
    "    )\n",
    "logging.info(f\"Train set shape: {X_train.shape}\")\n",
    "\n",
    "\n",
    "# --- Second Split: Val vs. Remainder (Test + Cal) ---\n",
    "if not X_remaining.empty:\n",
    "    test_cal_size = TEST_SIZE + CAL_SIZE\n",
    "    # Denominator for relative size calculation: size of the current remaining pool\n",
    "    current_remaining_size_frac = VAL_SIZE + test_cal_size # = val_test_cal_size\n",
    "\n",
    "    if np.isclose(VAL_SIZE, 0): # No Val set, pass remainder to next stage\n",
    "        X_temp2, y_temp2 = X_remaining, y_remaining # Remainder is Test + Cal\n",
    "        logging.info(\"Validation set is empty (VAL_SIZE=0).\")\n",
    "    elif np.isclose(test_cal_size, 0): # Only Val set left in remainder\n",
    "        X_val, y_val = X_remaining, y_remaining\n",
    "        X_temp2, y_temp2 = empty_X.copy(), empty_y.copy() # No data left for Test/Cal\n",
    "        logging.info(f\"Validation set shape: {X_val.shape}\")\n",
    "    else: # Split Val vs (Test + Cal)\n",
    "        # Proportion of (Test + Cal) relative to (Val + Test + Cal)\n",
    "        split_test_size = test_cal_size / current_remaining_size_frac\n",
    "        X_val, X_temp2, y_val, y_temp2 = train_test_split(\n",
    "            X_remaining, y_remaining,\n",
    "            test_size=split_test_size,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=get_stratify_array(y_remaining)\n",
    "        )\n",
    "        logging.info(f\"Validation set shape: {X_val.shape}\")\n",
    "else: # No data remaining after train split\n",
    "    X_temp2, y_temp2 = empty_X.copy(), empty_y.copy()\n",
    "    if not np.isclose(VAL_SIZE, 0): # Log only if Val set was expected\n",
    "       logging.info(\"Validation set is empty (no data remaining after train split).\")\n",
    "\n",
    "\n",
    "# --- Third Split: Test vs. Cal ---\n",
    "if not X_temp2.empty:\n",
    "    # Denominator for relative size calculation: size of the current remaining pool\n",
    "    current_remaining_size_frac = TEST_SIZE + CAL_SIZE # = test_cal_size\n",
    "\n",
    "    if np.isclose(CAL_SIZE, 0): # No Cal set, remainder is Test\n",
    "        X_test, y_test = X_temp2, y_temp2\n",
    "        logging.info(\"Calibration set is empty (CAL_SIZE=0).\")\n",
    "    elif np.isclose(TEST_SIZE, 0): # Only Cal set left in remainder\n",
    "        X_cal, y_cal = X_temp2, y_temp2\n",
    "        logging.info(\"Test set is empty (TEST_SIZE=0).\")\n",
    "    else: # Split Test vs Cal\n",
    "        # Proportion of Cal relative to (Test + Cal)\n",
    "        split_test_size = CAL_SIZE / current_remaining_size_frac\n",
    "        X_test, X_cal, y_test, y_cal = train_test_split(\n",
    "            X_temp2, y_temp2,\n",
    "            test_size=split_test_size,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=get_stratify_array(y_temp2)\n",
    "        )\n",
    "        # Logging shapes done after the if/else block\n",
    "else: # No data remaining for Test/Cal split\n",
    "    if not (np.isclose(TEST_SIZE, 0) and np.isclose(CAL_SIZE, 0)): # Log only if Test or Cal were expected\n",
    "        logging.info(\"Test and Calibration sets are empty (no data remaining for final split).\")\n",
    "\n",
    "# Log final shapes for Test and Cal\n",
    "logging.info(f\"Test set shape: {X_test.shape}\")\n",
    "logging.info(f\"Calibration set shape: {X_cal.shape}\")\n",
    "\n",
    "\n",
    "# --- Verification and Final Logging ---\n",
    "total_len = len(X_train) + len(X_val) + len(X_test) + len(X_cal)\n",
    "original_len = len(X)\n",
    "\n",
    "if total_len != original_len:\n",
    "     # Calculate actual proportions based on lengths\n",
    "     actual_train = len(X_train) / original_len if original_len > 0 else 0\n",
    "     actual_val = len(X_val) / original_len if original_len > 0 else 0\n",
    "     actual_test = len(X_test) / original_len if original_len > 0 else 0\n",
    "     actual_cal = len(X_cal) / original_len if original_len > 0 else 0\n",
    "     logging.warning(f\"Total split length ({total_len}) does not exactly match original length ({original_len}). \"\n",
    "                     f\"This can happen with stratification or rounding. \"\n",
    "                     f\"Target proportions: Train={TRAIN_SIZE:.3f}, Val={VAL_SIZE:.3f}, Test={TEST_SIZE:.3f}, Cal={CAL_SIZE:.3f}. \"\n",
    "                     f\"Actual proportions: Train={actual_train:.3f}, Val={actual_val:.3f}, Test={actual_test:.3f}, Cal={actual_cal:.3f}\")\n",
    "else:\n",
    "    logging.info(\"Split lengths verification successful.\")\n",
    "\n",
    "logging.info(\"Data splitting complete.\")\n",
    "\n",
    "# Log distributions, handling empty sets\n",
    "def log_distribution(name, y_set):\n",
    "    if y_set.empty:\n",
    "        logging.info(f\"{name} target distribution: Set is empty.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Use normalize=True, handle potential division by zero if counts are zero (though unlikely if not empty)\n",
    "            counts = y_set.value_counts()\n",
    "            dist = counts / counts.sum() if counts.sum() > 0 else counts\n",
    "            logging.info(f\"{name} target distribution:\\n{dist}\")\n",
    "            # Log absolute counts as well for clarity\n",
    "            logging.info(f\"{name} target counts:\\n{counts}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Could not calculate distribution for {name}: {e}\")\n",
    "            # Attempt to log raw value counts even if normalization fails\n",
    "            try:\n",
    "                logging.info(f\"{name} raw value counts:\\n{y_set.value_counts()}\")\n",
    "            except Exception as e_raw:\n",
    "                 logging.error(f\"Could not get raw value counts for {name}: {e_raw}\")\n",
    "\n",
    "\n",
    "log_distribution(\"Train\", y_train)\n",
    "log_distribution(\"Validation\", y_val)\n",
    "log_distribution(\"Test\", y_test)\n",
    "log_distribution(\"Calibration\", y_cal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization via Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler, train_test_split\n",
    "from sklearn.metrics import f1_score # Default scorer\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "\n",
    "# --- Internal Helper ---\n",
    "def _train_and_eval(model_class, params,\n",
    "                    X_train, y_train, X_val, y_val,\n",
    "                    resource, resource_type,\n",
    "                    scoring_func, random_state):\n",
    "    \"\"\"Internal helper function to train and evaluate a single configuration.\"\"\"\n",
    "    try:\n",
    "        # Instantiate the base model without iteration-specific params first\n",
    "        # Iteration param (e.g., n_estimators) will be handled later if needed\n",
    "        model = model_class(**params)\n",
    "\n",
    "        fit_duration = 0.0\n",
    "        eval_duration = 0.0\n",
    "        start_fit = time.time() # Start timing fit process\n",
    "\n",
    "        if resource_type == 'data_fraction':\n",
    "            # --- FIX 1: Implement data subsetting ---\n",
    "            if resource < 1.0:\n",
    "                # Use train_test_split to get a stratified fraction\n",
    "                # We only need the 'train' part of this split for the subset\n",
    "                try:\n",
    "                    X_subset, _, y_subset, _ = train_test_split(\n",
    "                        X_train, y_train,\n",
    "                        train_size=resource,\n",
    "                        random_state=random_state, # Use provided random state\n",
    "                        stratify=y_train # Stratify based on original train labels\n",
    "                    )\n",
    "                except ValueError as e:\n",
    "                    # Handle cases where stratification is not possible (e.g., too few samples)\n",
    "                    logging.warning(f\"Stratification failed for resource {resource:.2f}: {e}. Falling back to non-stratified split.\")\n",
    "                    X_subset, _, y_subset, _ = train_test_split(\n",
    "                        X_train, y_train,\n",
    "                        train_size=resource,\n",
    "                        random_state=random_state\n",
    "                    )\n",
    "            else:\n",
    "                # Use the full training data if resource is 1.0\n",
    "                X_subset, y_subset = X_train, y_train\n",
    "\n",
    "            # Ensure y_subset is numpy for fitting if needed by model\n",
    "            y_subset_np = y_subset.values if isinstance(y_subset, pd.Series) else y_subset\n",
    "\n",
    "            # Fit the model \n",
    "            model.fit(X_subset, y_subset_np)\n",
    "            fit_duration = time.time() - start_fit\n",
    "            # -----------------------------------------\n",
    "\n",
    "        elif resource_type == 'iterations':\n",
    "            # Resource represents n_estimators or similar iteration parameter\n",
    "            params_iter = params.copy() # Avoid modifying original params dict\n",
    "            iter_param_name = 'n_estimators' # Common case for RF, XGB, LGBM\n",
    "\n",
    "            # Ensure resource is an integer for iterations\n",
    "            params_iter[iter_param_name] = int(max(1, resource)) # Ensure at least 1 iteration\n",
    "            model = model_class(**params_iter) # Re-instantiate with correct n_estimators\n",
    "\n",
    "            # --- FIX 2 & 3: Conditional Fit Parameters ---\n",
    "            current_fit_args = {} # Dictionary for specific fit arguments\n",
    "            eval_set_for_fit = [(X_val, y_val)] # Common eval set\n",
    "\n",
    "            if model_class is xgb.XGBClassifier:\n",
    "                current_fit_args['eval_set'] = eval_set_for_fit\n",
    "                current_fit_args['verbose'] = False\n",
    "\n",
    "            elif model_class is lgb.LGBMClassifier:\n",
    "                current_fit_args['eval_set'] = eval_set_for_fit\n",
    "                if 'metric' in params_iter: # Get metric from HPO params\n",
    "                     current_fit_args['eval_metric'] = params_iter['metric']\n",
    "                elif isinstance(model.metric, str): # Get metric from model instance if set\n",
    "                     current_fit_args['eval_metric'] = model.metric\n",
    "                else: # Default if not found (might cause issues if early stopping expects it)\n",
    "                     logging.warning(f\"LGBM eval_metric not found in HPO params or model instance for config {params_iter}. Early stopping might fail.\")\n",
    "                     # You might need to add a default like 'logloss' or raise an error\n",
    "                     # current_fit_args['eval_metric'] = 'logloss' # Example default\n",
    "\n",
    "            # For models like RandomForest or DecisionTree, current_fit_args remains empty {}\n",
    "            # as they don't use eval_set or callbacks in their standard fit method\n",
    "\n",
    "            # Fit the model with appropriate arguments\n",
    "            # Ensure y_train is numpy if needed\n",
    "            y_train_np = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "            try:\n",
    "                model.fit(X_train, y_train_np, **current_fit_args)\n",
    "            except Exception as fit_error:\n",
    "                 logging.error(f\"Fit failed for config {params_iter} with resource {resource}: {fit_error}\")\n",
    "                 # logging.exception(\"Fit Traceback:\") # Uncomment for full traceback\n",
    "                 return -1.0 # Indicate failure\n",
    "            fit_duration = time.time() - start_fit\n",
    "            # -----------------------------------------\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid resource_type. Choose 'data_fraction' or 'iterations'.\")\n",
    "\n",
    "        # Evaluate on the full validation set (common part)\n",
    "        start_eval = time.time()\n",
    "        try:\n",
    "             y_pred_val = model.predict(X_val)\n",
    "             # Ensure y_val is numpy if needed by scoring_func\n",
    "             y_val_np = y_val.values if isinstance(y_val, pd.Series) else y_val\n",
    "             score = scoring_func(y_val_np, y_pred_val)\n",
    "        except Exception as eval_error:\n",
    "             logging.error(f\"Predict/Score failed for config {params} with resource {resource}: {eval_error}\")\n",
    "             score = -1.0 # Indicate failure\n",
    "        eval_duration = time.time() - start_eval\n",
    "\n",
    "        logging.debug(f\"Evaluated config: {params} | Resource: {resource:.2f} | Score: {score:.4f} | Fit: {fit_duration:.2f}s | Eval: {eval_duration:.2f}s\")\n",
    "        return score\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the configuration that caused the error\n",
    "        logging.error(f\"Error training/evaluating config {params} with resource {resource}: {e}\", exc_info=False) # Set exc_info=True for traceback if needed\n",
    "        return -1.0 # Return a clearly bad score\n",
    "\n",
    "\n",
    "def hyperband_hpo(model_class, param_space,\n",
    "                  X_train, y_train, X_val, y_val,\n",
    "                  max_resource, eta=3, resource_type='iterations',\n",
    "                  min_resource=1, # Min iterations or min data fraction\n",
    "                  scoring_func=f1_score, # Function accepting (y_true, y_pred)\n",
    "                  random_state=None): # For early stopping etc. passed to .fit()\n",
    "    \"\"\"\n",
    "    Performs Hyperband Hyperparameter Optimization.\n",
    "\n",
    "    Args:\n",
    "        model_class: The model class (e.g., SVC, RandomForestClassifier).\n",
    "        param_space (dict): Dictionary defining the hyperparameter search space\n",
    "                           compatible with ParameterSampler.\n",
    "        X_train, y_train: Training data and labels.\n",
    "        X_val, y_val: Validation data and labels for evaluation.\n",
    "        max_resource (float/int): Maximum resource allocation\n",
    "                                 (e.g., max n_estimators or 1.0 for data fraction).\n",
    "        eta (int): Reduction factor for successive halving (>= 2).\n",
    "        resource_type (str): How resource is allocated:\n",
    "                             'iterations' -> resource sets n_estimators (or similar).\n",
    "                             'data_fraction' -> resource is fraction of training data used (stratified).\n",
    "        min_resource (float/int): Minimum resource for the first iteration.\n",
    "                                 Must be >= 1 for 'iterations', > 0 for 'data_fraction'.\n",
    "        scoring_func (callable): Function to evaluate performance (e.g., f1_score).\n",
    "                                Higher score is assumed better.\n",
    "        random_state (int): Seed for reproducibility of parameter sampling and data subsetting.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_params, best_score)\n",
    "               best_params (dict): The hyperparameters of the best performing configuration.\n",
    "               best_score (float): The score achieved by the best configuration on the validation set\n",
    "                                  using the maximum resource.\n",
    "    \"\"\"\n",
    "\n",
    "    log_max_r = math.log(max_resource / min_resource, eta) if max_resource > min_resource and min_resource > 0 else 0\n",
    "    s_max = int(log_max_r)\n",
    "    B = (s_max + 1) * max_resource # Approximate total resource budget\n",
    "\n",
    "    logging.info(f\"--- Starting Hyperband HPO ---\")\n",
    "    logging.info(f\"Model: {model_class.__name__}\")\n",
    "    logging.info(f\"Resource Type: {resource_type}\")\n",
    "    logging.info(f\"Resource Range: [{min_resource}, {max_resource}]\")\n",
    "    logging.info(f\"Eta: {eta}\")\n",
    "    logging.info(f\"Max Brackets (s_max): {s_max}\")\n",
    "    logging.info(f\"Approx. Budget (B): {B:.2f}\")\n",
    "    logging.info(f\"Scoring: {scoring_func.__name__}\")\n",
    "\n",
    "    best_params = None\n",
    "    best_score = -1.0\n",
    "    total_configs_evaluated = 0\n",
    "    outer_tqdm = tqdm(range(s_max, -1, -1), desc=\"Hyperband Brackets (s)\")\n",
    "\n",
    "    # Outer loop: Iterate through brackets (s values)\n",
    "    for s in outer_tqdm:\n",
    "        n_configs = int(math.ceil(int(B / max_resource / (s + 1)) * eta**s)) # Number of configs in this bracket\n",
    "        r_initial = max_resource * eta**(-s) # Initial resource for this bracket\n",
    "        # Ensure initial resource is not less than min_resource\n",
    "        r_initial = max(r_initial, min_resource)\n",
    "\n",
    "        outer_tqdm.set_description(f\"Bracket s={s} (n={n_configs}, r0={r_initial:.2f})\")\n",
    "        logging.info(f\"\\n>> Bracket s={s}: n_configs={n_configs}, r_initial={r_initial:.2f}\")\n",
    "\n",
    "        # Sample configurations for this bracket\n",
    "        param_list = list(ParameterSampler(param_space, n_iter=n_configs, random_state=random_state + s if random_state is not None else None))\n",
    "        \n",
    "        # --- Add common fixed parameters ---\n",
    "        # Calculate scale_pos_weight once if needed\n",
    "        scale_pos_weight_val = None\n",
    "        if model_class in [xgb.XGBClassifier, lgb.LGBMClassifier]:\n",
    "             neg_count = (y_train == 0).sum()\n",
    "             pos_count = (y_train == 1).sum()\n",
    "             if pos_count > 0:\n",
    "                 scale_pos_weight_val = neg_count / pos_count\n",
    "\n",
    "        for p in param_list:\n",
    "             # Add random_state if model supports it and it's not sampled\n",
    "             if 'random_state' not in p and hasattr(model_class(random_state=1), 'random_state'): # Check if attr exists\n",
    "                 p['random_state'] = random_state\n",
    "             # Add class_weight='balanced' for relevant sklearn models if not sampled\n",
    "             if model_class in [SVC, RandomForestClassifier, DecisionTreeClassifier] and 'class_weight' not in p:\n",
    "                 p['class_weight'] = 'balanced'\n",
    "             # Add scale_pos_weight for boosting models if not sampled and calculated\n",
    "             if model_class in [xgb.XGBClassifier, lgb.LGBMClassifier] and 'scale_pos_weight' not in p and scale_pos_weight_val is not None:\n",
    "                  p['scale_pos_weight'] = scale_pos_weight_val\n",
    "             # For LightGBM, also consider adding 'objective': 'binary' if not sampled\n",
    "             if model_class is lgb.LGBMClassifier and 'objective' not in p:\n",
    "                  p['objective'] = 'binary'\n",
    "        # -----------------------------------\n",
    "\n",
    "        # Inner loop: Successive halving rounds\n",
    "        inner_tqdm = tqdm(range(s + 1), desc=f\"SH Round (s={s})\", leave=False)\n",
    "        for i in inner_tqdm:\n",
    "            current_resource = r_initial * eta**i\n",
    "            # Ensure resource doesn't exceed max_resource due to floating point/rounding\n",
    "            current_resource = min(current_resource, max_resource)\n",
    "\n",
    "            n_configs_in_round = len(param_list)\n",
    "            inner_tqdm.set_description(f\"SH Round i={i} (n={n_configs_in_round}, r={current_resource:.2f})\")\n",
    "            logging.info(f\"  -- Round i={i}: Evaluating {n_configs_in_round} configs with resource={current_resource:.2f} --\")\n",
    "\n",
    "            round_scores = []\n",
    "            # Use tqdm for the configurations within the round\n",
    "            eval_tqdm = tqdm(param_list, desc=f\"Evaluating Configs (i={i})\", leave=False)\n",
    "            for params in eval_tqdm:\n",
    "                score = _train_and_eval(model_class, params, X_train, y_train, X_val, y_val,\n",
    "                                        current_resource, resource_type, scoring_func,\n",
    "                                        random_state)\n",
    "                round_scores.append((score, params))\n",
    "                total_configs_evaluated += 1 # Count unique evaluations\n",
    "\n",
    "            # Sort by score (descending, higher is better)\n",
    "            round_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            # Track the best overall score and params seen so far *at max resource*\n",
    "            # Only update if we are actually at max resource in this round\n",
    "            if abs(current_resource - max_resource) < 1e-6: # Check if we are at max resource\n",
    "                 if round_scores and round_scores[0][0] > best_score:\n",
    "                      best_score = round_scores[0][0]\n",
    "                      best_params = round_scores[0][1]\n",
    "                      logging.info(f\"  ** New Best Found (Score: {best_score:.4f}) at max resource ** Params: {best_params}\")\n",
    "                      # Update outer tqdm description with best score found so far\n",
    "                      outer_tqdm.set_postfix_str(f\"Best F1: {best_score:.4f}\", refresh=True)\n",
    "\n",
    "\n",
    "            # --- Halving Step ---\n",
    "            n_keep = int(n_configs_in_round / eta)\n",
    "            logging.info(f\"  -- Round i={i}: Completed {len(round_scores)} evaluations. Keeping top {n_keep} configs. --\")\n",
    "\n",
    "            if n_keep < 1 or i == s: # Keep at least one, or if it's the last round\n",
    "                # If it's the last round, ensure the best score from *this bracket* at *max resource* is considered\n",
    "                if abs(current_resource - max_resource) < 1e-6 and round_scores:\n",
    "                     bracket_best_score = round_scores[0][0]\n",
    "                     bracket_best_params = round_scores[0][1]\n",
    "                     logging.info(f\"  Bracket s={s} final best score: {bracket_best_score:.4f}\")\n",
    "                     # No need to update global best here, already done above\n",
    "                break # Exit inner loop\n",
    "\n",
    "            # Prepare parameter list for the next round\n",
    "            param_list = [params for score, params in round_scores[:n_keep]]\n",
    "            if not param_list: # Safety break if list becomes empty unexpectedly\n",
    "                 logging.warning(f\"  Param list empty after halving round i={i}. Stopping bracket.\")\n",
    "                 break\n",
    "\n",
    "    logging.info(f\"\\n--- Hyperband HPO Finished ---\")\n",
    "    logging.info(f\"Total configurations evaluated (approx): {total_configs_evaluated}\") # Might overcount if errors happened\n",
    "    if best_params:\n",
    "        logging.info(f\"Best Overall Score ({scoring_func.__name__}): {best_score:.4f}\")\n",
    "        logging.info(f\"Best Params: {best_params}\")\n",
    "    else:\n",
    "        logging.warning(\"No best parameters found. Check logs for errors or increase resources/configs.\")\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platt Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_platt_scaler(base_estimator_class, best_params, X_train, y_train,\n",
    "                       score_method='decision_function', # Nuevo parmetro\n",
    "                       n_splits=5, random_state=None):\n",
    "    \"\"\"\n",
    "    Trains a base estimator and calibrates its outputs using Platt scaling\n",
    "    with k-fold cross-validation to obtain out-of-fold scores.\n",
    "\n",
    "    Args:\n",
    "        base_estimator_class: The class of the base estimator (e.g., SVC, RandomForestClassifier).\n",
    "        best_params (dict): Dictionary of best hyperparameters for the base estimator.\n",
    "        X_train (pd.DataFrame or np.ndarray): Training features.\n",
    "        y_train (pd.Series or np.ndarray): Training labels.\n",
    "        score_method (str): Method to get scores from the base estimator during CV.\n",
    "                            Options: 'decision_function', 'predict_proba',\n",
    "                                     'raw_margin_xgb', 'raw_score_lgbm'.\n",
    "        n_splits (int): Number of folds for cross-validation.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (fitted_base_estimator, fitted_platt_scaler)\n",
    "               Returns (None, None) if an error occurs.\n",
    "    \"\"\"\n",
    "    logging.info(f\"--- Starting Platt Scaling Training ({score_method}) ---\")\n",
    "    try:\n",
    "        # 1. Train the final base model on the entire training set\n",
    "        logging.info(\"Training final base model on full training data...\")\n",
    "        final_base_estimator = base_estimator_class(**best_params)\n",
    "        # Make sure y_train is a numpy array for fitting if needed\n",
    "        y_train_np = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "        final_base_estimator.fit(X_train, y_train_np)\n",
    "        logging.info(\"Final base model trained.\")\n",
    "\n",
    "        # 2. Get out-of-fold scores using k-fold CV\n",
    "        logging.info(f\"Performing {n_splits}-fold CV to get out-of-fold scores ({score_method})...\")\n",
    "        # Use StratifiedKFold for classification\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "        # Determine if X_train is DataFrame or ndarray for proper indexing\n",
    "        is_pandas_X = isinstance(X_train, pd.DataFrame)\n",
    "        is_pandas_y = isinstance(y_train, pd.Series)\n",
    "\n",
    "        oof_scores = np.zeros(len(y_train), dtype=float)\n",
    "        oof_true_labels = np.zeros(len(y_train), dtype=int)\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(tqdm(cv.split(X_train, y_train_np), total=n_splits, desc=f\"Platt CV ({score_method})\", leave=False)):\n",
    "            logging.info(f\"Processing Fold {fold+1}/{n_splits}...\")\n",
    "\n",
    "            # Select data based on index type\n",
    "            if is_pandas_X:\n",
    "                X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            else: # Assume numpy array\n",
    "                X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "\n",
    "            if is_pandas_y:\n",
    "                y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "            else: # Assume numpy array\n",
    "                y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "            # Ensure y_train_fold is numpy for fitting fold estimator\n",
    "            y_train_fold_np = y_train_fold.values if isinstance(y_train_fold, pd.Series) else y_train_fold\n",
    "\n",
    "\n",
    "            # Clone and train estimator on the fold's training data\n",
    "            # Use clone to ensure fresh state and proper param handling\n",
    "            estimator_fold = clone(final_base_estimator) # Clone the already instantiated final estimator\n",
    "            estimator_fold.fit(X_train_fold, y_train_fold_np)\n",
    "\n",
    "            # Get scores based on the specified method\n",
    "            scores_fold = None\n",
    "            if score_method == 'decision_function':\n",
    "                 if hasattr(estimator_fold, 'decision_function'):\n",
    "                     scores_fold = estimator_fold.decision_function(X_val_fold)\n",
    "                 else:\n",
    "                     raise AttributeError(f\"{base_estimator_class.__name__} does not have 'decision_function' method.\")\n",
    "            elif score_method == 'predict_proba':\n",
    "                 if hasattr(estimator_fold, 'predict_proba'):\n",
    "                     # Use probability of the positive class (class 1)\n",
    "                     scores_fold = estimator_fold.predict_proba(X_val_fold)[:, 1]\n",
    "                 else:\n",
    "                      raise AttributeError(f\"{base_estimator_class.__name__} does not have 'predict_proba' method.\")\n",
    "            elif score_method == 'raw_margin_xgb':\n",
    "                 # Assumes XGBoost model\n",
    "                 scores_fold = estimator_fold.predict(X_val_fold, output_margin=True)\n",
    "            elif score_method == 'raw_score_lgbm':\n",
    "                 # Assumes LightGBM model\n",
    "                 scores_fold = estimator_fold.predict(X_val_fold, raw_score=True)\n",
    "            else:\n",
    "                 raise ValueError(f\"Unsupported score_method: {score_method}\")\n",
    "\n",
    "            # Store results\n",
    "            oof_scores[val_idx] = scores_fold\n",
    "            # Ensure y_val_fold is numpy for assignment\n",
    "            y_val_fold_np = y_val_fold.values if isinstance(y_val_fold, pd.Series) else y_val_fold\n",
    "            oof_true_labels[val_idx] = y_val_fold_np\n",
    "\n",
    "        logging.info(\"Out-of-fold scores collected.\")\n",
    "\n",
    "        # Reshape scores for Logistic Regression input\n",
    "        oof_scores_reshaped = oof_scores.reshape(-1, 1)\n",
    "\n",
    "        # 3. Train the Logistic Regression scaler\n",
    "        logging.info(\"Training Logistic Regression (Platt) scaler...\")\n",
    "        # Use high C to approximate original Platt scaling (low regularization)\n",
    "        platt_scaler = LogisticRegression(C=1e10, solver='liblinear', random_state=random_state)\n",
    "        platt_scaler.fit(oof_scores_reshaped, oof_true_labels)\n",
    "        logging.info(\"Platt scaler trained.\")\n",
    "\n",
    "        # Verify shapes one last time\n",
    "        if oof_scores_reshaped.shape[0] != len(oof_true_labels):\n",
    "            raise ValueError(f\"Shape mismatch after collecting OOF scores: scores {oof_scores_reshaped.shape[0]}, labels {len(oof_true_labels)}\")\n",
    "\n",
    "        logging.info(f\"--- Platt Scaling Training ({score_method}) Complete ---\")\n",
    "        return final_base_estimator, platt_scaler\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during Platt scaling ({score_method}): {e}\", exc_info=True)\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mondrian Inductive Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs_to_alphas(\n",
    "        p_mat: np.ndarray,\n",
    "        *,\n",
    "        ncm: str = \"margin\",\n",
    "        classes=None,\n",
    "        y=None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert probability matrix -> -scores.\n",
    "\n",
    "     If *classes* & *y* are given   vector (n_samples,)  - use for CALIBRATION.\n",
    "     Otherwise                      matrix (n_samples, n_classes) - use for TEST.\n",
    "\n",
    "    The behaviour follows crepes.extras.{margin|hinge}\n",
    "    \"\"\"\n",
    "    if p_mat.ndim != 2:\n",
    "        raise ValueError(\"p_mat must be 2-D (n_samples, n_classes).\")\n",
    "\n",
    "    func = {\"margin\": margin, \"hinge\": hinge}.get(ncm.lower())\n",
    "    if func is None:\n",
    "        raise ValueError(\"ncm must be 'margin' or 'hinge'.\")\n",
    "\n",
    "    return func(p_mat, classes, y) if (classes is not None and y is not None) \\\n",
    "           else func(p_mat)\n",
    "\n",
    "\n",
    "# Fit Mondrian ICP using the calibration split.\n",
    "def fit_mondrian_classifier(\n",
    "        probs_cal: np.ndarray,\n",
    "        y_cal: np.ndarray,\n",
    "        *,\n",
    "        classes=None,\n",
    "        bins_cal=None,\n",
    "        ncm: str = \"margin\"\n",
    ") -> \"ConformalClassifier\":\n",
    "    \"\"\"\n",
    "    Fit a Mondrian ConformalClassifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    probs_cal : (n_cal, n_classes) calibrated probabilities\n",
    "    y_cal     : (n_cal)           true labels of the calibration set\n",
    "    classes   : (n_classes)       class order in *probs_cal*; if None  np.arange\n",
    "    bins_cal  : (n_cal)           Mondrian categories; if None  class-conditional\n",
    "    ncm       : str                nonconformity function (\"margin\" or \"hinge\")\n",
    "    \"\"\"\n",
    "    logging.info(\"--- Fitting Mondrian Conformal Classifier ---\")\n",
    "    if probs_cal is None or len(probs_cal) == 0:\n",
    "        logging.error(\"Calibration probabilities are empty or None. Cannot fit Mondrian classifier.\")\n",
    "        return None\n",
    "    if y_cal is None or len(y_cal) == 0:\n",
    "        logging.error(\"Calibration labels are empty or None. Cannot fit Mondrian classifier.\")\n",
    "        return None\n",
    "    try:\n",
    "        if classes is None:\n",
    "            classes = np.arange(probs_cal.shape[1])\n",
    "            logging.info(f\"No classes provided. Using default: {classes}\")\n",
    "        #  must be a vector, so pass *classes* & *y_cal*\n",
    "        alphas_cal = probs_to_alphas(\n",
    "            probs_cal, ncm=ncm, classes=classes, y=y_cal)\n",
    "        logging.info(f\"Alphas for calibration computed using ncm='{ncm}'. Shape: {alphas_cal.shape}\")\n",
    "\n",
    "        # default Mondrian rule = class-conditional\n",
    "        if bins_cal is None:\n",
    "            bins_cal = y_cal\n",
    "            logging.info(\"No bins_cal provided. Using class-conditional Mondrian bins (y_cal).\")\n",
    "\n",
    "        cc = ConformalClassifier()\n",
    "        cc.fit(alphas_cal, bins=bins_cal)\n",
    "        logging.info(\"--- Mondrian Conformal Classifier Fitted ---\")\n",
    "        return cc\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fitting Mondrian classifier: {e}\", exc_info=True)\n",
    "        return None\n",
    "    \n",
    "# Evaluate Mondrian ICP\n",
    "def evaluate_mondrian_prediction(\n",
    "        fitted_cc,\n",
    "        probs_test: np.ndarray,\n",
    "        *,\n",
    "        bins_test,\n",
    "        y_test_true=None,\n",
    "        ncm: str = \"margin\",\n",
    "        alpha: float = 0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict sets and (optionally) compute coverage / size statistics.\n",
    "    \"\"\"\n",
    "    logging.info(f\"--- Evaluating Mondrian Prediction (alpha={alpha}, ncm={ncm}) ---\")\n",
    "    if fitted_cc is None:\n",
    "        logging.error(\"Fitted classifier is None. Cannot evaluate.\")\n",
    "        return None, None, None, None\n",
    "    if probs_test is None or len(probs_test) == 0:\n",
    "        logging.warning(\"Test probabilities are empty/None. Skipping evaluation.\")\n",
    "        return 0.0, 0.0, np.array([[]]), {}\n",
    "\n",
    "    try:\n",
    "        alphas_test = probs_to_alphas(probs_test, ncm=ncm)\n",
    "        pred_sets = fitted_cc.predict_set(\n",
    "            alphas_test, bins=bins_test, confidence=1 - alpha\n",
    "        )\n",
    "\n",
    "        if y_test_true is None:\n",
    "            logging.info(\"No test labels provided. Returning prediction sets only (inference mode).\")\n",
    "            return pred_sets\n",
    "\n",
    "        y_test_true = np.asarray(y_test_true)\n",
    "        n_test = len(y_test_true)\n",
    "        if n_test == 0:\n",
    "            logging.warning(\"Test labels are empty. Skipping evaluation.\")\n",
    "            return 0.0, 0.0, pred_sets, {}\n",
    "\n",
    "        contains = np.asarray([\n",
    "            y_test_true[i] in np.where(pred_sets[i])[0]\n",
    "            for i in range(n_test)\n",
    "        ])\n",
    "        coverage = contains.mean()\n",
    "        avg_size = pred_sets.sum(axis=1).mean()\n",
    "\n",
    "        class_cov = {}\n",
    "        for c in np.unique(y_test_true):\n",
    "            mask = (y_test_true == c)\n",
    "            class_cov[c] = contains[mask].mean() if mask.sum() > 0 else float(\"nan\")\n",
    "            logging.info(f\"Mondrian CP Coverage for class {c}: {class_cov[c]:.4f}\")\n",
    "\n",
    "        n_empty = np.sum(pred_sets.sum(axis=1) == 0)\n",
    "        if n_empty > 0:\n",
    "            logging.warning(f\"{n_empty} conformal prediction sets are empty out of {n_test} samples.\")\n",
    "\n",
    "        logging.info(f\"Mondrian CP Coverage: {coverage:.4f}\")\n",
    "        logging.info(f\"Mondrian CP Avg Set Size: {avg_size:.4f}\")\n",
    "        logging.info(\"--- Mondrian Prediction Evaluation Complete ---\")\n",
    "\n",
    "        return coverage, avg_size, pred_sets, class_cov\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during Mondrian prediction evaluation: {e}\", exc_info=True)\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Comprehensive Metrics ---\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_proba, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Calculates a comprehensive set of classification metrics.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Ground truth labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        y_proba (array-like): Predicted probabilities for the positive class (class 1).\n",
    "        model_name (str): Name of the model for logging.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing calculated metrics.\n",
    "              Returns None if input arrays are empty or invalid.\n",
    "    \"\"\"\n",
    "    if len(y_true) == 0 or len(y_pred) == 0 or len(y_proba) == 0:\n",
    "        logging.error(f\"[{model_name}] Empty input arrays provided for metric calculation.\")\n",
    "        return None\n",
    "    if len(y_true) != len(y_pred) or len(y_true) != len(y_proba):\n",
    "        logging.error(f\"[{model_name}] Mismatched lengths in input arrays for metric calculation.\")\n",
    "        return None\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # --- Threshold-based Metrics (using y_pred) ---\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['precision'] = precision\n",
    "    metrics['recall_tpr'] = recall # True Positive Rate (Sensitivity)\n",
    "    metrics['f1_score'] = f1\n",
    "\n",
    "    # Specificity (True Negative Rate)\n",
    "    metrics['specificity_tnr'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "    # Geometric Mean\n",
    "    metrics['g_mean'] = np.sqrt(metrics['recall_tpr'] * metrics['specificity_tnr'])\n",
    "\n",
    "    # Confusion Matrix\n",
    "    metrics['confusion_matrix'] = {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp}\n",
    "\n",
    "    # --- Ranking/Probabilistic Metrics (using y_proba) ---\n",
    "    try:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_true, y_proba)\n",
    "    except ValueError as e:\n",
    "        logging.warning(f\"[{model_name}] Could not calculate ROC AUC: {e}. Setting to 0.0.\")\n",
    "        metrics['roc_auc'] = 0.0 # Handle cases with only one class present\n",
    "\n",
    "    # PR AUC\n",
    "    pr_curve_precision, pr_curve_recall, _ = precision_recall_curve(y_true, y_proba)\n",
    "    metrics['pr_auc'] = auc(pr_curve_recall, pr_curve_precision) # Note order: recall is x, precision is y\n",
    "\n",
    "    # Brier Score\n",
    "    metrics['brier_score'] = brier_score_loss(y_true, y_proba)\n",
    "\n",
    "    logging.info(f\"--- {model_name} Metrics ---\")\n",
    "    logging.info(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    logging.info(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    logging.info(f\"Recall (TPR): {metrics['recall_tpr']:.4f}\")\n",
    "    logging.info(f\"Specificity (TNR): {metrics['specificity_tnr']:.4f}\")\n",
    "    logging.info(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    logging.info(f\"G-Mean: {metrics['g_mean']:.4f}\")\n",
    "    logging.info(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    logging.info(f\"PR AUC: {metrics['pr_auc']:.4f}\")\n",
    "    logging.info(f\"Brier Score: {metrics['brier_score']:.4f}\")\n",
    "    logging.info(f\"Confusion Matrix (TN, FP, FN, TP): ({tn}, {fp}, {fn}, {tp})\")\n",
    "\n",
    "    # Optional: Plot Confusion Matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap([[tn, fp], [fn, tp]], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted Galaxy (0)', 'Predicted Star (1)'],\n",
    "                yticklabels=['Actual Galaxy (0)', 'Actual Star (1)'])\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    cm_filename = os.path.join(MODEL_DIR, f\"{model_name}_confusion_matrix_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\")\n",
    "    plt.savefig(cm_filename)\n",
    "    plt.close()\n",
    "    logging.info(f\"Confusion matrix plot saved to {cm_filename}\")\n",
    "\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Scaling ---\n",
    "# Important for SVM, not used for the other models.\n",
    "# Fit scaler ONLY on training data, then transform all sets.\n",
    "\n",
    "# Check if training set and other datasets are non-empty before scaling\n",
    "if len(X_train) > 0 and TRAIN_SIZE > 0:\n",
    "    logging.info(\"Applying StandardScaler to features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "else:\n",
    "    logging.info(\"Empty training set, NOT able to apply StandardScaler!\")\n",
    "    X_train_scaled = X_train\n",
    "\n",
    "if len(X_val) > 0 and VAL_SIZE > 0:\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "else:\n",
    "    X_val_scaled = X_val\n",
    "\n",
    "if len(X_test) > 0 and TEST_SIZE > 0:\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "else:\n",
    "    X_test_scaled = X_test\n",
    "\n",
    "if len(X_cal) > 0 and CAL_SIZE > 0:\n",
    "    X_cal_scaled = scaler.transform(X_cal)\n",
    "else:\n",
    "    X_cal_scaled = X_cal\n",
    "\n",
    "# Save the scaler if it was fitted\n",
    "if 'scaler' in locals():\n",
    "    scaler_filename = os.path.join(MODEL_DIR, f\"scaler_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib\")\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    logging.info(f\"Scaler saved to {scaler_filename}\")\n",
    "\n",
    "# Use scaled data for models sensitive to scale (like SVM)\n",
    "logging.info(\"Feature scaling complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {} # Dictionary to store metrics for each model\n",
    "\n",
    "ALPHA = 0.9 #Conformal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for SVM (using data fraction)\n",
    "MAX_RESOURCE_SVM = 1.0  # Max data fraction\n",
    "MIN_RESOURCE_SVM = 0.1  # Min data fraction (adjust based on minority class size)\n",
    "ETA_SVM = 3\n",
    "RESOURCE_TYPE_SVM = 'data_fraction'\n",
    "model_name_svm = \"SVM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODIFICADO PARA PRUEBA RPIDA!\n",
    "MAX_RESOURCE_SVM = 1.0  # Se mantiene en 1.0 para usar todos los datos al final\n",
    "MIN_RESOURCE_SVM = 0.5  # Aumentado para reducir s_max (menos brackets/configs)\n",
    "ETA_SVM = 4             # Aumentado para eliminar configuraciones ms rpido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bracket s=0 (n=1, r0=1.00):   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Bracket s=0 (n=1, r0=1.00): 100%|| 1/1 [00:04<00:00,  4.94s/it, Best F1: 0.6460]\n",
      "                                                                           \r"
     ]
    }
   ],
   "source": [
    "logging.info(f\"\\n\\n===== Starting Workflow for {model_name_svm} =====\")\n",
    "timestamp_svm = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "hpo_start_time_svm = time.time()\n",
    "\n",
    "# --- 1.1 SVM: Define Search Space and HPO Params ---\n",
    "param_space_svm = {\n",
    "    'C': loguniform(1e-2, 1e3),\n",
    "    'gamma': loguniform(1e-4, 1e1),\n",
    "    'kernel': ['rbf'], # Example: Fixed RBF kernel\n",
    "    # 'kernel': ['rbf', 'linear'], # Example: If you want to search kernels\n",
    "    # class_weight is added automatically inside hyperband_hpo\n",
    "    # random_state is added automatically inside hyperband_hpo\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# --- 1.2 SVM: Run Hyperband HPO ---\n",
    "logging.info(f\"--- [{model_name_svm}] Running Hyperband HPO ---\")\n",
    "best_params_svm, best_score_hpo_svm = hyperband_hpo(\n",
    "    model_class=SVC,\n",
    "    param_space=param_space_svm,\n",
    "    X_train=X_train_scaled, # USE SCALED DATA\n",
    "    y_train=y_train,\n",
    "    X_val=X_val_scaled,     # USE SCALED DATA\n",
    "    y_val=y_val,\n",
    "    max_resource=MAX_RESOURCE_SVM,\n",
    "    eta=ETA_SVM,\n",
    "    resource_type=RESOURCE_TYPE_SVM,\n",
    "    min_resource=MIN_RESOURCE_SVM,\n",
    "    scoring_func=f1_score,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "hpo_duration_svm = time.time() - hpo_start_time_svm\n",
    "logging.info(f\"--- [{model_name_svm}] HPO finished in {hpo_duration_svm:.2f} seconds ---\")\n",
    "\n",
    "# --- 1.3 SVM: Train Final Model & Platt Scaler (using Full Training Set) ---\n",
    "fitted_svm_base = None\n",
    "platt_scaler_svm = None\n",
    "if best_params_svm:\n",
    "    logging.info(f\"--- [{model_name_svm}] Training final model and Platt scaler ---\")\n",
    "    platt_start_time_svm = time.time()\n",
    "    # Ensure necessary fixed parameters are present for the final fit\n",
    "    best_params_svm['random_state'] = RANDOM_SEED\n",
    "    if 'class_weight' not in best_params_svm: best_params_svm['class_weight'] = 'balanced'\n",
    "    if 'probability' in best_params_svm: del best_params_svm['probability'] # Use decision_function\n",
    "\n",
    "    fitted_svm_base, platt_scaler_svm = train_platt_scaler(\n",
    "        base_estimator_class=SVC, # Pass the class\n",
    "        best_params=best_params_svm,\n",
    "        X_train=X_train_scaled, # Use scaled training data\n",
    "        y_train=y_train,        # Use original y_train for CV indexing\n",
    "        n_splits=5,             # Folds for Platt CV\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    platt_duration_svm = time.time() - platt_start_time_svm\n",
    "    if fitted_svm_base and platt_scaler_svm:\n",
    "        logging.info(f\"--- [{model_name_svm}] Platt scaling finished in {platt_duration_svm:.2f} seconds ---\")\n",
    "        # Optional: Save models\n",
    "        # joblib.dump(...)\n",
    "    else:\n",
    "        logging.error(f\"[{model_name_svm}] Failed to train base model or Platt scaler.\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_svm}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "\n",
    "# --- Section 1.4: Mondrian ICP Calibration ---\n",
    "fitted_cc_svm = None # Initialize classifier variable\n",
    "if fitted_svm_base and platt_scaler_svm:\n",
    "    if not y_cal.empty:\n",
    "        logging.info(f\"--- [{model_name_svm}] Calibrating Mondrian Conformal Prediction ---\")\n",
    "        mcp_cal_start_time_svm = time.time()\n",
    "\n",
    "        # Calculate probabilities needed for crepes on Calibration set\n",
    "        decision_scores_cal_svm = fitted_svm_base.decision_function(X_cal_scaled)\n",
    "        probs_cal_svm = platt_scaler_svm.predict_proba(decision_scores_cal_svm.reshape(-1, 1)) # (n_cal, 2)\n",
    "\n",
    "        # Define Mondrian Bins (Class-conditional example)\n",
    "        #bins_cal_svm = y_cal.values # Assumes y_cal is pd.Series/np.array\n",
    "        # Use predicted labels as bins for Mondrian calibration\n",
    "        bins_cal_svm = np.argmax(probs_cal_svm, axis=1)  # Predicted class labels\n",
    "\n",
    "        # Fit the Mondrian classifier\n",
    "        fitted_cc_svm = fit_mondrian_classifier(\n",
    "            probs_cal_svm,\n",
    "            y_cal.values,                              \n",
    "            classes = platt_scaler_svm.classes_,       \n",
    "            bins_cal = bins_cal_svm)\n",
    "\n",
    "        mcp_cal_duration_svm = time.time() - mcp_cal_start_time_svm\n",
    "        if fitted_cc_svm:\n",
    "            logging.info(f\"--- [{model_name_svm}] Mondrian CP calibration finished in {mcp_cal_duration_svm:.2f} seconds ---\")\n",
    "            # Optional: Save the fitted_cc_svm object using joblib alongside the base model and Platt scaler\n",
    "            # cc_filename = os.path.join(MODEL_DIR, f\"{model_name_svm}_mondrian_classifier_{timestamp_svm}.joblib\")\n",
    "            # joblib.dump(fitted_cc_svm, cc_filename)\n",
    "            # logging.info(f\"Mondrian classifier saved to {cc_filename}\")\n",
    "        else:\n",
    "            logging.error(f\"[{model_name_svm}] Failed to fit Mondrian classifier.\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_svm}] Calibration set is empty. Skipping Mondrian ICP calibration.\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_svm}] Base model or Platt scaler not available. Skipping Mondrian ICP calibration.\")\n",
    "\n",
    "\n",
    "# --- Section 1.5: Final Evaluation ---\n",
    "if fitted_svm_base and platt_scaler_svm: # Check base model availability\n",
    "    logging.info(f\"--- [{model_name_svm}] Final Evaluation on Test Set ---\")\n",
    "    eval_start_time_svm = time.time()\n",
    "\n",
    "    # --- Calculate Base Metrics (Same as before) ---\n",
    "    decision_scores_test_svm = fitted_svm_base.decision_function(X_test_scaled)\n",
    "    probs_test_svm_full = platt_scaler_svm.predict_proba(decision_scores_test_svm.reshape(-1, 1)) # (n_test, 2)\n",
    "    y_proba_test_svm = probs_test_svm_full[:, 1] # Prob positive class\n",
    "    y_pred_test_svm = (y_proba_test_svm >= 0.5).astype(int)\n",
    "    metrics_svm = calculate_metrics(y_test, y_pred_test_svm, y_proba_test_svm, model_name=model_name_svm)\n",
    "\n",
    "    # --- Mondrian Conformal Prediction Evaluation ---\n",
    "    cp_coverage_mond_svm, cp_avg_set_size_mond_svm = None, None # Initialize results\n",
    "\n",
    "    if fitted_cc_svm is not None: # Check if Mondrian classifier was fitted successfully\n",
    "        mcp_eval_start_time_svm = time.time()\n",
    "        # Evaluate the fitted Mondrian classifier\n",
    "        #bins_test_svm = y_test.values if not y_test.empty else np.array([])#(Class-conditional example)\n",
    "        bins_test_svm = np.argmax(probs_test_svm_full, axis=1)  # predicted label bins\n",
    "        coverage, size, _, class_cov = evaluate_mondrian_prediction(\n",
    "            fitted_cc_svm,\n",
    "            probs_test_svm_full,\n",
    "            bins_test = bins_test_svm,\n",
    "            y_test_true = y_test.values,\n",
    "            alpha = ALPHA)\n",
    "        cp_coverage_mond_svm = coverage\n",
    "        cp_avg_set_size_mond_svm = size\n",
    "        class_coverage_dict = class_cov\n",
    "        mcp_eval_duration_svm = time.time() - mcp_eval_start_time_svm\n",
    "        logging.info(f\"--- [{model_name_svm}] Mondrian CP evaluation finished in {mcp_eval_duration_svm:.2f} seconds ---\")\n",
    "    else:\n",
    "         logging.warning(f\"[{model_name_svm}] Skipping Mondrian CP evaluation: Classifier not fitted.\")\n",
    "\n",
    "    eval_duration_svm = time.time() - eval_start_time_svm # Total eval time\n",
    "    logging.info(f\"--- [{model_name_svm}] Total Evaluation finished in {eval_duration_svm:.2f} seconds ---\")\n",
    "\n",
    "    # --- Store results (Same as before, using the new variables) ---\n",
    "    all_results[model_name_svm] = {\n",
    "        'metrics': metrics_svm,\n",
    "        'cp_coverage_mond': cp_coverage_mond_svm,           # Store Mondrian coverage\n",
    "        'cp_class_coverage_dict': class_coverage_dict,\n",
    "        'cp_avg_set_size_mond': cp_avg_set_size_mond_svm,     # Store Mondrian avg set size\n",
    "        'best_hpo_params': best_params_svm,\n",
    "        'hpo_f1_score': best_score_hpo_svm,\n",
    "        'hpo_duration_s': hpo_duration_svm,\n",
    "    }\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_svm}] Skipping final evaluation (Base model or Platt scaler not available).\")\n",
    "\n",
    "# Keep the final logging line:\n",
    "logging.info(f\"===== Finished Workflow for {model_name_svm} =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for CART (using data fraction)\n",
    "MAX_RESOURCE_CART = 1.0\n",
    "MIN_RESOURCE_CART = 0.1 # Can start with smaller fraction for trees\n",
    "ETA_CART = 3\n",
    "RESOURCE_TYPE_CART = 'data_fraction'\n",
    "model_name_cart = \"CART\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODIFICADO PARA PRUEBA RPIDA!\n",
    "MAX_RESOURCE_CART = 1.0\n",
    "MIN_RESOURCE_CART = 0.5 # Aumentado para reducir s_max\n",
    "ETA_CART = 4            # Aumentado para eliminar configuraciones ms rpido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bracket s=0 (n=1, r0=1.00):   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Bracket s=0 (n=1, r0=1.00): 100%|| 1/1 [00:00<00:00,  1.77it/s, Best F1: 0.5267]\n",
      "                                                                       \r"
     ]
    }
   ],
   "source": [
    "logging.info(f\"\\n\\n===== Starting Workflow for {model_name_cart} =====\")\n",
    "timestamp_cart = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "hpo_start_time_cart = time.time()\n",
    "\n",
    "# --- 2.1 CART: Define Search Space and HPO Params ---\n",
    "param_space_cart = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': randint(3, 50),\n",
    "    'min_samples_split': randint(2, 100),\n",
    "    'min_samples_leaf': randint(1, 50),\n",
    "    # class_weight added automatically\n",
    "    # random_state added automatically\n",
    "}\n",
    "\n",
    "# --- 2.2 CART: Run Hyperband HPO ---\n",
    "logging.info(f\"--- [{model_name_cart}] Running Hyperband HPO ---\")\n",
    "best_params_cart, best_score_hpo_cart = hyperband_hpo(\n",
    "    model_class=DecisionTreeClassifier,\n",
    "    param_space=param_space_cart,\n",
    "    X_train=X_train, # USE UNSCALED DATA\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,     # USE UNSCALED DATA\n",
    "    y_val=y_val,\n",
    "    max_resource=MAX_RESOURCE_CART,\n",
    "    eta=ETA_CART,\n",
    "    resource_type=RESOURCE_TYPE_CART,\n",
    "    min_resource=MIN_RESOURCE_CART,\n",
    "    scoring_func=f1_score,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "hpo_duration_cart = time.time() - hpo_start_time_cart\n",
    "logging.info(f\"--- [{model_name_cart}] HPO finished in {hpo_duration_cart:.2f} seconds ---\")\n",
    "\n",
    "# --- 2.3 CART: Train Final Model & Platt Scaler ---\n",
    "fitted_cart_base = None\n",
    "platt_scaler_cart = None\n",
    "if best_params_cart:\n",
    "    logging.info(f\"--- [{model_name_cart}] Training final model and Platt scaler ---\")\n",
    "    platt_start_time_cart = time.time()\n",
    "    # Ensure necessary fixed parameters are present for the final fit\n",
    "    best_params_cart['random_state'] = RANDOM_SEED\n",
    "    if 'class_weight' not in best_params_cart: best_params_cart['class_weight'] = 'balanced'\n",
    "\n",
    "    # Use the modified train_platt_scaler with predict_proba\n",
    "    fitted_cart_base, platt_scaler_cart = train_platt_scaler(\n",
    "        base_estimator_class=DecisionTreeClassifier,\n",
    "        best_params=best_params_cart,\n",
    "        X_train=X_train, # Use UNSCALED training data\n",
    "        y_train=y_train,\n",
    "        score_method='predict_proba', # <<< Specify score method for CART\n",
    "        n_splits=5,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    platt_duration_cart = time.time() - platt_start_time_cart\n",
    "    if fitted_cart_base and platt_scaler_cart:\n",
    "        logging.info(f\"--- [{model_name_cart}] Platt scaling finished in {platt_duration_cart:.2f} seconds ---\")\n",
    "        # Optional: Save models\n",
    "        # joblib.dump(...)\n",
    "    else:\n",
    "        logging.error(f\"[{model_name_cart}] Failed to train base model or Platt scaler.\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_cart}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "\n",
    "# --- 2.4 CART: Mondrian ICP Calibration (using Calibration Set) ---\n",
    "fitted_cc_cart = None  # Initialize Mondrian classifier variable\n",
    "if fitted_cart_base and platt_scaler_cart:\n",
    "    if not y_cal.empty:\n",
    "        logging.info(f\"--- [{model_name_cart}] Calibrating Mondrian Conformal Prediction ---\")\n",
    "        mcp_cal_start_time_cart = time.time()\n",
    "\n",
    "        # Get base model probabilities (class 1) for calibration set\n",
    "        base_probs_cal_cart = fitted_cart_base.predict_proba(X_cal)[:, 1].reshape(-1, 1)  # UNSCALED cal data\n",
    "        # Get calibrated probabilities from Platt scaler\n",
    "        calibrated_probs_cal_cart = platt_scaler_cart.predict_proba(base_probs_cal_cart)  # (n_cal, 2)\n",
    "\n",
    "        # Define Mondrian Bins (Class-conditional example)\n",
    "        bins_cal_cart = y_cal.values  # Assumes y_cal is pd.Series/np.array\n",
    "\n",
    "        # Fit the Mondrian classifier\n",
    "        fitted_cc_cart = fit_mondrian_classifier(calibrated_probs_cal_cart, bins_cal=bins_cal_cart)\n",
    "\n",
    "        mcp_cal_duration_cart = time.time() - mcp_cal_start_time_cart\n",
    "        if fitted_cc_cart:\n",
    "            logging.info(f\"--- [{model_name_cart}] Mondrian CP calibration finished in {mcp_cal_duration_cart:.2f} seconds ---\")\n",
    "            # Optional: Save the fitted_cc_cart object using joblib\n",
    "            # cc_filename = os.path.join(MODEL_DIR, f\"{model_name_cart}_mondrian_classifier_{timestamp_cart}.joblib\")\n",
    "            # joblib.dump(fitted_cc_cart, cc_filename)\n",
    "            # logging.info(f\"Mondrian classifier saved to {cc_filename}\")\n",
    "        else:\n",
    "            logging.error(f\"[{model_name_cart}] Failed to fit Mondrian classifier.\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_cart}] Calibration set is empty. Skipping Mondrian ICP calibration.\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_cart}] Base model or Platt scaler not available. Skipping Mondrian ICP calibration.\")\n",
    "\n",
    "# --- 2.5 CART: Final Evaluation (using Test Set) ---\n",
    "if fitted_cart_base and platt_scaler_cart:\n",
    "    logging.info(f\"--- [{model_name_cart}] Final Evaluation on Test Set ---\")\n",
    "    eval_start_time_cart = time.time()\n",
    "    # Get base model probabilities (class 1) for test set\n",
    "    base_probs_test_cart = fitted_cart_base.predict_proba(X_test)[:, 1].reshape(-1, 1)  # UNSCALED test data\n",
    "    # Get calibrated probabilities from Platt scaler\n",
    "    calibrated_probs_test_cart = platt_scaler_cart.predict_proba(base_probs_test_cart)\n",
    "    y_proba_test_cart = calibrated_probs_test_cart[:, 1]  # Probability of positive class\n",
    "    y_pred_test_cart = (y_proba_test_cart >= 0.5).astype(int)  # Threshold calibrated probabilities\n",
    "\n",
    "    metrics_cart = calculate_metrics(y_test, y_pred_test_cart, y_proba_test_cart, model_name=model_name_cart)\n",
    "\n",
    "    # --- Mondrian Conformal Prediction Evaluation ---\n",
    "    cp_coverage_mond_cart, cp_avg_set_size_mond_cart = None, None  # Initialize results\n",
    "\n",
    "    if fitted_cc_cart is not None:  # Check if Mondrian classifier was fitted successfully\n",
    "        mcp_eval_start_time_cart = time.time()\n",
    "        # Define Mondrian Bins for test set (Class-conditional example)\n",
    "        bins_test_cart = y_test.values if not y_test.empty else np.array([])\n",
    "        y_test_true_np_cart = y_test.values if not y_test.empty else np.array([])\n",
    "\n",
    "        # Evaluate the fitted Mondrian classifier\n",
    "        cp_coverage_mond_cart, cp_avg_set_size_mond_cart, _, class_coverage_dict = evaluate_mondrian_prediction(\n",
    "            fitted_cc=fitted_cc_cart,                # Pass the fitted classifier\n",
    "            probs_test=calibrated_probs_test_cart,   # Pass test probabilities (n_test, 2)\n",
    "            y_test_true=y_test_true_np_cart,         # Pass true test labels\n",
    "            bins_test=bins_test_cart,                # Pass test bins\n",
    "            alpha=ALPHA\n",
    "        )\n",
    "        mcp_eval_duration_cart = time.time() - mcp_eval_start_time_cart\n",
    "        logging.info(f\"--- [{model_name_cart}] Mondrian CP evaluation finished in {mcp_eval_duration_cart:.2f} seconds ---\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_cart}] Skipping Mondrian CP evaluation: Classifier not fitted.\")\n",
    "\n",
    "    eval_duration_cart = time.time() - eval_start_time_cart\n",
    "    logging.info(f\"--- [{model_name_cart}] Total Evaluation finished in {eval_duration_cart:.2f} seconds ---\")\n",
    "\n",
    "    # Store results\n",
    "    all_results[model_name_cart] = {\n",
    "        'metrics': metrics_cart,\n",
    "        'cp_coverage_mond': cp_coverage_mond_cart,\n",
    "        'cp_class_coverage_dict': class_coverage_dict,\n",
    "        'cp_avg_set_size_mond': cp_avg_set_size_mond_cart,\n",
    "        'best_hpo_params': best_params_cart,\n",
    "        'hpo_f1_score': best_score_hpo_cart,\n",
    "        'hpo_duration_s': hpo_duration_cart,\n",
    "    }\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_cart}] Skipping final evaluation (Base model or Platt scaler not available).\")\n",
    "\n",
    "logging.info(f\"===== Finished Workflow for {model_name_cart} =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for RF (using iterations)\n",
    "MAX_RESOURCE_RF = 300  # Max n_estimators\n",
    "MIN_RESOURCE_RF = 20   # Min n_estimators\n",
    "ETA_RF = 3\n",
    "RESOURCE_TYPE_RF = 'iterations'\n",
    "model_name_rf = \"Random_Forest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODIFICADO PARA PRUEBA RPIDA!\n",
    "MAX_RESOURCE_RF = 20   # Reducido drsticamente! (Antes 300)\n",
    "MIN_RESOURCE_RF = 5    # Mnimo bajo pero cercano a max para pocos brackets\n",
    "ETA_RF = 4             # Aumentado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bracket s=1 (n=4, r0=5.00):   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Bracket s=0 (n=2, r0=20.00):  50%|     | 1/2 [00:14<00:14, 14.34s/it, Best F1: 0.7778]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Bracket s=0 (n=2, r0=20.00): 100%|| 2/2 [00:25<00:00, 12.70s/it, Best F1: 0.8011]\n",
      "                                                                       \r"
     ]
    }
   ],
   "source": [
    "logging.info(f\"\\n\\n===== Starting Workflow for {model_name_rf} =====\")\n",
    "timestamp_rf = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "hpo_start_time_rf = time.time()\n",
    "\n",
    "# --- 3.1 RF: Define Search Space and HPO Params ---\n",
    "param_space_rf = {\n",
    "    # n_estimators is controlled by resource_type='iterations'\n",
    "    'max_depth': randint(5, 50),\n",
    "    'min_samples_split': randint(2, 50),\n",
    "    'min_samples_leaf': randint(1, 25),\n",
    "    'max_features': ['sqrt', 'log2', None], # None means max_features=n_features\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    # class_weight added automatically\n",
    "    # random_state added automatically\n",
    "}\n",
    "\n",
    "# --- 3.2 RF: Run Hyperband HPO ---\n",
    "logging.info(f\"--- [{model_name_rf}] Running Hyperband HPO ---\")\n",
    "best_params_rf, best_score_hpo_rf = hyperband_hpo(\n",
    "    model_class=RandomForestClassifier,\n",
    "    param_space=param_space_rf,\n",
    "    X_train=X_train, # USE UNSCALED DATA\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,     # USE UNSCALED DATA\n",
    "    y_val=y_val,\n",
    "    max_resource=MAX_RESOURCE_RF,\n",
    "    eta=ETA_RF,\n",
    "    resource_type=RESOURCE_TYPE_RF,\n",
    "    min_resource=MIN_RESOURCE_RF,\n",
    "    scoring_func=f1_score,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "hpo_duration_rf = time.time() - hpo_start_time_rf\n",
    "logging.info(f\"--- [{model_name_rf}] HPO finished in {hpo_duration_rf:.2f} seconds ---\")\n",
    "\n",
    "\n",
    "# --- 3.3 RF: Train Final Model & Platt Scaler ---\n",
    "fitted_rf_base = None\n",
    "platt_scaler_rf = None\n",
    "if best_params_rf:\n",
    "    logging.info(f\"--- [{model_name_rf}] Training final model and Platt scaler ---\")\n",
    "    platt_start_time_rf = time.time()\n",
    "    # Ensure necessary fixed parameters are present\n",
    "    best_params_rf['random_state'] = RANDOM_SEED\n",
    "    if 'class_weight' not in best_params_rf: best_params_rf['class_weight'] = 'balanced'\n",
    "    best_params_rf['n_jobs'] = -1 # Use all cores\n",
    "\n",
    "    # Use the modified train_platt_scaler with predict_proba\n",
    "    fitted_rf_base, platt_scaler_rf = train_platt_scaler(\n",
    "        base_estimator_class=RandomForestClassifier,\n",
    "        best_params=best_params_rf,\n",
    "        X_train=X_train, # Use UNSCALED training data\n",
    "        y_train=y_train,\n",
    "        score_method='predict_proba', # <<< Specify score method for RF\n",
    "        n_splits=5,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    platt_duration_rf = time.time() - platt_start_time_rf\n",
    "    if fitted_rf_base and platt_scaler_rf:\n",
    "        logging.info(f\"--- [{model_name_rf}] Platt scaling finished in {platt_duration_rf:.2f} seconds ---\")\n",
    "        # Optional: Save models\n",
    "        # joblib.dump(...)\n",
    "    else:\n",
    "        logging.error(f\"[{model_name_rf}] Failed to train base model or Platt scaler.\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_rf}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "\n",
    "# --- 3.4 RF: Mondrian ICP Calibration ---\n",
    "fitted_cc_rf = None  # Initialize Mondrian classifier variable\n",
    "if fitted_rf_base and platt_scaler_rf:\n",
    "    if not y_cal.empty:\n",
    "        logging.info(f\"--- [{model_name_rf}] Calibrating Mondrian Conformal Prediction ---\")\n",
    "        mcp_cal_start_time_rf = time.time()\n",
    "\n",
    "        # Calculate probabilities needed for Mondrian ICP on Calibration set\n",
    "        base_probs_cal_rf = fitted_rf_base.predict_proba(X_cal)[:, 1].reshape(-1, 1)  # UNSCALED cal data, prob class 1\n",
    "        probs_cal_rf = platt_scaler_rf.predict_proba(base_probs_cal_rf)  # Calibrated probs for BOTH classes\n",
    "\n",
    "        # Define Mondrian Bins (Class-conditional example)\n",
    "        bins_cal_rf = y_cal.values  # Assumes y_cal is pd.Series/np.array\n",
    "\n",
    "        # Fit the Mondrian classifier\n",
    "        fitted_cc_rf = fit_mondrian_classifier(probs_cal_rf, bins_cal=bins_cal_rf)\n",
    "\n",
    "        mcp_cal_duration_rf = time.time() - mcp_cal_start_time_rf\n",
    "        if fitted_cc_rf:\n",
    "            logging.info(f\"--- [{model_name_rf}] Mondrian CP calibration finished in {mcp_cal_duration_rf:.2f} seconds ---\")\n",
    "            # Optional: Save the fitted_cc_rf object using joblib alongside the base model and Platt scaler\n",
    "            # cc_filename = os.path.join(MODEL_DIR, f\"{model_name_rf}_mondrian_classifier_{timestamp_rf}.joblib\")\n",
    "            # joblib.dump(fitted_cc_rf, cc_filename)\n",
    "            # logging.info(f\"Mondrian classifier saved to {cc_filename}\")\n",
    "        else:\n",
    "            logging.error(f\"[{model_name_rf}] Failed to fit Mondrian classifier.\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_rf}] Calibration set is empty. Skipping Mondrian ICP calibration.\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_rf}] Base model or Platt scaler not available. Skipping Mondrian ICP calibration.\")\n",
    "\n",
    "# --- 3.5 RF: Final Evaluation ---\n",
    "if fitted_rf_base and platt_scaler_rf:\n",
    "    logging.info(f\"--- [{model_name_rf}] Final Evaluation on Test Set ---\")\n",
    "    eval_start_time_rf = time.time()\n",
    "\n",
    "    # --- Calculate Base Metrics (Same as before) ---\n",
    "    base_probs_test_rf = fitted_rf_base.predict_proba(X_test)[:, 1].reshape(-1, 1)  # UNSCALED test data, prob class 1\n",
    "    probs_test_rf_full = platt_scaler_rf.predict_proba(base_probs_test_rf)  # Calibrated probs for BOTH classes\n",
    "    y_proba_test_rf = probs_test_rf_full[:, 1]\n",
    "    y_pred_test_rf = (y_proba_test_rf >= 0.5).astype(int)\n",
    "    metrics_rf = calculate_metrics(y_test, y_pred_test_rf, y_proba_test_rf, model_name=model_name_rf)\n",
    "\n",
    "    # --- Mondrian Conformal Prediction Evaluation ---\n",
    "    cp_coverage_mond_rf, cp_avg_set_size_mond_rf = None, None  # Initialize results\n",
    "\n",
    "    if fitted_cc_rf is not None:  # Check if Mondrian classifier was fitted successfully\n",
    "        mcp_eval_start_time_rf = time.time()\n",
    "        # Define Mondrian Bins for test set (Class-conditional example)\n",
    "        bins_test_rf = y_test.values if not y_test.empty else np.array([])\n",
    "        y_test_true_np = y_test.values if not y_test.empty else np.array([])\n",
    "\n",
    "        # Evaluate the fitted Mondrian classifier\n",
    "        cp_coverage_mond_rf, cp_avg_set_size_mond_rf, _, class_coverage_dict = evaluate_mondrian_prediction(\n",
    "            fitted_cc=fitted_cc_rf,            # Pass the fitted classifier\n",
    "            probs_test=probs_test_rf_full,     # Pass test probabilities (n_test, 2)\n",
    "            y_test_true=y_test_true_np,        # Pass true test labels\n",
    "            bins_test=bins_test_rf,            # Pass test bins\n",
    "            alpha=ALPHA\n",
    "        )\n",
    "        mcp_eval_duration_rf = time.time() - mcp_eval_start_time_rf\n",
    "        logging.info(f\"--- [{model_name_rf}] Mondrian CP evaluation finished in {mcp_eval_duration_rf:.2f} seconds ---\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_rf}] Skipping Mondrian CP evaluation: Classifier not fitted.\")\n",
    "\n",
    "    eval_duration_rf = time.time() - eval_start_time_rf  # Total eval time\n",
    "    logging.info(f\"--- [{model_name_rf}] Total Evaluation finished in {eval_duration_rf:.2f} seconds ---\")\n",
    "\n",
    "    # --- Store results (Same as before, using the new variables) ---\n",
    "    all_results[model_name_rf] = {\n",
    "        'metrics': metrics_rf,\n",
    "        'cp_coverage_mond': cp_coverage_mond_rf,           # Store Mondrian coverage\n",
    "        'cp_class_coverage_dict': class_coverage_dict,\n",
    "        'cp_avg_set_size_mond': cp_avg_set_size_mond_rf,   # Store Mondrian avg set size\n",
    "        'best_hpo_params': best_params_rf,\n",
    "        'hpo_f1_score': best_score_hpo_rf,\n",
    "        'hpo_duration_s': hpo_duration_rf,\n",
    "    }\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_rf}] Skipping final evaluation (Base model or Platt scaler not available).\")\n",
    "\n",
    "logging.info(f\"===== Finished Workflow for {model_name_rf} =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for XGB (using iterations)\n",
    "MAX_RESOURCE_XGB = 500 # Max n_estimators\n",
    "MIN_RESOURCE_XGB = 30  # Min n_estimators\n",
    "ETA_XGB = 3\n",
    "RESOURCE_TYPE_XGB = 'iterations'\n",
    "model_name_xgb = \"XGBoost\"\n",
    "ROUNDS = 20        # Number of rounds to wait for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODIFICADO PARA PRUEBA RPIDA!\n",
    "MAX_RESOURCE_XGB = 30  # Reducido drsticamente! (Antes 500)\n",
    "MIN_RESOURCE_XGB = 10  # Mnimo bajo pero cercano a max\n",
    "ETA_XGB = 4            # Aumentado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bracket s=0 (n=1, r0=30.00):   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Bracket s=0 (n=1, r0=30.00): 100%|| 1/1 [00:00<00:00,  4.48it/s, Best F1: 0.6058]\n",
      "                                                                        \r"
     ]
    }
   ],
   "source": [
    "logging.info(f\"\\n\\n===== Starting Workflow for {model_name_xgb} =====\")\n",
    "timestamp_xgb = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "hpo_start_time_xgb = time.time()\n",
    "\n",
    "# --- 4.1 XGB: Define Search Space and HPO Params ---\n",
    "param_space_xgb = {\n",
    "    # n_estimators controlled by resource\n",
    "    'learning_rate': loguniform(0.01, 0.3),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'subsample': uniform(0.6, 0.4), # range [0.6, 1.0)\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'gamma': loguniform(1e-2, 1.0), # Min loss reduction\n",
    "    'reg_alpha': loguniform(1e-3, 1.0), # L1 reg\n",
    "    'reg_lambda': loguniform(1e-3, 1.0), # L2 reg\n",
    "    # scale_pos_weight added automatically\n",
    "    # random_state added automatically\n",
    "    'objective': ['binary:logistic'], # Fixed objective\n",
    "    'eval_metric': ['logloss'],        # Fixed eval metric for early stopping\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# --- 4.2 XGB: Run Hyperband HPO ---\n",
    "logging.info(f\"--- [{model_name_xgb}] Running Hyperband HPO ---\")\n",
    "best_params_xgb, best_score_hpo_xgb = hyperband_hpo(\n",
    "    model_class=xgb.XGBClassifier,\n",
    "    param_space=param_space_xgb,\n",
    "    X_train=X_train, # USE UNSCALED DATA\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,     # USE UNSCALED DATA\n",
    "    y_val=y_val,\n",
    "    max_resource=MAX_RESOURCE_XGB,\n",
    "    eta=ETA_XGB,\n",
    "    resource_type=RESOURCE_TYPE_XGB,\n",
    "    min_resource=MIN_RESOURCE_XGB,\n",
    "    scoring_func=f1_score,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "hpo_duration_xgb = time.time() - hpo_start_time_xgb\n",
    "logging.info(f\"--- [{model_name_xgb}] HPO finished in {hpo_duration_xgb:.2f} seconds ---\")\n",
    "\n",
    "\n",
    "# --- 4.3 XGB: Train Final Model & Platt Scaler ---\n",
    "fitted_xgb_base = None\n",
    "platt_scaler_xgb = None\n",
    "final_best_params_xgb = None # Initialize\n",
    "\n",
    "if best_params_xgb:\n",
    "    logging.info(f\"--- [{model_name_xgb}] Determining best iteration and training Platt scaler ---\")\n",
    "    platt_start_time_xgb = time.time()\n",
    "\n",
    "    # 1. Determine best iteration using early stopping on validation set\n",
    "    temp_best_params_xgb = best_params_xgb.copy() # Work with a copy\n",
    "    temp_best_params_xgb['random_state'] = RANDOM_SEED\n",
    "    if 'objective' not in temp_best_params_xgb: temp_best_params_xgb['objective'] = 'binary:logistic'\n",
    "    if 'eval_metric' not in temp_best_params_xgb: temp_best_params_xgb['eval_metric'] = 'logloss'\n",
    "    if 'n_jobs' not in temp_best_params_xgb: temp_best_params_xgb['n_jobs'] = -1\n",
    "    if 'scale_pos_weight' not in temp_best_params_xgb:\n",
    "        neg_count = (y_train == 0).sum()\n",
    "        pos_count = (y_train == 1).sum()\n",
    "        if pos_count > 0:\n",
    "            temp_best_params_xgb['scale_pos_weight'] = neg_count / pos_count\n",
    "    #Define callbacks for the fit that determines the best iteration\n",
    "    xgb_final_iteration_callbacks = [\n",
    "            EarlyStopping(rounds=ROUNDS,        # Number of rounds to wait for improvement\n",
    "                            save_best=True,   # Saves the model from the best iteration\n",
    "                            metric_name='logloss', # Explicitly state the metric to monitor (optional but good practice)\n",
    "                            maximize=False)    # We want to minimize logloss\n",
    "        ]\n",
    "\n",
    "\n",
    "    logging.info(\"Training temporary XGBoost with early stopping to find best iteration...\")\n",
    "    temp_xgb_model = xgb.XGBClassifier(**temp_best_params_xgb, callbacks = xgb_final_iteration_callbacks)  \n",
    "    # Ensure X_val, y_val are appropriate (unscaled)\n",
    "    eval_set_final = [(X_val, y_val)]\n",
    "    temp_xgb_model.fit(X_train, y_train,\n",
    "                   eval_set=eval_set_final,\n",
    "                   verbose=False) \n",
    "    \n",
    "    # Retrieve the best iteration\n",
    "    best_iteration = temp_xgb_model.best_iteration\n",
    "    if best_iteration is None or best_iteration <= 0:\n",
    "        logging.warning(f\"Early stopping did not trigger or returned invalid iteration ({best_iteration}). Using max_resource ({MAX_RESOURCE_XGB}) as n_estimators.\")\n",
    "        best_iteration = MAX_RESOURCE_XGB\n",
    "    logging.info(f\"Best iteration found: {best_iteration}\")\n",
    "\n",
    "    # Update best_params with the optimal number of estimators found\n",
    "    final_best_params_xgb = temp_best_params_xgb.copy()\n",
    "    final_best_params_xgb['n_estimators'] = best_iteration\n",
    "\n",
    "    # 2. Train final model and Platt scaler using train_platt_scaler\n",
    "    logging.info(f\"--- [{model_name_xgb}] Training final model ({final_best_params_xgb['n_estimators']} est.) and Platt scaler ---\")\n",
    "    fitted_xgb_base, platt_scaler_xgb = train_platt_scaler(\n",
    "        base_estimator_class=xgb.XGBClassifier,\n",
    "        best_params=final_best_params_xgb,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        score_method='raw_margin_xgb',\n",
    "        n_splits=5,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    platt_duration_xgb = time.time() - platt_start_time_xgb\n",
    "    if fitted_xgb_base and platt_scaler_xgb:\n",
    "        logging.info(f\"--- [{model_name_xgb}] Platt scaling finished in {platt_duration_xgb:.2f} seconds ---\")\n",
    "        # Optional: Save models\n",
    "        # joblib.dump(...)\n",
    "    else:\n",
    "        logging.error(f\"[{model_name_xgb}] Failed to train base model or Platt scaler.\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_xgb}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "\n",
    "\n",
    "# --- 4.4 XGB: Mondrian ICP Calibration ---\n",
    "fitted_cc_xgb = None  # Initialize Mondrian classifier variable\n",
    "if fitted_xgb_base and platt_scaler_xgb:\n",
    "    if not y_cal.empty:\n",
    "        logging.info(f\"--- [{model_name_xgb}] Calibrating Mondrian Conformal Prediction ---\")\n",
    "        mcp_cal_start_time_xgb = time.time()\n",
    "\n",
    "        # Calculate probabilities needed for Mondrian ICP on Calibration set\n",
    "        base_raw_cal_xgb = fitted_xgb_base.predict(X_cal, output_margin=True).reshape(-1, 1)  # UNSCALED cal data\n",
    "        probs_cal_xgb = platt_scaler_xgb.predict_proba(base_raw_cal_xgb)  # Calibrated probs for BOTH classes\n",
    "\n",
    "        # Define Mondrian Bins (Class-conditional example)\n",
    "        bins_cal_xgb = y_cal.values  # Assumes y_cal is pd.Series/np.array\n",
    "\n",
    "        # Fit the Mondrian classifier\n",
    "        fitted_cc_xgb = fit_mondrian_classifier(probs_cal_xgb, bins_cal=bins_cal_xgb)\n",
    "\n",
    "        mcp_cal_duration_xgb = time.time() - mcp_cal_start_time_xgb\n",
    "        if fitted_cc_xgb:\n",
    "            logging.info(f\"--- [{model_name_xgb}] Mondrian CP calibration finished in {mcp_cal_duration_xgb:.2f} seconds ---\")\n",
    "            # Optional: Save the fitted_cc_xgb object using joblib alongside the base model and Platt scaler\n",
    "            # cc_filename = os.path.join(MODEL_DIR, f\"{model_name_xgb}_mondrian_classifier_{timestamp_xgb}.joblib\")\n",
    "            # joblib.dump(fitted_cc_xgb, cc_filename)\n",
    "            # logging.info(f\"Mondrian classifier saved to {cc_filename}\")\n",
    "        else:\n",
    "            logging.error(f\"[{model_name_xgb}] Failed to fit Mondrian classifier.\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_xgb}] Calibration set is empty. Skipping Mondrian ICP calibration.\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_xgb}] Base model or Platt scaler not available. Skipping Mondrian ICP calibration.\")\n",
    "\n",
    "# --- 4.5 XGB: Final Evaluation ---\n",
    "if fitted_xgb_base and platt_scaler_xgb:\n",
    "    logging.info(f\"--- [{model_name_xgb}] Final Evaluation on Test Set ---\")\n",
    "    eval_start_time_xgb = time.time()\n",
    "\n",
    "    # --- Calculate Base Metrics (Same as before) ---\n",
    "    base_raw_test_xgb = fitted_xgb_base.predict(X_test, output_margin=True).reshape(-1, 1)  # UNSCALED test data\n",
    "    probs_test_xgb_full = platt_scaler_xgb.predict_proba(base_raw_test_xgb)  # Calibrated probs for BOTH classes\n",
    "    y_proba_test_xgb = probs_test_xgb_full[:, 1]\n",
    "    y_pred_test_xgb = (y_proba_test_xgb >= 0.5).astype(int)\n",
    "    metrics_xgb = calculate_metrics(y_test, y_pred_test_xgb, y_proba_test_xgb, model_name=model_name_xgb)\n",
    "\n",
    "    # --- Mondrian Conformal Prediction Evaluation ---\n",
    "    cp_coverage_mond_xgb, cp_avg_set_size_mond_xgb = None, None  # Initialize results\n",
    "\n",
    "    if fitted_cc_xgb is not None:  # Check if Mondrian classifier was fitted successfully\n",
    "        mcp_eval_start_time_xgb = time.time()\n",
    "        # Define Mondrian Bins for test set (Class-conditional example)\n",
    "        bins_test_xgb = y_test.values if not y_test.empty else np.array([])\n",
    "        y_test_true_np = y_test.values if not y_test.empty else np.array([])\n",
    "\n",
    "        # Evaluate the fitted Mondrian classifier\n",
    "        cp_coverage_mond_xgb, cp_avg_set_size_mond_xgb, _, class_coverage_dict = evaluate_mondrian_prediction(\n",
    "            fitted_cc=fitted_cc_xgb,            # Pass the fitted classifier\n",
    "            probs_test=probs_test_xgb_full,     # Pass test probabilities (n_test, 2)\n",
    "            y_test_true=y_test_true_np,         # Pass true test labels\n",
    "            bins_test=bins_test_xgb,            # Pass test bins\n",
    "            alpha=ALPHA\n",
    "        )\n",
    "        mcp_eval_duration_xgb = time.time() - mcp_eval_start_time_xgb\n",
    "        logging.info(f\"--- [{model_name_xgb}] Mondrian CP evaluation finished in {mcp_eval_duration_xgb:.2f} seconds ---\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_xgb}] Skipping Mondrian CP evaluation: Classifier not fitted.\")\n",
    "\n",
    "    eval_duration_xgb = time.time() - eval_start_time_xgb  # Total eval time\n",
    "    logging.info(f\"--- [{model_name_xgb}] Total Evaluation finished in {eval_duration_xgb:.2f} seconds ---\")\n",
    "\n",
    "    # --- Store results (Same as before, using the new variables) ---\n",
    "    all_results[model_name_xgb] = {\n",
    "        'metrics': metrics_xgb,\n",
    "        'cp_coverage_mond': cp_coverage_mond_xgb,           # Store Mondrian coverage\n",
    "        'cp_class_coverage_dict': class_coverage_dict,\n",
    "        'cp_avg_set_size_mond': cp_avg_set_size_mond_xgb,   # Store Mondrian avg set size\n",
    "        'best_hpo_params': best_params_xgb,\n",
    "        'final_n_estimators': final_best_params_xgb.get('n_estimators', None) if final_best_params_xgb else None,\n",
    "        'hpo_f1_score': best_score_hpo_xgb,\n",
    "        'hpo_duration_s': hpo_duration_xgb,\n",
    "    }\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_xgb}] Skipping final evaluation (Base model or Platt scaler not available).\")\n",
    "\n",
    "logging.info(f\"===== Finished Workflow for {model_name_xgb} =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for LGBM (using iterations)\n",
    "MAX_RESOURCE_LGBM = 500 # Max n_estimators\n",
    "MIN_RESOURCE_LGBM = 30  # Min n_estimators\n",
    "ETA_LGBM = 3\n",
    "RESOURCE_TYPE_LGBM = 'iterations'\n",
    "model_name_lgbm = \"LightGBM\"\n",
    "ROUNDS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODIFICADO PARA PRUEBA RPIDA!\n",
    "MAX_RESOURCE_LGBM = 30  # Reducido drsticamente! (Antes 500)\n",
    "MIN_RESOURCE_LGBM = 10  # Mnimo bajo pero cercano a max\n",
    "ETA_LGBM = 4            # Aumentado\n",
    "RESOURCE_TYPE_LGBM = 'iterations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bracket s=0 (n=1, r0=30.00):   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Bracket s=0 (n=1, r0=30.00): 100%|| 1/1 [00:00<00:00,  6.03it/s, Best F1: 0.7617]\n",
      "                                                                        \r"
     ]
    }
   ],
   "source": [
    "logging.info(f\"\\n\\n===== Starting Workflow for {model_name_lgbm} =====\")\n",
    "timestamp_lgbm = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "hpo_start_time_lgbm = time.time()\n",
    "\n",
    "# --- 5.1 LGBM: Define Search Space and HPO Params ---\n",
    "param_space_lgbm = {\n",
    "    # n_estimators controlled by resource\n",
    "    'learning_rate': loguniform(0.01, 0.3),\n",
    "    'num_leaves': randint(20, 100),\n",
    "    'max_depth': randint(3, 15), # Often kept lower than XGB depth\n",
    "    'subsample': uniform(0.6, 0.4), # Aliased as bagging_fraction\n",
    "    'colsample_bytree': uniform(0.6, 0.4), # Aliased as feature_fraction\n",
    "    'reg_alpha': loguniform(1e-3, 1.0), # L1\n",
    "    'reg_lambda': loguniform(1e-3, 1.0), # L2\n",
    "    # scale_pos_weight or is_unbalance=True added automatically\n",
    "    # random_state added automatically\n",
    "    'objective': ['binary'], # Fixed objective\n",
    "    'metric': ['logloss'],   # Fixed metric for early stopping\n",
    "    'verbose': [-1]          # Suppress LightGBM's internal verbosity if desired\n",
    "}\n",
    "\n",
    "\n",
    "# --- 5.2 LGBM: Run Hyperband HPO ---\n",
    "logging.info(f\"--- [{model_name_lgbm}] Running Hyperband HPO ---\")\n",
    "best_params_lgbm, best_score_hpo_lgbm = hyperband_hpo(\n",
    "    model_class=lgb.LGBMClassifier,\n",
    "    param_space=param_space_lgbm,\n",
    "    X_train=X_train, # USE UNSCALED DATA\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,     # USE UNSCALED DATA\n",
    "    y_val=y_val,\n",
    "    max_resource=MAX_RESOURCE_LGBM,\n",
    "    eta=ETA_LGBM,\n",
    "    resource_type=RESOURCE_TYPE_LGBM,\n",
    "    min_resource=MIN_RESOURCE_LGBM,\n",
    "    scoring_func=f1_score,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "hpo_duration_lgbm = time.time() - hpo_start_time_lgbm\n",
    "logging.info(f\"--- [{model_name_lgbm}] HPO finished in {hpo_duration_lgbm:.2f} seconds ---\")\n",
    "\n",
    "\n",
    "# --- 5.3 LGBM: Train Final Model & Platt Scaler ---\n",
    "fitted_lgbm_base = None\n",
    "platt_scaler_lgbm = None\n",
    "final_best_params_lgbm = None # Initialize\n",
    "\n",
    "if best_params_lgbm:\n",
    "    logging.info(f\"--- [{model_name_lgbm}] Determining best iteration and training Platt scaler ---\")\n",
    "    platt_start_time_lgbm = time.time()\n",
    "\n",
    "    # 1. Determine best iteration using early stopping on validation set\n",
    "    temp_best_params_lgbm = best_params_lgbm.copy() # Work with a copy\n",
    "    temp_best_params_lgbm['random_state'] = RANDOM_SEED\n",
    "    if 'objective' not in temp_best_params_lgbm: temp_best_params_lgbm['objective'] = 'binary'\n",
    "    if 'metric' not in temp_best_params_lgbm: temp_best_params_lgbm['metric'] = 'logloss'\n",
    "    if 'n_jobs' not in temp_best_params_lgbm: temp_best_params_lgbm['n_jobs'] = -1\n",
    "    if 'verbose' not in temp_best_params_lgbm: temp_best_params_lgbm['verbose'] = -1 # Control verbosity\n",
    "    if 'scale_pos_weight' not in temp_best_params_lgbm:\n",
    "        neg_count = (y_train == 0).sum(); pos_count = (y_train == 1).sum()\n",
    "        if pos_count > 0:\n",
    "            temp_best_params_lgbm['scale_pos_weight'] = neg_count / pos_count\n",
    "            if 'is_unbalance' in temp_best_params_lgbm: del temp_best_params_lgbm['is_unbalance']\n",
    "        elif 'is_unbalance' not in temp_best_params_lgbm:\n",
    "            temp_best_params_lgbm['is_unbalance'] = True\n",
    "\n",
    "    # Define callbacks for *final* iteration finding\n",
    "    callbacks_final = [\n",
    "        early_stopping(stopping_rounds=ROUNDS, verbose=False)\n",
    "    ]\n",
    "\n",
    "    # Define the metric to monitor for early stopping\n",
    "    metric_to_monitor = 'logloss'  # Or 'auc', etc.\n",
    "\n",
    "    logging.info(f\"Training temporary LightGBM with early stopping (monitoring '{metric_to_monitor}') to find best iteration...\")\n",
    "    temp_lgbm_model = lgb.LGBMClassifier(**temp_best_params_lgbm)\n",
    "    eval_set_final_lgbm = [(X_val, y_val)]\n",
    "\n",
    "    # Check if eval_set is valid before fitting\n",
    "    if not eval_set_final_lgbm or not isinstance(eval_set_final_lgbm, list) or not eval_set_final_lgbm[0]:\n",
    "        raise ValueError(\"eval_set_final_lgbm is not correctly defined before fitting.\")\n",
    "    if len(eval_set_final_lgbm[0]) != 2:\n",
    "        raise ValueError(\"Each element in eval_set must be a tuple (X, y).\")\n",
    "\n",
    "    try:\n",
    "        temp_lgbm_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set_final_lgbm,\n",
    "            eval_metric=metric_to_monitor,\n",
    "            callbacks=callbacks_final\n",
    "        )\n",
    "\n",
    "        # Retrieve the best iteration\n",
    "        best_iteration_lgbm = temp_lgbm_model.best_iteration_\n",
    "        if best_iteration_lgbm is None or best_iteration_lgbm <= 0:\n",
    "            logging.warning(f\"Early stopping did not trigger or returned invalid iteration ({best_iteration_lgbm}). Using max_resource ({MAX_RESOURCE_LGBM}) as n_estimators.\")\n",
    "            best_iteration_lgbm = MAX_RESOURCE_LGBM\n",
    "        logging.info(f\"Best iteration found: {best_iteration_lgbm}\")\n",
    "\n",
    "        # Update best_params with the optimal number of estimators found\n",
    "        final_best_params_lgbm = temp_best_params_lgbm.copy()\n",
    "        final_best_params_lgbm['n_estimators'] = best_iteration_lgbm\n",
    "\n",
    "        # 2. Train final model and Platt scaler using train_platt_scaler\n",
    "        logging.info(f\"--- [{model_name_lgbm}] Training final model ({final_best_params_lgbm['n_estimators']} est.) and Platt scaler ---\")\n",
    "        fitted_lgbm_base, platt_scaler_lgbm = train_platt_scaler(\n",
    "            base_estimator_class=lgb.LGBMClassifier,\n",
    "            best_params=final_best_params_lgbm,  # Use params with best_iteration\n",
    "            X_train=X_train,  # Use UNSCALED training data\n",
    "            y_train=y_train,\n",
    "            score_method='raw_score_lgbm',  # <<< Specify score method for LGBM\n",
    "            n_splits=5,\n",
    "            random_state=RANDOM_SEED\n",
    "        )\n",
    "        platt_duration_lgbm = time.time() - platt_start_time_lgbm\n",
    "        if fitted_lgbm_base and platt_scaler_lgbm:\n",
    "            logging.info(f\"--- [{model_name_lgbm}] Platt scaling finished in {platt_duration_lgbm:.2f} seconds ---\")\n",
    "            # Optional: Save models\n",
    "            # joblib.dump(...)\n",
    "        else:\n",
    "            logging.error(f\"[{model_name_lgbm}] Failed to train base model or Platt scaler.\")\n",
    "    except ValueError as ve:\n",
    "        logging.error(f\"ValueError during temp_lgbm_model.fit: {ve}\")\n",
    "        logging.error(f\"Shapes: X_train={X_train.shape}, y_train={y_train.shape}, X_val={X_val.shape}, y_val={y_val.shape}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# --- 5.4 LGBM: Mondrian ICP Calibration ---\n",
    "fitted_cc_lgbm = None  # Initialize Mondrian classifier variable\n",
    "if fitted_lgbm_base and platt_scaler_lgbm:\n",
    "    if not y_cal.empty:\n",
    "        logging.info(f\"--- [{model_name_lgbm}] Calibrating Mondrian Conformal Prediction ---\")\n",
    "        mcp_cal_start_time_lgbm = time.time()\n",
    "\n",
    "        # Calculate probabilities needed for Mondrian ICP on Calibration set\n",
    "        base_raw_cal_lgbm = fitted_lgbm_base.predict(X_cal, raw_score=True).reshape(-1, 1)  # UNSCALED cal data\n",
    "        probs_cal_lgbm = platt_scaler_lgbm.predict_proba(base_raw_cal_lgbm)  # Calibrated probs for BOTH classes\n",
    "\n",
    "        # Define Mondrian Bins (Class-conditional example)\n",
    "        bins_cal_lgbm = y_cal.values  # Assumes y_cal is pd.Series/np.array\n",
    "\n",
    "        # Fit the Mondrian classifier\n",
    "        fitted_cc_lgbm = fit_mondrian_classifier(probs_cal_lgbm, bins_cal=bins_cal_lgbm)\n",
    "\n",
    "        mcp_cal_duration_lgbm = time.time() - mcp_cal_start_time_lgbm\n",
    "        if fitted_cc_lgbm:\n",
    "            logging.info(f\"--- [{model_name_lgbm}] Mondrian CP calibration finished in {mcp_cal_duration_lgbm:.2f} seconds ---\")\n",
    "            # Optional: Save the fitted_cc_lgbm object using joblib alongside the base model and Platt scaler\n",
    "            # cc_filename = os.path.join(MODEL_DIR, f\"{model_name_lgbm}_mondrian_classifier_{timestamp_lgbm}.joblib\")\n",
    "            # joblib.dump(fitted_cc_lgbm, cc_filename)\n",
    "            # logging.info(f\"Mondrian classifier saved to {cc_filename}\")\n",
    "        else:\n",
    "            logging.error(f\"[{model_name_lgbm}] Failed to fit Mondrian classifier.\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_lgbm}] Calibration set is empty. Skipping Mondrian ICP calibration.\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_lgbm}] Base model or Platt scaler not available. Skipping Mondrian ICP calibration.\")\n",
    "\n",
    "\n",
    "# --- 5.5 LGBM: Final Evaluation ---\n",
    "if fitted_lgbm_base and platt_scaler_lgbm:\n",
    "    logging.info(f\"--- [{model_name_lgbm}] Final Evaluation on Test Set ---\")\n",
    "    eval_start_time_lgbm = time.time()\n",
    "\n",
    "    # --- Calculate Base Metrics (Same as before) ---\n",
    "    base_raw_test_lgbm = fitted_lgbm_base.predict(X_test, raw_score=True).reshape(-1, 1)  # UNSCALED test data\n",
    "    probs_test_lgbm_full = platt_scaler_lgbm.predict_proba(base_raw_test_lgbm)  # Calibrated probs for BOTH classes\n",
    "    y_proba_test_lgbm = probs_test_lgbm_full[:, 1]\n",
    "    y_pred_test_lgbm = (y_proba_test_lgbm >= 0.5).astype(int)\n",
    "    metrics_lgbm = calculate_metrics(y_test, y_pred_test_lgbm, y_proba_test_lgbm, model_name=model_name_lgbm)\n",
    "\n",
    "    # --- Mondrian Conformal Prediction Evaluation ---\n",
    "    cp_coverage_mond_lgbm, cp_avg_set_size_mond_lgbm = None, None  # Initialize results\n",
    "\n",
    "    if fitted_cc_lgbm is not None:  # Check if Mondrian classifier was fitted successfully\n",
    "        mcp_eval_start_time_lgbm = time.time()\n",
    "        # Define Mondrian Bins for test set (Class-conditional example)\n",
    "        bins_test_lgbm = y_test.values if not y_test.empty else np.array([])\n",
    "        y_test_true_np = y_test.values if not y_test.empty else np.array([])\n",
    "\n",
    "        # Evaluate the fitted Mondrian classifier\n",
    "        cp_coverage_mond_lgbm, cp_avg_set_size_mond_lgbm, _, class_coverage_dict = evaluate_mondrian_prediction(\n",
    "            fitted_cc=fitted_cc_lgbm,            # Pass the fitted classifier\n",
    "            probs_test=probs_test_lgbm_full,     # Pass test probabilities (n_test, 2)\n",
    "            y_test_true=y_test_true_np,          # Pass true test labels\n",
    "            bins_test=bins_test_lgbm,            # Pass test bins\n",
    "            alpha=ALPHA\n",
    "        )\n",
    "        mcp_eval_duration_lgbm = time.time() - mcp_eval_start_time_lgbm\n",
    "        logging.info(f\"--- [{model_name_lgbm}] Mondrian CP evaluation finished in {mcp_eval_duration_lgbm:.2f} seconds ---\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_lgbm}] Skipping Mondrian CP evaluation: Classifier not fitted.\")\n",
    "\n",
    "    eval_duration_lgbm = time.time() - eval_start_time_lgbm  # Total eval time\n",
    "    logging.info(f\"--- [{model_name_lgbm}] Total Evaluation finished in {eval_duration_lgbm:.2f} seconds ---\")\n",
    "\n",
    "    # --- Store results (Same as before, using the new variables) ---\n",
    "    all_results[model_name_lgbm] = {\n",
    "        'metrics': metrics_lgbm,\n",
    "        'cp_coverage_mond': cp_coverage_mond_lgbm,           # Store Mondrian coverage\n",
    "        'cp_class_coverage_dict': class_coverage_dict,\n",
    "        'cp_avg_set_size_mond': cp_avg_set_size_mond_lgbm,   # Store Mondrian avg set size\n",
    "        'best_hpo_params': best_params_lgbm, # Original HPO params\n",
    "        # Store actual used estimators if available\n",
    "        'final_n_estimators': final_best_params_lgbm.get('n_estimators', None) if final_best_params_lgbm else None,\n",
    "        'hpo_f1_score': best_score_hpo_lgbm,\n",
    "        'hpo_duration_s': hpo_duration_lgbm,\n",
    "    }\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_lgbm}] Skipping final evaluation (Base model or Platt scaler not available).\")\n",
    "\n",
    "logging.info(f\"===== Finished Workflow for {model_name_lgbm} =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Performance Metrics Summary =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall_tpr</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>specificity_tnr</th>\n",
       "      <th>g_mean</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>brier_score</th>\n",
       "      <th>...</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>CP Coverage (Mondrian)</th>\n",
       "      <th>CP Avg Set Size (Mondrian)</th>\n",
       "      <th>CP Coverage (Mondrian, class 0)</th>\n",
       "      <th>CP Coverage (Mondrian, class 1)</th>\n",
       "      <th>HPO F1</th>\n",
       "      <th>HPO Duration (s)</th>\n",
       "      <th>Final Estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.9628</td>\n",
       "      <td>0.8494</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.6832</td>\n",
       "      <td>0.9923</td>\n",
       "      <td>0.7530</td>\n",
       "      <td>0.9356</td>\n",
       "      <td>0.7503</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>165</td>\n",
       "      <td>220</td>\n",
       "      <td>0.1055</td>\n",
       "      <td>0.1059</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.0857</td>\n",
       "      <td>0.6460</td>\n",
       "      <td>4.9489</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  accuracy  precision  recall_tpr  f1_score  specificity_tnr  g_mean  \\\n",
       "0   SVM    0.9628     0.8494      0.5714    0.6832           0.9923  0.7530   \n",
       "\n",
       "   roc_auc  pr_auc  brier_score  ...  FP   FN   TP  CP Coverage (Mondrian)  \\\n",
       "0   0.9356  0.7503       0.0306  ...  39  165  220                  0.1055   \n",
       "\n",
       "   CP Avg Set Size (Mondrian)  CP Coverage (Mondrian, class 0)  \\\n",
       "0                      0.1059                           0.1070   \n",
       "\n",
       "   CP Coverage (Mondrian, class 1)  HPO F1  HPO Duration (s)  Final Estimators  \n",
       "0                           0.0857  0.6460            4.9489               N/A  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_summary = []\n",
    "for model_name, results_data in all_results.items():\n",
    "    summary = {'Model': model_name}\n",
    "    metrics = results_data.get('metrics')\n",
    "    if metrics:\n",
    "        summary.update(metrics)\n",
    "        cm = summary.pop('confusion_matrix', None)\n",
    "        if cm:\n",
    "            summary['TN'] = cm.get('tn')\n",
    "            summary['FP'] = cm.get('fp')\n",
    "            summary['FN'] = cm.get('fn')\n",
    "            summary['TP'] = cm.get('tp')\n",
    "\n",
    "    # --- Use the new Mondrian CP results ---\n",
    "    summary['CP Coverage (Mondrian)'] = results_data.get('cp_coverage_mond')\n",
    "    summary['CP Avg Set Size (Mondrian)'] = results_data.get('cp_avg_set_size_mond')\n",
    "\n",
    "    # --- Add per-class Mondrian CP coverage ---\n",
    "    class_coverage_dict = results_data.get('cp_class_coverage_dict', {})\n",
    "    # Add columns for class 0 and class 1 coverage (use None if missing)\n",
    "    summary['CP Coverage (Mondrian, class 0)'] = class_coverage_dict.get(0, None)\n",
    "    summary['CP Coverage (Mondrian, class 1)'] = class_coverage_dict.get(1, None)\n",
    "\n",
    "    summary['HPO F1'] = results_data.get('hpo_f1_score')\n",
    "    summary['HPO Duration (s)'] = results_data.get('hpo_duration_s')\n",
    "    summary['Final Estimators'] = results_data.get('final_n_estimators', 'N/A') # Keep if relevant (not for SVM)\n",
    "    results_summary.append(summary)\n",
    "\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "\n",
    "# Set display options for float formatting\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}' if isinstance(x, float) else x)\n",
    "\n",
    "print(\"\\n===== Performance Metrics Summary =====\")\n",
    "from IPython.display import display # Make sure display is imported\n",
    "if not results_df.empty:\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display.\")\n",
    "\n",
    "\n",
    "# --- Update CSV saving ---\n",
    "results_csv_path = os.path.join(MODEL_DIR, f\"model_comparison_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "try:\n",
    "    results_df.to_csv(results_csv_path, index=False)\n",
    "    logging.info(f\"Results summary DataFrame saved to {results_csv_path}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to save results summary CSV: {e}\")\n",
    "\n",
    "\n",
    "# --- Optional Plotting (Update or Remove) ---\n",
    "# The existing plotting code for 'CP Empty %' and 'CP Multi-Class %'\n",
    "# might need to be removed or adapted if you don't calculate these stats\n",
    "# with the new mondrian_icp function.\n",
    "# For now, let's comment it out as crepes doesn't directly return these counts easily.\n",
    "\n",
    "# if not results_df.empty and 'CP Empty Sets' in results_df.columns and 'CP Multi-Class Sets' in results_df.columns:\n",
    "#    ... (keep the existing plot code commented out or remove it) ...\n",
    "# else:\n",
    "#    logging.warning(\"Could not plot CP set types: Results DataFrame is empty or missing required columns.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
