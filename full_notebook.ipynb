{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f92c47e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter nbconvert --to script full_notebook.ipynb\n",
    "#! Ver cómo cambiar el hyperband para que busque más configuraciones si saturan. O para que descarte las que saturan a una sola clase.\n",
    "#! Comparar con lo que encuentra Random Search y Grid search.\n",
    "#! El number of estimators del random forest, hacer una especie de early stopping hasta el máximo en el que se hizo el hpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f2cc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove any existing log files\n",
    "# import os\n",
    "# import glob\n",
    "# import logging\n",
    "\n",
    "# # Reset logger to avoid any issues with permissions\n",
    "# logging.shutdown()\n",
    "# # Remove loggers\n",
    "# for log_file in glob.glob(\"*.log\"):\n",
    "#     os.remove(log_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0f56a",
   "metadata": {},
   "source": [
    "# 0. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eba63ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # For confusion matrix heatmap\n",
    "from scipy.stats import randint, uniform, loguniform # Ensure loguniform is imported if used\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "import logging\n",
    "from tqdm import tqdm  \n",
    "from IPython.display import display\n",
    "#from tqdm.notebook import tqdm # Needs pip install ipywidgets\n",
    "#from tqdm.auto import tqdm\n",
    "import joblib # For saving/loading models efficiently\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, ParameterSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from scipy.stats import loguniform # For hyperparameter distributions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone, BaseEstimator, ClassifierMixin\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support, roc_auc_score,\n",
    "    brier_score_loss, precision_recall_curve, auc, f1_score, average_precision_score\n",
    ")   \n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "# Boosting models\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "\n",
    "# Configure logging\n",
    "logging.shutdown()\n",
    "logging.basicConfig(\n",
    "    filename=f'models_{datetime.now().strftime(\"%d_%H-%M-%S\")}.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True\n",
    ")\n",
    "# Prevent logs from being printed to console\n",
    "logging.getLogger().handlers = [h for h in logging.getLogger().handlers if isinstance(h, logging.FileHandler)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a8e78",
   "metadata": {},
   "source": [
    "## 1. Loading Dataset & Feature Selection\n",
    "\n",
    "- Group 1: Morphology (magnitudes with errors).\n",
    "- Group 2: Photometry (magnitudes with errors).\n",
    "- Group 3: Combined morphology and photometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2edcbbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the df\n",
    "df = pd.read_csv('data/match_alhambra_cosmos2020_ACS_class_0.8arcsec.csv')\n",
    "logging.info(f\"DataFrame created with shape: {df.shape}\")\n",
    "# Map ACS classification: 1 (Galaxy, Majority) -> 0, 2 (Star, minority) -> 1, 3 (Fake) -> drop\n",
    "logging.info(\"Original class counts:\")\n",
    "logging.info(df['acs_mu_class'].value_counts().to_string())\n",
    "\n",
    "# Drop fake detections (class 3)\n",
    "# Drop fake detections\n",
    "n_fakes = (df['acs_mu_class'] == 3).sum()\n",
    "logging.info(f\"Number of fake detections (class 3): {n_fakes}\")\n",
    "df = df[df['acs_mu_class'] != 3]\n",
    "\n",
    "# Map classifications\n",
    "df['acs_mu_class'] = df['acs_mu_class'].map({1: 0, 2: 1})\n",
    "\n",
    "logging.info(\"After dropping fakes and mapping classes (0: Galaxy, 1: Star):\")\n",
    "logging.info(df['acs_mu_class'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4a03fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features\n",
    "\n",
    "# --- Modeling features ---\n",
    "\n",
    "# 1. ALHAMBRA Morphology Features (SExtractor-based)\n",
    "morphology_features = [\n",
    "    'area', 'fwhm', 'stell', \n",
    "    'ell', #'a', 'b', #'theta', #a,b son fwhm y ell. Theta no da info.\n",
    "    'rk', \n",
    "    'rf'\n",
    "]\n",
    "\n",
    "morphology_err = [\n",
    "    's2n'\n",
    "]\n",
    "\n",
    "morphology_mags_errors = morphology_features + morphology_err\n",
    "\n",
    "# 2. ALHAMBRA Photometry (Optical + NIR + Synthetic)\n",
    "OPTICAL_MAG_COLS = [\n",
    "    'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W',\n",
    "    'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W',\n",
    "    'F799W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W'\n",
    "]\n",
    "photometry_magnitudes = (\n",
    "    OPTICAL_MAG_COLS +\n",
    "    ['J', 'H', 'KS', 'F814W']\n",
    ")\n",
    "\n",
    "OPTICAL_ERR_COLS = [\n",
    "    'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W',\n",
    "    'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W',\n",
    "    'dF799W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W'\n",
    "]\n",
    "photometry_uncertainties = (\n",
    "    OPTICAL_ERR_COLS +\n",
    "    ['dJ', 'dH', 'dKS', 'dF814W']\n",
    ")\n",
    "\n",
    "photometry_mags_errors = photometry_magnitudes + photometry_uncertainties\n",
    "\n",
    "# Target from acs\n",
    "target_variable = ['acs_mu_class'] # The COSMOS classification label to predict\n",
    "\n",
    "# --- Non-Modeling features ---\n",
    "\n",
    "# ALHAMBRA Photometric Redshift & Derived Features (BPZ-based)\n",
    "redshift_magnitudes = [\n",
    "    'zb_1', 'zb_Min_1', 'zb_Max_1', 'Tb_1',\n",
    "    'z_ml', 't_ml',\n",
    "    'Stell_Mass_1', 'M_Abs_1', 'MagPrior'\n",
    "]\n",
    "\n",
    "redshift_uncertainties = [\n",
    "    'Odds_1', 'Chi2'\n",
    "]\n",
    "\n",
    "redshift_mags_errors = redshift_magnitudes + redshift_uncertainties\n",
    "\n",
    "# ALHAMBRA Quality/Auxiliary Features (per-band quality etc.)\n",
    "OPTICAL_IRMS_COLS = [\n",
    "    'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W',\n",
    "    'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W',\n",
    "    'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W',\n",
    "    'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W'\n",
    "]\n",
    "quality_aux_features = (\n",
    "    ['nfobs'] +\n",
    "    OPTICAL_IRMS_COLS +\n",
    "    ['irms_J', 'irms_H', 'irms_KS', 'irms_F814W']\n",
    ")\n",
    "\n",
    "# Flags and identifiers\n",
    "\n",
    "non_modeling_identifiers = ['ID_1', 'id_2'] # ALHAMBRA ID, COSMOS ID\n",
    "\n",
    "non_modeling_astrometry = [\n",
    "    'RA_1', 'Dec_1', 'x', 'y', # ALHAMBRA Astrometry\n",
    "    'ra_2', 'dec_2',          # COSMOS Astrometry\n",
    "    'Separation'              # Matching Quality\n",
    "]\n",
    "\n",
    "non_modeling_flags = [\n",
    "    'photoflag', 'xray', 'PercW', 'Satur_Flag', # ALHAMBRA Object/Photometry Flags\n",
    "    'irms_OPT_Flag', 'irms_NIR_Flag'           # ALHAMBRA Overall Quality Flags\n",
    "]\n",
    "\n",
    "alhambra_prediction = ['Stellar_Flag'] # ALHAMBRA's own classification\n",
    "\n",
    "non_modeling_aperture_mags = [ # Specific aperture mags, usually use total mags\n",
    "    'F814W_3arcs', 'dF814W_3arcs', 'F814W_3arcs_corr'\n",
    "]\n",
    "\n",
    "non_modeling_cosmos_features = [ # Measurements/flags derived from COSMOS data (HST, HSC, VISTA...)\n",
    "    'model_flag',\n",
    "    'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista',\n",
    "    'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid',\n",
    "    'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid',\n",
    "    'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid',\n",
    "    'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid',\n",
    "    'acs_f814w_mag', 'acs_f814w_magerr',\n",
    "    'acs_fwhm_world', 'acs_mu_max',\n",
    "    'solution_model' # This is categorical, but still COSMOS-derived info\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Storing features\n",
    "feature_sets = {\n",
    "        # --- Potential Input Feature Sets ---\n",
    "        'morphology_only': morphology_mags_errors,\n",
    "        'photometry_magnitudes_only': photometry_magnitudes,\n",
    "        'photometry_mags_errors': photometry_mags_errors,\n",
    "        'photometry_plus_morphology': photometry_mags_errors + morphology_mags_errors,\n",
    "        'photometry_no_redshift': photometry_mags_errors + morphology_mags_errors + quality_aux_features,\n",
    "        'redshift_only': redshift_mags_errors,\n",
    "        'full_alhambra_all': (morphology_mags_errors +\n",
    "                            photometry_mags_errors +\n",
    "                            redshift_mags_errors + \n",
    "                            quality_aux_features),\n",
    "\n",
    "        # --- Excluded Feature Sets ---\n",
    "        'non_modeling_identifiers': non_modeling_identifiers,\n",
    "        'non_modeling_astrometry': non_modeling_astrometry,\n",
    "        'non_modeling_flags': non_modeling_flags,\n",
    "        'non_modeling_aperture_mags': non_modeling_aperture_mags,\n",
    "        'non_modeling_cosmos_features': non_modeling_cosmos_features,\n",
    "        'alhambra_prediction': alhambra_prediction,\n",
    "        'target_variable': target_variable\n",
    "    }\n",
    "\n",
    "# Groups of features for modeling\n",
    "groups = {\n",
    "        'group_1': feature_sets.get('morphology_only', []) + feature_sets.get('target_variable', []),\n",
    "        'group_2': feature_sets.get('photometry_mags_errors', []) + feature_sets.get('target_variable', []),\n",
    "        'group_3': (feature_sets.get('morphology_only', []) +\n",
    "                   feature_sets.get('photometry_mags_errors', []) +\n",
    "                   feature_sets.get('target_variable', []))\n",
    "    }\n",
    "\n",
    "# --- Function to get a specific feature set ---\n",
    "\n",
    "def get_feature_set(df, set_name, groups = groups):\n",
    "    \"\"\"\n",
    "    Selects columns from a DataFrame based on a predefined feature set name,\n",
    "    including six specific groups defined by combinations of morphology,\n",
    "    photometry magnitudes, uncertainties, and redshift features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        set_name (str): The name of the desired feature set group:\n",
    "                        'group_1' to 'group_6' as defined below.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing only the columns\n",
    "                      belonging to the specified feature set group.\n",
    "                      Returns an empty DataFrame if no columns are found.\n",
    "    \"\"\"\n",
    "\n",
    "    if set_name not in groups:\n",
    "        raise ValueError(f\"Feature set group '{set_name}' not defined. \"\n",
    "                         f\"Available groups: {list(groups.keys())}\")\n",
    "\n",
    "    required_cols_in_set = groups[set_name]\n",
    "\n",
    "    # Find which of these columns actually exist in the DataFrame\n",
    "    available_cols = [col for col in required_cols_in_set if col in df.columns]\n",
    "\n",
    "    # Warn if some columns from the set definition are missing\n",
    "    missing_cols = [col for col in required_cols_in_set if col not in available_cols]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: The following columns defined for feature set group '{set_name}'\"\n",
    "              f\" were not found in the DataFrame and will be excluded: {missing_cols}\")\n",
    "\n",
    "    if not available_cols:\n",
    "        print(f\"Warning: No columns for feature set group '{set_name}' found in the DataFrame.\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "\n",
    "    print(f\"Selecting feature set group '{set_name}' with {len(available_cols)} columns.\")\n",
    "    return df[available_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca368d9",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55b117b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting parameters\n",
    "TEST_SIZE = 0.20 # Test set proportion\n",
    "VAL_SIZE = 0.10 # Validation set proportion\n",
    "CAL_SIZE = 0.00 # Calibration set proportion\n",
    "# Train size will be 1 - (TEST_SIZE + VAL_SIZE + CAL_SIZE)\n",
    "\n",
    "TARGET_COLUMN = feature_sets.get('target_variable', [])[0]\n",
    "RANDOM_SEED = 33 # For reproducibility\n",
    "\n",
    "# Model saving directory\n",
    "MODEL_DIR = \"trained_models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Data splitting strategy ('stratified' or 'random')\n",
    "SPLIT_STRATEGY = 'stratified' # Recommended for imbalanced datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "561ab2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Cleaning ---\n",
    "def clean_data(df, feature_group, target_column, logger=logging):\n",
    "    \"\"\"\n",
    "    Cleans the input DataFrame by selecting features for the given group,\n",
    "    dropping NaNs, and separating features and target.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        feature_group (str): The feature group to use (e.g., 'group_1', 'group_2', etc.).\n",
    "        target_column (str): The name of the target column.\n",
    "        logger (logging.Logger): Logger for info and error messages.\n",
    "\n",
    "    Returns:\n",
    "        X (pd.DataFrame): Cleaned feature DataFrame.\n",
    "        y (pd.Series): Target variable.\n",
    "        df_clean (pd.DataFrame): The cleaned DataFrame (features + target).\n",
    "    \"\"\"\n",
    "    logger.info(f\"Original dataset size: {df.shape}\")\n",
    "\n",
    "    # Get the feature columns for the selected group using get_feature_set\n",
    "    df_clean = get_feature_set(df, feature_group).dropna().copy()\n",
    "    logger.info(f\"Dataset size after dropping NaNs: {df_clean.shape}\")\n",
    "\n",
    "    # Ensure target_column is defined correctly\n",
    "    if target_column not in df_clean.columns:\n",
    "        raise KeyError(f\"Target column '{target_column}' not found in the cleaned DataFrame columns: {df_clean.columns.tolist()}\")\n",
    "\n",
    "    # Log value counts for target\n",
    "    logger.info(f\"Value counts for target:\\n1 (Star): {(df_clean[target_column] == 1).sum()}\\n0 (Galaxy): {(df_clean[target_column] == 0).sum()}\")\n",
    "\n",
    "    # Separate features (X) and target (y) for the cleaned DataFrame\n",
    "    X = df_clean.drop(columns=[target_column])\n",
    "    y = df_clean[target_column]\n",
    "    return X, y, df_clean\n",
    "\n",
    "# Example usage:\n",
    "# X, y, df_clean = clean_data(df, feature_group='group_7', target_column=TARGET_COLUMN, logger=logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4576d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Splitting ---\n",
    "def split_data(X, y):\n",
    "    \"\"\"\n",
    "    Splits the data into train, validation, test, and calibration sets according to the global\n",
    "    split proportions and strategy. Uses global variables:\n",
    "        - TEST_SIZE, VAL_SIZE, CAL_SIZE, SPLIT_STRATEGY, RANDOM_SEED\n",
    "\n",
    "    The logic and split order is identical to the original code.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature DataFrame.\n",
    "        y (pd.Series): Target variable.\n",
    "\n",
    "    Returns:\n",
    "        (X_train, y_train, X_val, y_val, X_test, y_test, X_cal, y_cal): tuple of splits.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Splitting data using '{SPLIT_STRATEGY}' strategy...\")\n",
    "\n",
    "    # --- Validate Proportions ---\n",
    "    if not (0 <= TEST_SIZE <= 1 and 0 <= VAL_SIZE <= 1 and 0 <= CAL_SIZE <= 1):\n",
    "        raise ValueError(\"Split proportions (TEST_SIZE, VAL_SIZE, CAL_SIZE) must be between 0 and 1.\")\n",
    "\n",
    "    TRAIN_SIZE = 1.0 - TEST_SIZE - VAL_SIZE - CAL_SIZE\n",
    "    if not (0 <= TRAIN_SIZE <= 1):\n",
    "        raise ValueError(f\"Calculated TRAIN_SIZE ({TRAIN_SIZE:.3f}) is invalid. Sum of TEST_SIZE, VAL_SIZE, and CAL_SIZE must be between 0 and 1.\")\n",
    "\n",
    "    if not np.isclose(TRAIN_SIZE + TEST_SIZE + VAL_SIZE + CAL_SIZE, 1.0):\n",
    "        # This check might be redundant given the calculation of TRAIN_SIZE, but good for safety.\n",
    "        raise ValueError(\"Sum of split proportions must be equal to 1.\")\n",
    "\n",
    "    if np.isclose(TRAIN_SIZE, 0) and (np.isclose(VAL_SIZE, 0) or np.isclose(TEST_SIZE, 0) or np.isclose(CAL_SIZE, 0)):\n",
    "        # Avoid scenarios where train is 0 but other splits are also 0, leading to ambiguity.\n",
    "        # If only train is 0, it might be valid in some rare cases, but usually requires at least one other non-zero split.\n",
    "        # Let's enforce Train > 0 for typical ML workflows.\n",
    "        # If you need zero training data, adjust this check.\n",
    "        logging.warning(\"TRAIN_SIZE is zero or near zero. Ensure this is intended.\")\n",
    "        if TRAIN_SIZE < 0: # Definitely an error\n",
    "            raise ValueError(\"TRAIN_SIZE cannot be negative.\")\n",
    "        # Allow TRAIN_SIZE = 0 only if explicitly handled later, otherwise raise error?\n",
    "        # For now, let's proceed but log a warning. If TRAIN_SIZE must be > 0, uncomment the raise below.\n",
    "        # raise ValueError(\"TRAIN_SIZE must be greater than 0 for typical model training.\")\n",
    "\n",
    "    logging.info(f\"Target split ratios: Train={TRAIN_SIZE:.2f}, Val={VAL_SIZE:.2f}, Test={TEST_SIZE:.2f}, Cal={CAL_SIZE:.2f}\")\n",
    "\n",
    "    # --- Initialize Splits ---\n",
    "    # Use iloc[0:0] to create empty DataFrames/Series with the same columns/dtype\n",
    "    empty_X = X.iloc[0:0]\n",
    "    empty_y = y.iloc[0:0]\n",
    "    X_train, y_train = empty_X.copy(), empty_y.copy()\n",
    "    X_val, y_val = empty_X.copy(), empty_y.copy()\n",
    "    X_test, y_test = empty_X.copy(), empty_y.copy()\n",
    "    X_cal, y_cal = empty_X.copy(), empty_y.copy()\n",
    "\n",
    "    # Temporary variables for sequential splitting\n",
    "    X_remaining, y_remaining = X.copy(), y.copy() # Use copies to avoid modifying original X, y\n",
    "\n",
    "    # --- Stratification Option ---\n",
    "    # Define stratify_func only once\n",
    "    def get_stratify_array(y_arr):\n",
    "        return y_arr if SPLIT_STRATEGY == 'stratified' and not y_arr.empty else None\n",
    "\n",
    "    # --- First Split: Train vs. Remainder (Val + Test + Cal) ---\n",
    "    val_test_cal_size = VAL_SIZE + TEST_SIZE + CAL_SIZE\n",
    "\n",
    "    if np.isclose(val_test_cal_size, 0): # Only Train set needed\n",
    "        X_train, y_train = X_remaining, y_remaining\n",
    "        logging.info(\"All data assigned to Train set (Val, Test, Cal sizes are 0).\")\n",
    "        X_remaining, y_remaining = empty_X.copy(), empty_y.copy() # No remainder\n",
    "    elif np.isclose(TRAIN_SIZE, 0): # No Train set needed\n",
    "        logging.info(\"Train set is empty (TRAIN_SIZE=0). Remainder passed to next splits.\")\n",
    "        # X_remaining, y_remaining already hold all data\n",
    "    else: # Split Train vs Remainder\n",
    "        split_test_size = val_test_cal_size # Proportion of remainder relative to total (1.0)\n",
    "        X_train, X_remaining, y_train, y_remaining = train_test_split(\n",
    "            X_remaining, y_remaining,\n",
    "            test_size=split_test_size,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=get_stratify_array(y_remaining)\n",
    "        )\n",
    "    logging.info(f\"Train set shape: {X_train.shape}\")\n",
    "\n",
    "    # --- Second Split: Val vs. Remainder (Test + Cal) ---\n",
    "    if not X_remaining.empty:\n",
    "        test_cal_size = TEST_SIZE + CAL_SIZE\n",
    "        # Denominator for relative size calculation: size of the current remaining pool\n",
    "        current_remaining_size_frac = VAL_SIZE + test_cal_size # = val_test_cal_size\n",
    "\n",
    "        if np.isclose(VAL_SIZE, 0): # No Val set, pass remainder to next stage\n",
    "            X_temp2, y_temp2 = X_remaining, y_remaining # Remainder is Test + Cal\n",
    "            logging.info(\"Validation set is empty (VAL_SIZE=0).\")\n",
    "        elif np.isclose(test_cal_size, 0): # Only Val set left in remainder\n",
    "            X_val, y_val = X_remaining, y_remaining\n",
    "            X_temp2, y_temp2 = empty_X.copy(), empty_y.copy() # No data left for Test/Cal\n",
    "            logging.info(f\"Validation set shape: {X_val.shape}\")\n",
    "        else: # Split Val vs (Test + Cal)\n",
    "            # Proportion of (Test + Cal) relative to (Val + Test + Cal)\n",
    "            split_test_size = test_cal_size / current_remaining_size_frac\n",
    "            X_val, X_temp2, y_val, y_temp2 = train_test_split(\n",
    "                X_remaining, y_remaining,\n",
    "                test_size=split_test_size,\n",
    "                random_state=RANDOM_SEED,\n",
    "                stratify=get_stratify_array(y_remaining)\n",
    "            )\n",
    "            logging.info(f\"Validation set shape: {X_val.shape}\")\n",
    "    else: # No data remaining after train split\n",
    "        X_temp2, y_temp2 = empty_X.copy(), empty_y.copy()\n",
    "        if not np.isclose(VAL_SIZE, 0): # Log only if Val set was expected\n",
    "            logging.info(\"Validation set is empty (no data remaining after train split).\")\n",
    "\n",
    "    # --- Third Split: Test vs. Cal ---\n",
    "    if not X_temp2.empty:\n",
    "        # Denominator for relative size calculation: size of the current remaining pool\n",
    "        current_remaining_size_frac = TEST_SIZE + CAL_SIZE # = test_cal_size\n",
    "\n",
    "        if np.isclose(CAL_SIZE, 0): # No Cal set, remainder is Test\n",
    "            X_test, y_test = X_temp2, y_temp2\n",
    "            logging.info(\"Calibration set is empty (CAL_SIZE=0).\")\n",
    "        elif np.isclose(TEST_SIZE, 0): # Only Cal set left in remainder\n",
    "            X_cal, y_cal = X_temp2, y_temp2\n",
    "            logging.info(\"Test set is empty (TEST_SIZE=0).\")\n",
    "        else: # Split Test vs Cal\n",
    "            # Proportion of Cal relative to (Test + Cal)\n",
    "            split_test_size = CAL_SIZE / current_remaining_size_frac\n",
    "            X_test, X_cal, y_test, y_cal = train_test_split(\n",
    "                X_temp2, y_temp2,\n",
    "                test_size=split_test_size,\n",
    "                random_state=RANDOM_SEED,\n",
    "                stratify=get_stratify_array(y_temp2)\n",
    "            )\n",
    "            # Logging shapes done after the if/else block\n",
    "    else: # No data remaining for Test/Cal split\n",
    "        if not (np.isclose(TEST_SIZE, 0) and np.isclose(CAL_SIZE, 0)): # Log only if Test or Cal were expected\n",
    "            logging.info(\"Test and Calibration sets are empty (no data remaining for final split).\")\n",
    "\n",
    "    # Log final shapes for Test and Cal\n",
    "    logging.info(f\"Test set shape: {X_test.shape}\")\n",
    "    logging.info(f\"Calibration set shape: {X_cal.shape}\")\n",
    "\n",
    "    # --- Verification and Final Logging ---\n",
    "    total_len = len(X_train) + len(X_val) + len(X_test) + len(X_cal)\n",
    "    original_len = len(X)\n",
    "\n",
    "    if total_len != original_len:\n",
    "        # Calculate actual proportions based on lengths\n",
    "        actual_train = len(X_train) / original_len if original_len > 0 else 0\n",
    "        actual_val = len(X_val) / original_len if original_len > 0 else 0\n",
    "        actual_test = len(X_test) / original_len if original_len > 0 else 0\n",
    "        actual_cal = len(X_cal) / original_len if original_len > 0 else 0\n",
    "        logging.warning(f\"Total split length ({total_len}) does not exactly match original length ({original_len}). \"\n",
    "                        f\"This can happen with stratification or rounding. \"\n",
    "                        f\"Target proportions: Train={TRAIN_SIZE:.3f}, Val={VAL_SIZE:.3f}, Test={TEST_SIZE:.3f}, Cal={CAL_SIZE:.3f}. \"\n",
    "                        f\"Actual proportions: Train={actual_train:.3f}, Val={actual_val:.3f}, Test={actual_test:.3f}, Cal={actual_cal:.3f}\")\n",
    "    else:\n",
    "        logging.info(\"Split lengths verification successful.\")\n",
    "\n",
    "    logging.info(\"Data splitting complete.\")\n",
    "\n",
    "    # Log distributions, handling empty sets\n",
    "    def log_distribution(name, y_set):\n",
    "        if y_set.empty:\n",
    "            logging.info(f\"{name} target distribution: Set is empty.\")\n",
    "        else:\n",
    "            try:\n",
    "                # Use normalize=True, handle potential division by zero if counts are zero (though unlikely if not empty)\n",
    "                counts = y_set.value_counts()\n",
    "                dist = counts / counts.sum() if counts.sum() > 0 else counts\n",
    "                logging.info(f\"{name} target distribution:\\n{dist}\")\n",
    "                # Log absolute counts as well for clarity\n",
    "                logging.info(f\"{name} target counts:\\n{counts}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Could not calculate distribution for {name}: {e}\")\n",
    "                # Attempt to log raw value counts even if normalization fails\n",
    "                try:\n",
    "                    logging.info(f\"{name} raw value counts:\\n{y_set.value_counts()}\")\n",
    "                except Exception as e_raw:\n",
    "                    logging.error(f\"Could not get raw value counts for {name}: {e_raw}\")\n",
    "\n",
    "    log_distribution(\"Train\", y_train)\n",
    "    log_distribution(\"Validation\", y_val)\n",
    "    log_distribution(\"Test\", y_test)\n",
    "    log_distribution(\"Calibration\", y_cal)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_cal, y_cal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1aa63",
   "metadata": {},
   "source": [
    "# 3. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25e5249",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization via Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fc67c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#                      DEFINITIVE HPO FUNCTIONS BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Internal Helper for Hyperband ---\n",
    "def _train_and_eval(model_class, params,\n",
    "                    X_train, y_train, X_val, y_val,\n",
    "                    resource, resource_type,\n",
    "                    scoring_func, random_state):\n",
    "    \"\"\"Internal helper function to train and evaluate a single configuration for Hyperband.\"\"\"\n",
    "    try:\n",
    "        model = model_class(**params)\n",
    "        fit_duration = 0.0\n",
    "        start_fit = time.time()\n",
    "\n",
    "        if resource_type == 'data_fraction':\n",
    "            # If the resource is 1.0, use the full dataset directly.\n",
    "            # Otherwise, perform the split.\n",
    "            if np.isclose(resource, 1.0):\n",
    "                X_subset, y_subset = X_train, y_train\n",
    "            else:\n",
    "                # Use a fraction of the data for training\n",
    "                train_size = resource if resource > 0 else 0.1 # Ensure non-zero train_size\n",
    "                X_subset, _, y_subset, _ = train_test_split(\n",
    "                    X_train, y_train, train_size=train_size, random_state=random_state, stratify=y_train\n",
    "                )\n",
    "            \n",
    "            y_subset_np = y_subset.values if isinstance(y_subset, pd.Series) else y_subset\n",
    "            model.fit(X_subset, y_subset_np)\n",
    "            fit_duration = time.time() - start_fit\n",
    "\n",
    "        elif resource_type == 'iterations':\n",
    "            # Use a fraction of the iterations (n_estimators) for training\n",
    "            params_iter = params.copy()\n",
    "            params_iter['n_estimators'] = int(max(1, resource))\n",
    "            model = model_class(**params_iter)\n",
    "\n",
    "            current_fit_args = {}\n",
    "            eval_set_for_fit = [(X_val, y_val)]\n",
    "\n",
    "            if model_class is xgb.XGBClassifier:\n",
    "                current_fit_args['eval_set'] = eval_set_for_fit\n",
    "                current_fit_args['verbose'] = False\n",
    "\n",
    "            # CORRECTED: Only one 'elif' for LightGBM\n",
    "            elif model_class is lgb.LGBMClassifier:\n",
    "                current_fit_args['eval_set'] = eval_set_for_fit\n",
    "                # Pass the metric if it's in the HPO params, otherwise it will use the model's default\n",
    "                if 'metric' in params_iter and params_iter['metric']:\n",
    "                     current_fit_args['eval_metric'] = params_iter['metric']\n",
    "\n",
    "            y_train_np = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "            model.fit(X_train, y_train_np, **current_fit_args)\n",
    "            fit_duration = time.time() - start_fit\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid resource_type. Choose 'data_fraction' or 'iterations'.\")\n",
    "\n",
    "        # Evaluate on the full validation set\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        y_val_np = y_val.values if isinstance(y_val, pd.Series) else y_val\n",
    "        score = scoring_func(y_val_np, y_pred_val)\n",
    "        return score\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in _train_and_eval for config {params} with resource {resource}: {e}\", exc_info=False)\n",
    "        return -1.0\n",
    "\n",
    "# --- Helper function for Random and Grid Search (uses full resource) ---\n",
    "def _train_and_eval_single_config(model_class, params, X_train, y_train, X_val, y_val, scoring_func, random_state):\n",
    "    \"\"\"Trains and evaluates a single model configuration with full resources.\"\"\"\n",
    "    try:\n",
    "        params_to_fit = params.copy()\n",
    "        if 'random_state' not in params_to_fit and hasattr(model_class(random_state=1), 'random_state'):\n",
    "            params_to_fit['random_state'] = random_state\n",
    "        if model_class in [SVC, RandomForestClassifier, DecisionTreeClassifier] and 'class_weight' not in params_to_fit:\n",
    "            params_to_fit['class_weight'] = 'balanced'\n",
    "        if model_class in [xgb.XGBClassifier, lgb.LGBMClassifier] and 'scale_pos_weight' not in params_to_fit:\n",
    "            neg_count, pos_count = np.bincount(y_train)\n",
    "            if pos_count > 0: params_to_fit['scale_pos_weight'] = neg_count / pos_count\n",
    "        if model_class is lgb.LGBMClassifier and 'objective' not in params_to_fit:\n",
    "            params_to_fit['objective'] = 'binary'\n",
    "            \n",
    "        model = model_class(**params_to_fit)\n",
    "        \n",
    "        fit_args = {}\n",
    "        if model_class is xgb.XGBClassifier or model_class is lgb.LGBMClassifier:\n",
    "            fit_args['eval_set'] = [(X_val, y_val)]\n",
    "            if model_class is xgb.XGBClassifier: fit_args['verbose'] = False\n",
    "        \n",
    "        y_train_np = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "        model.fit(X_train, y_train_np, **fit_args)\n",
    "        \n",
    "        y_pred_val = model.predict(X_val)\n",
    "        y_val_np = y_val.values if isinstance(y_val, pd.Series) else y_val\n",
    "        return scoring_func(y_val_np, y_pred_val)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in _train_and_eval_single_config for config {params}: {e}\", exc_info=False)\n",
    "        return -1.0\n",
    "\n",
    "# --- Random Search HPO ---\n",
    "def random_search_hpo(model_class, param_space, X_train, y_train, X_val, y_val, n_iter, scoring_func, random_state):\n",
    "    \"\"\"Performs Random Search HPO.\"\"\"\n",
    "    logging.info(f\"--- Starting Random Search (n_iter={n_iter}) ---\")\n",
    "    start_time = time.time()\n",
    "    param_sampler = ParameterSampler(param_space, n_iter=n_iter, random_state=random_state)\n",
    "    best_score, best_params, evaluated = -1.0, None, 0\n",
    "    \n",
    "    for params in tqdm(param_sampler, desc=\"Random Search\", total=n_iter, leave=False):\n",
    "        score = _train_and_eval_single_config(model_class, params, X_train, y_train, X_val, y_val, scoring_func, random_state)\n",
    "        if score > best_score:\n",
    "            best_score, best_params = score, params\n",
    "        evaluated += 1\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    return {'best_params': best_params, 'best_score': best_score, 'total_time': total_time, 'n_configs': evaluated, 'mean_time_per_config': total_time / evaluated if evaluated > 0 else 0}\n",
    "\n",
    "# --- Grid Search HPO ---\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "def grid_search_hpo(model_class, grid_param_space, X_train, y_train, X_val, y_val, scoring_func, random_state):\n",
    "    \"\"\"Performs Grid Search HPO.\"\"\"\n",
    "    param_grid = list(ParameterGrid(grid_param_space))\n",
    "    n_configs = len(param_grid)\n",
    "    logging.info(f\"--- Starting Grid Search (n_configs={n_configs}) ---\")\n",
    "    start_time = time.time()\n",
    "    best_score, best_params = -1.0, None\n",
    "    \n",
    "    for params in tqdm(param_grid, desc=\"Grid Search\", total=n_configs, leave=False):\n",
    "        score = _train_and_eval_single_config(model_class, params, X_train, y_train, X_val, y_val, scoring_func, random_state)\n",
    "        if score > best_score:\n",
    "            best_score, best_params = score, params\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    return {'best_params': best_params, 'best_score': best_score, 'total_time': total_time, 'n_configs': n_configs, 'mean_time_per_config': total_time / n_configs if n_configs > 0 else 0}\n",
    "\n",
    "# --- Hyperband HPO (returns a dictionary) ---\n",
    "def hyperband_hpo(model_class, param_space,\n",
    "                  X_train, y_train, X_val, y_val,\n",
    "                  max_resource, eta=3, resource_type='iterations', min_resource=1,\n",
    "                  scoring_func=f1_score, random_state=None, config_multiplier=1):\n",
    "    \"\"\"Performs Hyperband HPO. MODIFIED: Returns a dictionary with detailed stats.\"\"\"\n",
    "    start_time = time.time()\n",
    "    s_max = int(math.log(max_resource / min_resource, eta)) if max_resource > min_resource and min_resource > 0 else 0\n",
    "    B = (s_max + 1) * max_resource\n",
    "    logging.info(f\"--- Starting Hyperband HPO ---\")\n",
    "    \n",
    "    best_params, best_score, total_evals = None, -1.0, 0\n",
    "    \n",
    "    for s in range(s_max, -1, -1):\n",
    "        n = int(math.ceil(B / max_resource / (s + 1)) * eta**s) * config_multiplier\n",
    "        r = max_resource * eta**(-s)\n",
    "        \n",
    "        param_list = list(ParameterSampler(param_space, n_iter=n, random_state=random_state + s if random_state else None))\n",
    "        \n",
    "        for p in param_list:\n",
    "             if 'random_state' not in p and hasattr(model_class(random_state=1), 'random_state'): p['random_state'] = random_state\n",
    "             if model_class in [SVC, RandomForestClassifier, DecisionTreeClassifier] and 'class_weight' not in p: p['class_weight'] = 'balanced'\n",
    "\n",
    "        for i in range(s + 1):\n",
    "            n_i, r_i = len(param_list), min(r * eta**i, max_resource)\n",
    "            round_scores = [( _train_and_eval(model_class, p, X_train, y_train, X_val, y_val, r_i, resource_type, scoring_func, random_state), p) for p in param_list]\n",
    "            total_evals += len(round_scores)\n",
    "            round_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "            \n",
    "            if abs(r_i - max_resource) < 1e-6 and round_scores and round_scores[0][0] > best_score:\n",
    "                best_score, best_params = round_scores[0][0], round_scores[0][1]\n",
    "\n",
    "            n_keep = int(n_i / eta)\n",
    "            if n_keep < 1: break\n",
    "            param_list = [p for score, p in round_scores[:n_keep]]\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    logging.info(f\"Hyperband finished in {total_time:.2f}s. Best score: {best_score:.4f}\")\n",
    "    return {'best_params': best_params, 'best_score': best_score, 'total_time': total_time, 'n_configs': total_evals, 'mean_time_per_config': total_time / total_evals if total_evals > 0 else 0}\n",
    "\n",
    "# --- HPO Comparison Orchestrator ---\n",
    "def run_hpo_comparison(\n",
    "    model_class, model_name,\n",
    "    param_space_continuous, param_space_grid,\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    hpo_settings, scoring_func, random_state\n",
    "):\n",
    "    \"\"\"Runs HPO comparison and returns the overall best result.\"\"\"\n",
    "    # logging.info(f\"\\n{'='*20} Starting HPO Comparison for {model_name} {'='*20}\")\n",
    "    \n",
    "    hyperband_res = hyperband_hpo(model_class, param_space_continuous, X_train, y_train, X_val, y_val, scoring_func=scoring_func, random_state=random_state, **hpo_settings['hyperband'])\n",
    "    # random_res = random_search_hpo(model_class, param_space_continuous, X_train, y_train, X_val, y_val, scoring_func=scoring_func, random_state=random_state, **hpo_settings['random'])\n",
    "    # grid_res = grid_search_hpo(model_class, param_space_grid, X_train, y_train, X_val, y_val, scoring_func=scoring_func, random_state=random_state)\n",
    "    \n",
    "    results_map = {\"Hyperband\": hyperband_res#, \"Random Search\": random_res, \"Grid Search\": grid_res\n",
    "                    }\n",
    "    best_method = max(results_map, key=lambda k: results_map[k]['best_score'])\n",
    "    \n",
    "    # logging.info(f\"\\n--- HPO Comparison Summary for {model_name} ---\")\n",
    "    # for method, res in results_map.items():\n",
    "    #     logging.info(f\"  Method: {method} | Best Score: {res['best_score']:.4f} | Time: {res['total_time']:.2f}s | Configs: {res['n_configs']}\")\n",
    "    logging.info(f\"--- Overall Best Method: {best_method} (Score: {results_map[best_method]['best_score']:.4f}) ---\")\n",
    "    \n",
    "    return {'best_params_overall': results_map[best_method]['best_params'], 'best_score_overall': results_map[best_method]['best_score'], 'best_method': best_method, 'all_results': results_map}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe9f2d",
   "metadata": {},
   "source": [
    "### Calibration (Platt Scaling/Isotonic Regression/Cross Venn Abers Predictors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4a1430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raw_cvap import RawVennAbers, CVAPPredictorRaw\n",
    "\n",
    "# --- Helper Function to Get Scores ---\n",
    "def get_scores(estimator, X, score_method):\n",
    "    \"\"\"Gets scores from an estimator based on the specified method.\"\"\"\n",
    "    if score_method == 'decision_function':\n",
    "        if hasattr(estimator, 'decision_function'):\n",
    "            scores = estimator.decision_function(X)\n",
    "            # Ensure scores are 1D for binary classification\n",
    "            if scores.ndim == 2 and scores.shape[1] == 1:\n",
    "                 scores = scores.flatten()\n",
    "            elif scores.ndim > 1:\n",
    "                 # For binary, decision_function should be 1D. If not, maybe multiclass? Raise error.\n",
    "                 raise ValueError(f\"decision_function returned shape {scores.shape}, expected 1D for binary classification.\")\n",
    "            return scores\n",
    "        else:\n",
    "            raise AttributeError(f\"{estimator.__class__.__name__} does not have 'decision_function' method.\")\n",
    "    elif score_method == 'predict_proba':\n",
    "        if hasattr(estimator, 'predict_proba'):\n",
    "            # Return probability of the positive class (class 1)\n",
    "            proba = estimator.predict_proba(X)\n",
    "            if proba.shape[1] != 2:\n",
    "                 raise ValueError(f\"predict_proba returned shape {proba.shape}, expected (n_samples, 2)\")\n",
    "            return proba[:, 1]\n",
    "        else:\n",
    "            raise AttributeError(f\"{estimator.__class__.__name__} does not have 'predict_proba' method.\")\n",
    "    elif score_method == 'raw_margin_xgb':\n",
    "        # Check if it looks like an XGBoost model (basic check)\n",
    "        # A more robust check might involve isinstance(estimator, xgboost.XGBModel) after importing xgboost\n",
    "        if hasattr(estimator, 'predict') and hasattr(estimator, 'get_params') and 'objective' in estimator.get_params():\n",
    "             try:\n",
    "                 # XGBoost convention: predict with output_margin=True gives raw scores\n",
    "                 # For binary classification, this is usually a single value per instance\n",
    "                 scores = estimator.predict(X, output_margin=True)\n",
    "                 return scores.flatten() # Ensure 1D\n",
    "             except TypeError as e:\n",
    "                  # Check if the error message specifically mentions 'output_margin'\n",
    "                  if 'output_margin' in str(e):\n",
    "                      raise TypeError(f\"'output_margin' might not be a valid parameter for predict in this XGBoost version or configuration. Error: {e}\")\n",
    "                  else:\n",
    "                      raise TypeError(f\"Error calling predict with output_margin=True on {estimator.__class__.__name__}. Is it an XGBoost model? Original error: {e}\")\n",
    "\n",
    "        else:\n",
    "             raise AttributeError(f\"{estimator.__class__.__name__} might not be an XGBoost model supporting 'output_margin'.\")\n",
    "    elif score_method == 'raw_score_lgbm':\n",
    "         # Check if it looks like a LightGBM model (basic check)\n",
    "         # A more robust check might involve isinstance(estimator, lightgbm.LGBMModel) after importing lightgbm\n",
    "        if hasattr(estimator, 'predict') and hasattr(estimator, 'get_params') and 'objective' in estimator.get_params():\n",
    "             try:\n",
    "                 # LightGBM convention: predict with raw_score=True gives raw scores\n",
    "                 # For binary classification, output shape might depend on objective.\n",
    "                 # Often (n_samples,) or (n_samples, 1) for binary logloss/cross_entropy\n",
    "                 scores = estimator.predict(X, raw_score=True)\n",
    "                 # Handle potential (n_samples, 1) output for binary\n",
    "                 if scores.ndim == 2 and scores.shape[1] == 1:\n",
    "                     scores = scores.flatten()\n",
    "                 elif scores.ndim != 1:\n",
    "                      # If multiclass raw_score=True might return (n_samples, n_classes)\n",
    "                      raise ValueError(f\"LightGBM raw_score returned shape {scores.shape}. Expected 1D for binary.\")\n",
    "                 return scores\n",
    "             except TypeError as e:\n",
    "                 # Check if the error message specifically mentions 'raw_score'\n",
    "                 if 'raw_score' in str(e):\n",
    "                      raise TypeError(f\"'raw_score' might not be a valid parameter for predict in this LightGBM version or configuration. Error: {e}\")\n",
    "                 else:\n",
    "                      raise TypeError(f\"Error calling predict with raw_score=True on {estimator.__class__.__name__}. Is it a LightGBM model? Original error: {e}\")\n",
    "        else:\n",
    "             raise AttributeError(f\"{estimator.__class__.__name__} might not be a LightGBM model supporting 'raw_score'.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported score_method: {score_method}. Choose 'decision_function', 'predict_proba', 'raw_margin_xgb', or 'raw_score_lgbm'.\")\n",
    "\n",
    "# --- Loading and saving models ---\n",
    "def _get_save_path(model_dir: str, model_name: str, group_name: str, calibration_method: str) -> str:\n",
    "    \"\"\"\n",
    "    Constructs the standardized file path including a group subfolder\n",
    "    and ensures the target directory exists.\n",
    "\n",
    "    Args:\n",
    "        model_dir (str): The base directory for saving models.\n",
    "        model_name (str): Name of the model (e.g., 'SVM').\n",
    "        group_name (str): Name of the data group (e.g., 'group_1'). Used for subfolder.\n",
    "        calibration_method (str): The calibration method ('platt', 'isotonic', 'cvap').\n",
    "\n",
    "    Returns:\n",
    "        str: The full, absolute path to the save file.\n",
    "             Example: /path/to/models/group_1/SVM_group_1_platt.joblib\n",
    "\n",
    "    Raises:\n",
    "        OSError: If the directory cannot be created due to permissions or other OS issues.\n",
    "    \"\"\"\n",
    "    # Define the filename (remains the same as before)\n",
    "    filename = f\"{model_name}_{group_name}_{calibration_method}.joblib\"\n",
    "\n",
    "    # Define the target directory path including the group subfolder\n",
    "    # Use group_name as the subfolder name\n",
    "    target_directory = os.path.join(model_dir, group_name)\n",
    "\n",
    "    # Ensure the target directory exists; create it if necessary\n",
    "    try:\n",
    "        # os.makedirs creates parent directories as needed (like model_dir if it doesn't exist)\n",
    "        # exist_ok=True prevents an error if the directory already exists\n",
    "        os.makedirs(target_directory, exist_ok=True)\n",
    "        logging.debug(f\"Ensured directory exists: {target_directory}\")\n",
    "    except OSError as e:\n",
    "        # Log the error and re-raise it so the calling function knows something went wrong\n",
    "        logging.error(f\"Could not create directory {target_directory}: {e}\", exc_info=True)\n",
    "        raise  # Re-raise the exception to halt execution if directory creation fails\n",
    "\n",
    "    # Construct the full file path by joining the target directory and filename\n",
    "    full_file_path = os.path.join(target_directory, filename)\n",
    "\n",
    "    # Optional: Return absolute path for clarity, especially if model_dir might be relative\n",
    "    return os.path.abspath(full_file_path)\n",
    "\n",
    "def save_predictor(predictor_object, file_path: str, model_name: str):\n",
    "    \"\"\"Saves the fitted predictor object (CalibratedClassifierCV or CVAPPredictorRaw) to a file.\"\"\"\n",
    "    try:\n",
    "        joblib.dump(predictor_object, file_path)\n",
    "        logging.info(f\"--- [{model_name}] Model with calibrator saved successfully to {file_path} ---\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"--- [{model_name}] Failed to save model with calibrator to {file_path}: {e} ---\", exc_info=True)\n",
    "\n",
    "def load_predictor(file_path: str, model_name: str):\n",
    "    \"\"\"Loads the fitted predictor object from a file.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        logging.info(f\"--- [{model_name}] Predictor save file not found at {file_path}. Proceeding with training. ---\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        loaded_predictor = joblib.load(file_path)\n",
    "        # Basic checks: Is it an object? Does it have predict_proba? Is it fitted?\n",
    "        if loaded_predictor is not None and hasattr(loaded_predictor, 'predict_proba'):\n",
    "            # Check if it appears fitted (works for CalibratedClassifierCV, need similar for CVAPPredictorRaw if available)\n",
    "            try:\n",
    "                # Check if the base estimator inside CalibratedClassifierCV is fitted\n",
    "                if isinstance(loaded_predictor, CalibratedClassifierCV):\n",
    "                     # Access the fitted base estimator(s)\n",
    "                     estimators_to_check = []\n",
    "                     if hasattr(loaded_predictor, 'calibrated_classifiers_') and loaded_predictor.calibrated_classifiers_:\n",
    "                         estimators_to_check = [cc.base_estimator for cc in loaded_predictor.calibrated_classifiers_]\n",
    "                     elif hasattr(loaded_predictor, 'base_estimator_'): # For older sklearn? Or if fitted directly without CV part?\n",
    "                          estimators_to_check = [loaded_predictor.base_estimator_]\n",
    "\n",
    "                     if not estimators_to_check:\n",
    "                          raise ValueError(\"Could not find base estimator(s) in loaded CalibratedClassifierCV\")\n",
    "\n",
    "                     # Check if *all* base estimators are fitted\n",
    "                     all_fitted = all(hasattr(est, \"classes_\") or isinstance(getattr(est, \"_is_fitted\", lambda: False)(), bool) for est in estimators_to_check) # Use _is_fitted where available\n",
    "                     if not all_fitted:\n",
    "                          logging.warning(f\"--- [{model_name}] Loaded predictor from {file_path} has unfitted base estimator(s). Retraining. ---\")\n",
    "                          return None\n",
    "                # Add check for CVAPPredictorRaw if it has a standard fitted attribute or method\n",
    "                elif isinstance(loaded_predictor, CVAPPredictorRaw):\n",
    "                     # Assuming CVAPPredictorRaw has a final_estimator_ that should be fitted\n",
    "                     if not hasattr(loaded_predictor, 'final_estimator_') or not hasattr(loaded_predictor.final_estimator_, \"classes_\"):\n",
    "                           logging.warning(f\"--- [{model_name}] Loaded CVAPPredictorRaw from {file_path} seems incomplete or unfitted. Retraining. ---\")\n",
    "                           return None\n",
    "                else:\n",
    "                     # For other types, maybe just rely on predict_proba existing\n",
    "                     pass\n",
    "\n",
    "                logging.info(f\"--- [{model_name}] Predictor loaded successfully from {file_path} ---\")\n",
    "                return loaded_predictor\n",
    "\n",
    "            except (AttributeError, ValueError, TypeError) as fit_check_err:\n",
    "                 logging.warning(f\"--- [{model_name}] Loaded predictor from {file_path} raised error during fitted check ({fit_check_err}). Assuming invalid. Retraining. ---\")\n",
    "                 return None\n",
    "        else:\n",
    "            logging.warning(f\"--- [{model_name}] Loaded object from {file_path} is not a valid predictor (None or no predict_proba). Proceeding with training. ---\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"--- [{model_name}] Failed to load predictor from {file_path}: {e}. Proceeding with training. ---\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main Unified Function ---\n",
    "def train_calibrate_model(base_estimator_class, best_params, X_train, y_train,\n",
    "                          model_name: str,\n",
    "                          group_name: str,\n",
    "                          MODEL_DIR: str,\n",
    "                          calibration_method='platt', # 'platt', 'isotonic', 'cvap'\n",
    "                          n_splits=5, random_state=None,\n",
    "                          # CVAP specific params\n",
    "                          score_method='decision_function', # 'decision_function', 'predict_proba', 'raw_margin_xgb', 'raw_score_lgbm'\n",
    "                          cvap_loss='log', # 'log' or 'brier' for aggregation\n",
    "                          cvap_precision=None, # Precision for rounding scores in VA fit\n",
    "                          # Platt/Isotonic specific params (CalibratedClassifierCV handles score method)\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Trains a base estimator and calibrates it, or loads a pre-trained/calibrated predictor.\n",
    "\n",
    "    Saves/Loads the single fitted predictor object (CalibratedClassifierCV or CVAPPredictorRaw)\n",
    "    into a file specific to the calibration method.\n",
    "\n",
    "    Args:\n",
    "        base_estimator_class: Class of the base estimator (e.g., SVC, RandomForestClassifier).\n",
    "        best_params (dict): Dictionary of best hyperparameters for the base estimator.\n",
    "        X_train (pd.DataFrame or np.ndarray): Training features.\n",
    "        y_train (pd.Series or np.ndarray): Training labels (binary 0/1).\n",
    "        model_name (str): Name of the model (e.g., 'SVM').\n",
    "        group_name (str): Name of the data group (e.g., 'group_1').\n",
    "        MODEL_DIR (str): Directory to save/load models.\n",
    "        calibration_method (str): 'platt', 'isotonic', or 'cvap'.\n",
    "        n_splits (int): Number of folds for cross-validation.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "        score_method (str): Method for CVAP scores ('decision_function', 'predict_proba', etc.).\n",
    "        cvap_loss (str): Aggregation loss for CVAP ('log' or 'brier').\n",
    "        cvap_precision (int, optional): Precision for CVAP VennAbers fit.\n",
    "\n",
    "    Returns:\n",
    "        object or None:\n",
    "            - Fitted predictor object with a `predict_proba` method\n",
    "              (either CalibratedClassifierCV or CVAPPredictorRaw).\n",
    "            - Returns None if an error occurs during training/loading or if loading fails validation.\n",
    "    \"\"\"\n",
    "    logging.info(f\"--- Preparing Model Training & Calibration ({calibration_method}) for {model_name} ({group_name}) ---\")\n",
    "\n",
    "    # --- Check for existing saved predictor first ---\n",
    "    save_path = _get_save_path(MODEL_DIR, model_name, group_name, calibration_method)\n",
    "    loaded_predictor = load_predictor(save_path, model_name)\n",
    "    if loaded_predictor is not None:\n",
    "        logging.info(f\"--- Using pre-trained and calibrated predictor from {save_path} ---\")\n",
    "        return loaded_predictor\n",
    "    # If loaded_predictor is None, load_predictor already logged the reason (not found or invalid)\n",
    "\n",
    "    # --- Proceed with training if predictor not loaded ---\n",
    "    logging.info(f\"--- Starting Model Training & Calibration ({calibration_method}) ---\")\n",
    "\n",
    "    # Input Type Handling (same as before)\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        X_train_np = X_train.values\n",
    "    else:\n",
    "        X_train_np = np.asarray(X_train)\n",
    "    if isinstance(y_train, pd.Series):\n",
    "        y_train_np = y_train.values\n",
    "    else:\n",
    "        y_train_np = np.asarray(y_train)\n",
    "    if len(np.unique(y_train_np)) != 2:\n",
    "        logging.error(f\"This function currently supports only binary classification. Found labels: {np.unique(y_train_np)}\")\n",
    "        raise ValueError(f\"Binary classification required. Found labels: {np.unique(y_train_np)}\")\n",
    "\n",
    "    # Instantiate the base estimator (same as before, including SVC probability logic)\n",
    "    try:\n",
    "        current_params = best_params.copy()\n",
    "        is_svc = issubclass(base_estimator_class, SVC)\n",
    "\n",
    "        needs_proba = False\n",
    "        # Check if base estimator *needs* predict_proba for the chosen calibration approach\n",
    "        if calibration_method in ['platt', 'isotonic']:\n",
    "            # CalibratedClassifierCV can use decision_function OR predict_proba.\n",
    "            # It automatically uses decision_function if available, unless method='isotonic'\n",
    "            # AND the base estimator only has predict_proba.\n",
    "            # Forcing probability=True for SVC with Platt is common practice, though not strictly required by CCCV if decision_function exists.\n",
    "            # Let's keep the original logic for SVC to be safe.\n",
    "            if is_svc: needs_proba = True\n",
    "        elif calibration_method == 'cvap' and score_method == 'predict_proba':\n",
    "             needs_proba = True # Only if base estimator needs probability for predict_proba\n",
    "\n",
    "        if needs_proba and is_svc and not current_params.get('probability', False):\n",
    "             logging.warning(f\"Setting probability=True for SVC as required by calibration setup.\")\n",
    "             current_params['probability'] = True\n",
    "        elif needs_proba and not hasattr(base_estimator_class(**current_params), 'predict_proba'):\n",
    "             # If we absolutely need predict_proba (e.g., CVAP with score_method='predict_proba')\n",
    "             # and the estimator doesn't have it, it's an issue.\n",
    "             logging.error(f\"Configuration requires 'predict_proba' (score_method='{score_method}'), \"\n",
    "                            f\"but {base_estimator_class.__name__} with params {current_params} might not support it.\")\n",
    "             # Decide whether to raise an error or just warn\n",
    "             # Raising an error is safer:\n",
    "             raise AttributeError(f\"{base_estimator_class.__name__} does not support 'predict_proba' needed for this configuration.\")\n",
    "             # Alternatively, warn and proceed, hoping get_scores handles it (though it will likely fail there):\n",
    "             # logging.warning(f\"Score method '{score_method}' requires 'predict_proba', but {base_estimator_class.__name__} might not support it with current params. Proceeding, but check estimator capabilities.\")\n",
    "\n",
    "        base_estimator = base_estimator_class(**current_params)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error instantiating base estimator {base_estimator_class.__name__} with params {current_params}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "    # --- Calibration Method Logic ---\n",
    "    final_predictor = None # This will hold the object to be returned/saved\n",
    "\n",
    "    try:\n",
    "        if calibration_method in ['platt', 'isotonic']:\n",
    "            logging.info(f\"Using CalibratedClassifierCV with method='{'sigmoid' if calibration_method == 'platt' else 'isotonic'}'\")\n",
    "\n",
    "            cv_strategy = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "            # Use the instantiated base_estimator. CalibratedClassifierCV clones it internally for CV.\n",
    "            # It then refits a final base estimator on all data.\n",
    "            calibrator = CalibratedClassifierCV(\n",
    "                base_estimator, # Pass the configured instance\n",
    "                method='sigmoid' if calibration_method == 'platt' else 'isotonic',\n",
    "                cv=cv_strategy,\n",
    "                n_jobs=-1,\n",
    "                #!ensemble=False # Default, ensures one final model refit on all data\n",
    "            )\n",
    "\n",
    "            logging.info(\"Fitting CalibratedClassifierCV...\")\n",
    "            calibrator.fit(X_train_np, y_train_np)\n",
    "            # No need to set fitted_ manually, sklearn does this.\n",
    "            logging.info(\"CalibratedClassifierCV fitting complete.\")\n",
    "\n",
    "            # The 'calibrator' object IS the final predictor we want.\n",
    "            final_predictor = calibrator\n",
    "\n",
    "            logging.info(f\"--- {calibration_method.capitalize()} Calibration Training Complete ---\")\n",
    "            # Save the single CalibratedClassifierCV object\n",
    "            save_predictor(final_predictor, save_path, model_name)\n",
    "            return final_predictor\n",
    "\n",
    "\n",
    "        elif calibration_method == 'cvap':\n",
    "            logging.info(f\"Using Cross Venn-Abers Prediction (CVAP) with score_method='{score_method}'\")\n",
    "\n",
    "            cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "            oof_scores_list, oof_y_cal_list = [], []\n",
    "            fold_estimators = [] # Store fold estimators if needed later, otherwise remove\n",
    "\n",
    "            logging.info(\"Generating out-of-fold scores for CVAP...\")\n",
    "            for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_np, y_train_np)):\n",
    "                logging.debug(f\"Fitting fold {fold+1}/{n_splits}\")\n",
    "                # Clone the original base estimator config for each fold\n",
    "                est = clone(base_estimator).fit(X_train_np[train_idx], y_train_np[train_idx])\n",
    "                scores = get_scores(est, X_train_np[val_idx], score_method)\n",
    "                oof_scores_list.append(scores)\n",
    "                oof_y_cal_list.append(y_train_np[val_idx])\n",
    "                fold_estimators.append(est) # Optional: keep if needed elsewhere\n",
    "            logging.info(\"Out-of-fold score generation complete.\")\n",
    "\n",
    "            logging.info(\"Fitting final base model on all data...\")\n",
    "            # Clone again to ensure a fresh fit on all data\n",
    "            final_base_estimator = clone(base_estimator).fit(X_train_np, y_train_np)\n",
    "            logging.info(\"Final base model fitting complete.\")\n",
    "\n",
    "            logging.info(\"Fitting VennAbers calibrators for each fold...\")\n",
    "            calibrators = [\n",
    "                RawVennAbers(precision=cvap_precision).fit(scores, y_cal)\n",
    "                for scores, y_cal in zip(oof_scores_list, oof_y_cal_list)\n",
    "            ]\n",
    "            logging.info(\"VennAbers fitting complete.\")\n",
    "\n",
    "            # Wrap them into CVAPPredictorRaw\n",
    "            cvap_predictor = CVAPPredictorRaw(\n",
    "                final_estimator_=final_base_estimator,\n",
    "                calibrators_=calibrators,\n",
    "                loss_=cvap_loss,\n",
    "                score_method_=score_method\n",
    "                # Note: CVAPPredictorRaw should internally set fitted_=True or provide predict_proba\n",
    "            )\n",
    "            # Manually set fitted_ flag if CVAPPredictorRaw doesn't do it\n",
    "            if not hasattr(cvap_predictor, 'fitted_'):\n",
    "                cvap_predictor.fitted_ = True\n",
    "\n",
    "            final_predictor = cvap_predictor\n",
    "            logging.info(\"--- CVAP Training Complete ---\")\n",
    "\n",
    "            # Save the single CVAPPredictorRaw object\n",
    "            save_predictor(final_predictor, save_path, model_name)\n",
    "            return final_predictor\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown calibration_method: {calibration_method}. Choose 'platt', 'isotonic', or 'cvap'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during {calibration_method} calibration for {model_name}: {e}\", exc_info=True)\n",
    "        return None # Return None on any error during training/calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca834ca",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9538c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Comprehensive Metrics ---\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_proba, model_name=\"Model\", conf_mat = False):\n",
    "    \"\"\"\n",
    "    Calculates a comprehensive set of classification metrics.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Ground truth labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        y_proba (array-like): Predicted probabilities for the positive class (class 1).\n",
    "        model_name (str): Name of the model for logging.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing calculated metrics.\n",
    "              Returns None if input arrays are empty or invalid.\n",
    "    \"\"\"\n",
    "    if len(y_true) == 0 or len(y_pred) == 0 or len(y_proba) == 0:\n",
    "        logging.error(f\"[{model_name}] Empty input arrays provided for metric calculation.\")\n",
    "        return None\n",
    "    if len(y_true) != len(y_pred) or len(y_true) != len(y_proba):\n",
    "        logging.error(f\"[{model_name}] Mismatched lengths in input arrays for metric calculation.\")\n",
    "        return None\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # --- Threshold-based Metrics (using y_pred) ---\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['precision'] = precision\n",
    "    metrics['recall_tpr'] = recall # True Positive Rate (Sensitivity)\n",
    "    metrics['f1_score'] = f1\n",
    "\n",
    "    # Specificity (True Negative Rate)\n",
    "    metrics['specificity_tnr'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "    # Geometric Mean\n",
    "    metrics['g_mean'] = np.sqrt(metrics['recall_tpr'] * metrics['specificity_tnr'])\n",
    "\n",
    "    # Confusion Matrix\n",
    "    metrics['confusion_matrix'] = {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp}\n",
    "\n",
    "    # --- Ranking/Probabilistic Metrics (using y_proba) ---\n",
    "    try:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_true, y_proba)\n",
    "    except ValueError as e:\n",
    "        logging.warning(f\"[{model_name}] Could not calculate ROC AUC: {e}. Setting to 0.0.\")\n",
    "        metrics['roc_auc'] = 0.0 # Handle cases with only one class present\n",
    "\n",
    "    # PR AUC\n",
    "    pr_curve_precision, pr_curve_recall, _ = precision_recall_curve(y_true, y_proba)\n",
    "    metrics['pr_auc'] = auc(pr_curve_recall, pr_curve_precision) # Note order: recall is x, precision is y\n",
    "\n",
    "    # Brier Score\n",
    "    metrics['brier_score'] = brier_score_loss(y_true, y_proba)\n",
    "\n",
    "    logging.info(f\"--- {model_name} Metrics ---\")\n",
    "    logging.info(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    logging.info(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    logging.info(f\"Recall (TPR): {metrics['recall_tpr']:.4f}\")\n",
    "    logging.info(f\"Specificity (TNR): {metrics['specificity_tnr']:.4f}\")\n",
    "    logging.info(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    logging.info(f\"G-Mean: {metrics['g_mean']:.4f}\")\n",
    "    logging.info(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    logging.info(f\"PR AUC: {metrics['pr_auc']:.4f}\")\n",
    "    logging.info(f\"Brier Score: {metrics['brier_score']:.4f}\")\n",
    "    logging.info(f\"Confusion Matrix (TN, FP, FN, TP): ({tn}, {fp}, {fn}, {tp})\")\n",
    "\n",
    "    # Optional: Plot Confusion Matrix\n",
    "    if conf_mat:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap([[tn, fp], [fn, tp]], annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['Predicted Galaxy (0)', 'Predicted Star (1)'],\n",
    "                    yticklabels=['Actual Galaxy (0)', 'Actual Star (1)'])\n",
    "        plt.title(f'{model_name} Confusion Matrix')\n",
    "        plt.ylabel('Actual Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        cm_filename = os.path.join(MODEL_DIR, f\"{model_name}_confusion_matrix_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\")\n",
    "        plt.savefig(cm_filename)\n",
    "        plt.close()\n",
    "        logging.info(f\"Confusion matrix plot saved to {cm_filename}\")\n",
    "\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf85e1b",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68e877b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Scaling ---\n",
    "# Important for SVM, not used for the other models.\n",
    "# Fit scaler ONLY on training data, then transform all sets.\n",
    "\n",
    "# Check if training set and other datasets are non-empty before scaling\n",
    "def apply_feature_scaling(\n",
    "    X_train, X_val, X_test, X_cal,\n",
    "    TRAIN_SIZE, VAL_SIZE, TEST_SIZE, CAL_SIZE,\n",
    "    MODEL_DIR,\n",
    "    save_scaler=True,\n",
    "    group_name=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies StandardScaler to the provided datasets if training data is available.\n",
    "    Optionally saves the fitted scaler to disk.\n",
    "    If group_name is provided, attempts to load the scaler from disk.\n",
    "    Returns: X_train_scaled, X_val_scaled, X_test_scaled, X_cal_scaled, scaler (or None)\n",
    "    \"\"\"\n",
    "\n",
    "    scaler = None\n",
    "    scaler_loaded = False\n",
    "    scaler_filename = None\n",
    "\n",
    "    # If group_name is provided, try to load the scaler from disk\n",
    "    if group_name is not None:\n",
    "        scaler_filename = os.path.join(MODEL_DIR, group_name, f\"scaler_{group_name}.joblib\")\n",
    "        # Ensure the directory exists before checking the file\n",
    "        os.makedirs(os.path.dirname(scaler_filename), exist_ok=True)\n",
    "        if os.path.exists(scaler_filename):\n",
    "            try:\n",
    "                scaler = joblib.load(scaler_filename)\n",
    "                scaler_loaded = True\n",
    "                logging.info(f\"Scaler loaded from {scaler_filename}\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Could not load scaler from {scaler_filename}: {e}. Fitting a new one.\")\n",
    "                scaler_loaded = False\n",
    "        else:\n",
    "            logging.info(f\"No existing scaler found for group '{group_name}', will fit a new one if possible.\")\n",
    "\n",
    "    # If scaler was not loaded, fit a new one if possible\n",
    "    if not scaler_loaded:\n",
    "        if len(X_train) > 0 and TRAIN_SIZE > 0:\n",
    "            logging.info(\"Applying StandardScaler to features...\")\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "        else:\n",
    "            logging.info(\"Empty training set, NOT able to apply StandardScaler!\")\n",
    "            # Keep as original (likely empty) DataFrame/array\n",
    "            X_train_scaled = X_train.copy()\n",
    "    else:\n",
    "        # If scaler was loaded, just transform\n",
    "        if len(X_train) > 0 and TRAIN_SIZE > 0:\n",
    "            X_train_scaled = scaler.transform(X_train)\n",
    "        else:\n",
    "            X_train_scaled = X_train.copy()\n",
    "\n",
    "    # Create empty copies to hold scaled data, preserving column names if pandas DataFrame\n",
    "    X_val_scaled = X_val.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    X_cal_scaled = X_cal.copy()\n",
    "\n",
    "    if scaler is not None:\n",
    "        if len(X_val) > 0 and VAL_SIZE > 0:\n",
    "            X_val_scaled = scaler.transform(X_val)\n",
    "        if len(X_test) > 0 and TEST_SIZE > 0:\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "        if len(X_cal) > 0 and CAL_SIZE > 0:\n",
    "            X_cal_scaled = scaler.transform(X_cal)\n",
    "\n",
    "    # Save the scaler if it was fitted and requested, and not already loaded\n",
    "    if scaler is not None and save_scaler and not scaler_loaded and scaler_filename is not None:\n",
    "        joblib.dump(scaler, scaler_filename)\n",
    "        logging.info(f\"Scaler saved to {scaler_filename}\")\n",
    "\n",
    "    logging.info(\"Feature scaling complete.\")\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, X_cal_scaled, scaler\n",
    "\n",
    "# Example usage:\n",
    "# X_train_scaled, X_val_scaled, X_test_scaled, X_cal_scaled, scaler = apply_feature_scaling(\n",
    "#     X_train, X_val, X_test, X_cal, TRAIN_SIZE, VAL_SIZE, TEST_SIZE, CAL_SIZE, MODEL_DIR,\n",
    "#     save_scaler=True, # Ensure scaler is saved\n",
    "#     group_name=group_name # Pass group name for potentially unique scaler filename\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfa8f49",
   "metadata": {},
   "source": [
    "## 4. Model Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3665cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {} # Dictionary to store metrics for each model\n",
    "\n",
    "CALIBRATOR = 'cvap' #'cvap' 'platt' 'isotonic'\n",
    "SCORING_FUNCTION = average_precision_score #f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca2d2ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for SVM (using data fraction)\n",
    "MAX_RESOURCE_SVM = 1.0  # Max data fraction\n",
    "MIN_RESOURCE_SVM = 0.1  # Min data fraction (adjust based on minority class size)\n",
    "ETA_SVM = 3\n",
    "RESOURCE_TYPE_SVM = 'data_fraction'\n",
    "model_name_svm = \"SVM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b95e2f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for CART (using data fraction)\n",
    "MAX_RESOURCE_CART = 1.0\n",
    "MIN_RESOURCE_CART = 0.1 # Can start with smaller fraction for trees\n",
    "ETA_CART = 3\n",
    "RESOURCE_TYPE_CART = 'data_fraction'\n",
    "model_name_cart = \"CART\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6971c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for RF (using iterations)\n",
    "MAX_RESOURCE_RF = 300  # Max n_estimators\n",
    "MIN_RESOURCE_RF = 20   # Min n_estimators\n",
    "ETA_RF = 3\n",
    "RESOURCE_TYPE_RF = 'iterations'\n",
    "model_name_rf = \"Random_Forest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f957357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for XGB (using iterations)\n",
    "MAX_RESOURCE_XGB = 500 # Max n_estimators\n",
    "MIN_RESOURCE_XGB = 30  # Min n_estimators\n",
    "ETA_XGB = 3\n",
    "RESOURCE_TYPE_XGB = 'iterations'\n",
    "model_name_xgb = \"XGBoost\"\n",
    "ROUNDS = 100        # Number of rounds to wait for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93f0744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for LGBM (using iterations)\n",
    "MAX_RESOURCE_LGBM = 500 # Max n_estimators\n",
    "MIN_RESOURCE_LGBM = 30  # Min n_estimators\n",
    "ETA_LGBM = 3\n",
    "RESOURCE_TYPE_LGBM = 'iterations'\n",
    "model_name_lgbm = \"LightGBM\"\n",
    "ROUNDS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7859c",
   "metadata": {},
   "source": [
    "### 3.1 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18317c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[24] - REPLACEMENT\n",
    "def svm_workflow(\n",
    "    X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test,\n",
    "    MAX_RESOURCE_SVM, MIN_RESOURCE_SVM, ETA_SVM, RESOURCE_TYPE_SVM,\n",
    "    model_name_svm, CALIBRATOR, RANDOM_SEED,\n",
    "    SVC=SVC, loguniform=loguniform, f1_score=f1_score, logging=logging,\n",
    "    best_params_svm=None,\n",
    "    group=\"\", model_dir=\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete SVM workflow: HPO (comparison), calibration and evaluation.\n",
    "    \"\"\"\n",
    "    logging.info(f\"\\n\\n===== Starting Workflow for {model_name_svm} ({group}) =====\")\n",
    "    timestamp_svm = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    all_results = {}\n",
    "    best_hpo_method_svm = \"pre-tuned\" if best_params_svm else None\n",
    "    best_score_hpo_svm = None\n",
    "\n",
    "    # --- HPO Step ---\n",
    "    if best_params_svm is None:\n",
    "        # Define HPO search spaces\n",
    "        param_space_svm_continuous = {\n",
    "            \"C\": loguniform(1e-3, 1e3),\n",
    "            \"gamma\": loguniform(1e-6, 1e1),\n",
    "            \"kernel\": [\"rbf\"],\n",
    "        }\n",
    "        # A smaller, discrete grid for a fair comparison\n",
    "        param_space_svm_grid = {\n",
    "            \"C\": np.logspace(-2, 2, 5), # [0.01, 0.1, 1, 10, 100]\n",
    "            \"gamma\": np.logspace(-5, 0, 6), # [1e-5, 1e-4, ..., 1]\n",
    "            \"kernel\": [\"rbf\"],\n",
    "        }\n",
    "        \n",
    "        hpo_settings = {\n",
    "            'hyperband': {'max_resource': MAX_RESOURCE_SVM, 'min_resource': MIN_RESOURCE_SVM, 'eta': ETA_SVM, 'resource_type': RESOURCE_TYPE_SVM},\n",
    "            'random': {'n_iter': 60},\n",
    "            'grid': {},\n",
    "        }\n",
    "\n",
    "        hpo_results = run_hpo_comparison(\n",
    "            model_class=SVC, model_name=model_name_svm,\n",
    "            param_space_continuous=param_space_svm_continuous,\n",
    "            param_space_grid=param_space_svm_grid,\n",
    "            X_train=X_train_scaled, y_train=y_train, X_val=X_val_scaled, y_val=y_val,\n",
    "            hpo_settings=hpo_settings, scoring_func=SCORING_FUNCTION, random_state=RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        best_params_svm = hpo_results['best_params_overall']\n",
    "        best_score_hpo_svm = hpo_results['best_score_overall']\n",
    "        best_hpo_method_svm = hpo_results['best_method']\n",
    "        hpo_duration_svm = hpo_results['all_results'][hpo_results['best_method']]['total_time']\n",
    "        logging.info(f\"--- [{model_name_svm}] HPO Comparison finished in {hpo_duration_svm:.2f} seconds ---\")\n",
    "    else:\n",
    "        hpo_duration_svm = 0.0\n",
    "        logging.info(f\"--- [{model_name_svm}] Using provided hyperparameters, skipping HPO. ---\")\n",
    "\n",
    "    # --- Training, Calibration, Evaluation (Unchanged) ---\n",
    "    final_svm_predictor = None\n",
    "    calibration_duration_svm = 0.0\n",
    "    if best_params_svm:\n",
    "        calibration_start_time_svm = time.time()\n",
    "        best_params_svm['random_state'] = RANDOM_SEED\n",
    "        if 'class_weight' not in best_params_svm: best_params_svm['class_weight'] = 'balanced'\n",
    "        if 'probability' in best_params_svm: del best_params_svm['probability']\n",
    "\n",
    "        final_svm_predictor = train_calibrate_model(\n",
    "            base_estimator_class=SVC, best_params=best_params_svm,\n",
    "            X_train=X_train_scaled, y_train=y_train,\n",
    "            model_name=model_name_svm, group_name=group, MODEL_DIR=model_dir,\n",
    "            calibration_method=CALIBRATOR, n_splits=5, random_state=RANDOM_SEED,\n",
    "            score_method='decision_function'\n",
    "        )\n",
    "        calibration_duration_svm = time.time() - calibration_start_time_svm\n",
    "        if not final_svm_predictor:\n",
    "            logging.error(f\"[{model_name_svm}] Failed to train or calibrate the model.\")\n",
    "            return None\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_svm}] HPO did not find best parameters. Aborting workflow.\")\n",
    "        return None\n",
    "\n",
    "    metrics_svm = None\n",
    "    eval_duration_svm = 0.0\n",
    "    if final_svm_predictor:\n",
    "        eval_start_time_svm = time.time()\n",
    "        y_proba_test_svm = final_svm_predictor.predict_proba(X_test_scaled)[:, 1]\n",
    "        y_pred_test_svm = (y_proba_test_svm >= 0.5).astype(int)\n",
    "        metrics_svm = calculate_metrics(y_test, y_pred_test_svm, y_proba_test_svm, model_name=model_name_svm)\n",
    "        eval_duration_svm = time.time() - eval_start_time_svm\n",
    "\n",
    "    # --- Store results ---\n",
    "    all_results[model_name_svm] = {\n",
    "        'metrics': metrics_svm,\n",
    "        'best_hpo_params': best_params_svm,\n",
    "        'best_hpo_method': best_hpo_method_svm, # Added\n",
    "        'hpo_f1_score': best_score_hpo_svm,\n",
    "        'hpo_duration_s': hpo_duration_svm,\n",
    "        'calibration_duration_s': calibration_duration_svm,\n",
    "        'evaluation_duration_s': eval_duration_svm,\n",
    "        'timestamp': timestamp_svm,\n",
    "        'group': group,\n",
    "        'calibration_method': CALIBRATOR\n",
    "    }\n",
    "    logging.info(f\"===== Finished Workflow for {model_name_svm} ({group}) =====\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0a169",
   "metadata": {},
   "source": [
    "### 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_workflow(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "    model_name_rf, MAX_RESOURCE_RF, MIN_RESOURCE_RF, ETA_RF, RESOURCE_TYPE_RF,\n",
    "    CALIBRATOR, RANDOM_SEED,\n",
    "    RandomForestClassifier, randint, loguniform, f1_score, hyperband_hpo,\n",
    "    train_calibrate_model, calculate_metrics, logging, np, datetime,\n",
    "    best_params_rf=None,\n",
    "    group=\"\", model_dir=\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete Random Forest workflow: HPO (comparison), calibration and evaluation.\n",
    "    \"\"\"\n",
    "    logging.info(f\"\\n\\n===== Starting Workflow for {model_name_rf} ({group}) =====\")\n",
    "    timestamp_rf = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    all_results = {}\n",
    "    best_hpo_method_rf = \"pre-tuned\" if best_params_rf else None\n",
    "    best_score_hpo_rf = None\n",
    "\n",
    "    # --- HPO Step ---\n",
    "    if best_params_rf is None:\n",
    "        param_space_rf_continuous = {\n",
    "            \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "            \"bootstrap\":  [True, False], \n",
    "            \"max_depth\": randint(3, 40),\n",
    "            \"min_samples_split\": randint(2, 50),\n",
    "            \"min_samples_leaf\": randint(1, 20),\n",
    "            \"max_features\": [\"sqrt\", \"log2\", 0.5],\n",
    "            \"ccp_alpha\": loguniform(1e-7, 1e-2),\n",
    "        }\n",
    "        param_space_rf_grid = {\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"max_depth\": [10, 25],\n",
    "            \"min_samples_leaf\": [1, 5, 10],\n",
    "            \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        }\n",
    "        # Note: n_estimators will be fixed to max_resource for random/grid\n",
    "        # and varied for hyperband\n",
    "        param_space_rf_continuous['n_estimators'] = randint(MIN_RESOURCE_RF, MAX_RESOURCE_RF + 1)\n",
    "        param_space_rf_grid['n_estimators'] = [int(MAX_RESOURCE_RF * 0.5), MAX_RESOURCE_RF]\n",
    "\n",
    "        hpo_settings = {\n",
    "            'hyperband': {'max_resource': MAX_RESOURCE_RF, 'min_resource': MIN_RESOURCE_RF, 'eta': ETA_RF, 'resource_type': RESOURCE_TYPE_RF},\n",
    "            'random': {'n_iter': 60},\n",
    "            'grid': {},\n",
    "        }\n",
    "        \n",
    "        hpo_results = run_hpo_comparison(\n",
    "            model_class=RandomForestClassifier, model_name=model_name_rf,\n",
    "            param_space_continuous=param_space_rf_continuous,\n",
    "            param_space_grid=param_space_rf_grid,\n",
    "            X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val,\n",
    "            hpo_settings=hpo_settings, scoring_func=SCORING_FUNCTION, random_state=RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        best_params_rf = hpo_results['best_params_overall']\n",
    "        best_score_hpo_rf = hpo_results['best_score_overall']\n",
    "        best_hpo_method_rf = hpo_results['best_method']\n",
    "        hpo_duration_rf = hpo_results['all_results'][hpo_results['best_method']]['total_time']\n",
    "        logging.info(f\"--- [{model_name_rf}] HPO Comparison finished in {hpo_duration_rf:.2f} seconds ---\")\n",
    "    else:\n",
    "        hpo_duration_rf = 0.0\n",
    "        logging.info(f\"--- [{model_name_rf}] Using provided hyperparameters, skipping HPO. ---\")\n",
    "    \n",
    "    # --- Training, Calibration, Evaluation (Unchanged) ---\n",
    "    final_rf_predictor = None\n",
    "    calibration_duration_rf = 0.0\n",
    "    if best_params_rf:\n",
    "        calibration_start_time_rf = time.time()\n",
    "        best_params_rf['random_state'] = RANDOM_SEED\n",
    "        if 'class_weight' not in best_params_rf: best_params_rf['class_weight'] = 'balanced'\n",
    "        best_params_rf['n_jobs'] = -1\n",
    "\n",
    "        final_rf_predictor = train_calibrate_model(\n",
    "            base_estimator_class=RandomForestClassifier, best_params=best_params_rf,\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            model_name=model_name_rf, group_name=group, MODEL_DIR=model_dir,\n",
    "            calibration_method=CALIBRATOR, n_splits=5, random_state=RANDOM_SEED,\n",
    "            score_method='predict_proba'\n",
    "        )\n",
    "        calibration_duration_rf = time.time() - calibration_start_time_rf\n",
    "        if not final_rf_predictor:\n",
    "            logging.error(f\"[{model_name_rf}] Failed to train or calibrate model.\")\n",
    "            return None\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_rf}] HPO did not find best parameters. Aborting workflow.\")\n",
    "        return None\n",
    "\n",
    "    metrics_rf = None\n",
    "    eval_duration_rf = 0.0\n",
    "    if final_rf_predictor:\n",
    "        eval_start_time_rf = time.time()\n",
    "        y_proba_test_rf = final_rf_predictor.predict_proba(X_test)[:, 1]\n",
    "        y_pred_test_rf = (y_proba_test_rf >= 0.5).astype(int)\n",
    "        metrics_rf = calculate_metrics(y_test, y_pred_test_rf, y_proba_test_rf, model_name=model_name_rf)\n",
    "        eval_duration_rf = time.time() - eval_start_time_rf\n",
    "\n",
    "    # --- Store results ---\n",
    "    all_results[model_name_rf] = {\n",
    "        'metrics': metrics_rf,\n",
    "        'best_hpo_params': best_params_rf,\n",
    "        'best_hpo_method': best_hpo_method_rf, # Added\n",
    "        'hpo_f1_score': best_score_hpo_rf,\n",
    "        'hpo_duration_s': hpo_duration_rf,\n",
    "        'calibration_duration_s': calibration_duration_rf,\n",
    "        'evaluation_duration_s': eval_duration_rf,\n",
    "        'timestamp': timestamp_rf,\n",
    "        'group': group,\n",
    "        'calibration_method': CALIBRATOR\n",
    "    }\n",
    "    logging.info(f\"===== Finished Workflow for {model_name_rf} ({group}) =====\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c03f0",
   "metadata": {},
   "source": [
    "### 3.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_workflow(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "    all_results, model_name_xgb,\n",
    "    MAX_RESOURCE_XGB, MIN_RESOURCE_XGB, ETA_XGB, RESOURCE_TYPE_XGB, ROUNDS,\n",
    "    CALIBRATOR, RANDOM_SEED,\n",
    "    loguniform, randint, uniform, xgb, hyperband_hpo, f1_score, EarlyStopping,\n",
    "    train_calibrate_model, calculate_metrics, logging, np,\n",
    "    best_params_xgb=None,\n",
    "    group=\"\", model_dir=\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete XGBoost workflow: HPO (comparison), calibration and evaluation.\n",
    "    \"\"\"\n",
    "    logging.info(f\"\\n\\n===== Starting Workflow for {model_name_xgb} ({group}) =====\")\n",
    "    timestamp_xgb = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    best_hpo_method_xgb = \"pre-tuned\" if best_params_xgb else None\n",
    "    best_score_hpo_xgb = None\n",
    "\n",
    "    # --- HPO Step ---\n",
    "    if best_params_xgb is None:\n",
    "        param_space_xgb_continuous = {\n",
    "            \"learning_rate\": loguniform(1e-3, 0.2),\n",
    "            \"max_depth\": randint(3, 11),\n",
    "            \"min_child_weight\": loguniform(5e-1, 10),\n",
    "            \"subsample\": uniform(0.5, 0.5),\n",
    "            \"colsample_bytree\": uniform(0.5, 0.5),\n",
    "            \"gamma\": loguniform(1e-4, 5),\n",
    "            \"reg_alpha\": loguniform(1e-4, 10),\n",
    "            \"reg_lambda\": loguniform(1e-3, 10),\n",
    "            \n",
    "            # fixed for a binary-classification use-case\n",
    "            \"objective\": [\"binary:logistic\"],\n",
    "            \"eval_metric\": [\"logloss\"],\n",
    "        }\n",
    "        param_space_xgb_grid = {\n",
    "            \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "            \"max_depth\": [4, 6, 8],\n",
    "            \"subsample\": [0.7, 1.0],\n",
    "            \"colsample_bytree\": [0.7, 1.0],\n",
    "            \"objective\": [\"binary:logistic\"],\n",
    "            \"eval_metric\": [\"logloss\"],\n",
    "        }\n",
    "        param_space_xgb_continuous['n_estimators'] = randint(MIN_RESOURCE_XGB, MAX_RESOURCE_XGB + 1)\n",
    "        param_space_xgb_grid['n_estimators'] = [int(MAX_RESOURCE_XGB * 0.5), MAX_RESOURCE_XGB]\n",
    "\n",
    "        hpo_settings = {\n",
    "            'hyperband': {'max_resource': MAX_RESOURCE_XGB, 'min_resource': MIN_RESOURCE_XGB, 'eta': ETA_XGB, 'resource_type': RESOURCE_TYPE_XGB},\n",
    "            'random': {'n_iter': 60},\n",
    "            'grid': {},\n",
    "        }\n",
    "        \n",
    "        hpo_results = run_hpo_comparison(\n",
    "            model_class=xgb.XGBClassifier, model_name=model_name_xgb,\n",
    "            param_space_continuous=param_space_xgb_continuous,\n",
    "            param_space_grid=param_space_xgb_grid,\n",
    "            X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val,\n",
    "            hpo_settings=hpo_settings, scoring_func=SCORING_FUNCTION, random_state=RANDOM_SEED\n",
    "        )\n",
    "\n",
    "        best_params_xgb = hpo_results['best_params_overall']\n",
    "        best_score_hpo_xgb = hpo_results['best_score_overall']\n",
    "        best_hpo_method_xgb = hpo_results['best_method']\n",
    "        hpo_duration_xgb = hpo_results['all_results'][hpo_results['best_method']]['total_time']\n",
    "        logging.info(f\"--- [{model_name_xgb}] HPO Comparison finished in {hpo_duration_xgb:.2f} seconds ---\")\n",
    "    else:\n",
    "        hpo_duration_xgb = 0.0\n",
    "        logging.info(f\"--- [{model_name_xgb}] Using provided hyperparameters, skipping HPO. ---\")\n",
    "    \n",
    "    # --- Training, Calibration, Evaluation (Unchanged) ---\n",
    "    final_xgb_predictor = None\n",
    "    calibration_duration_xgb = 0.0\n",
    "    final_best_params_xgb = None\n",
    "    if best_params_xgb:\n",
    "        calibration_start_time_xgb = time.time()\n",
    "        temp_best_params_xgb = best_params_xgb.copy()\n",
    "        temp_best_params_xgb['random_state'] = RANDOM_SEED\n",
    "        if 'scale_pos_weight' not in temp_best_params_xgb:\n",
    "            neg_count = (y_train == 0).sum(); pos_count = (y_train == 1).sum()\n",
    "            if pos_count > 0: temp_best_params_xgb['scale_pos_weight'] = neg_count / pos_count\n",
    "\n",
    "        callbacks = [EarlyStopping(rounds=ROUNDS, save_best=True, metric_name='logloss', maximize=False)]\n",
    "        temp_xgb_model = xgb.XGBClassifier(**temp_best_params_xgb, callbacks=callbacks)\n",
    "        temp_xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        \n",
    "        best_iteration = temp_xgb_model.best_iteration\n",
    "        if best_iteration is None or best_iteration <= 0: best_iteration = MAX_RESOURCE_XGB\n",
    "        \n",
    "        final_best_params_xgb = temp_best_params_xgb.copy()\n",
    "        final_best_params_xgb['n_estimators'] = best_iteration\n",
    "        \n",
    "        final_xgb_predictor = train_calibrate_model(\n",
    "            base_estimator_class=xgb.XGBClassifier, best_params=final_best_params_xgb,\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            model_name=model_name_xgb, group_name=group, MODEL_DIR=model_dir,\n",
    "            calibration_method=CALIBRATOR, n_splits=5, random_state=RANDOM_SEED,\n",
    "            score_method='raw_margin_xgb'\n",
    "        )\n",
    "        calibration_duration_xgb = time.time() - calibration_start_time_xgb\n",
    "        if not final_xgb_predictor:\n",
    "             logging.error(f\"[{model_name_xgb}] Failed to train/calibrate final model.\")\n",
    "             return {model_name_xgb: {\"status\": \"failed\"}}\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_xgb}] HPO did not find best parameters. Aborting workflow.\")\n",
    "        return {model_name_xgb: {\"status\": \"failed\"}}\n",
    "\n",
    "    metrics_xgb = None\n",
    "    eval_duration_xgb = 0.0\n",
    "    if final_xgb_predictor:\n",
    "        eval_start_time_xgb = time.time()\n",
    "        y_proba_test_xgb = final_xgb_predictor.predict_proba(X_test)[:, 1]\n",
    "        y_pred_test_xgb = (y_proba_test_xgb >= 0.5).astype(int)\n",
    "        metrics_xgb = calculate_metrics(y_test, y_pred_test_xgb, y_proba_test_xgb, model_name=model_name_xgb)\n",
    "        eval_duration_xgb = time.time() - eval_start_time_xgb\n",
    "\n",
    "    # --- Store results ---\n",
    "    # The original function returned 'all_results', but it's better to return a dict for the current model\n",
    "    results_for_this_model = {model_name_xgb: {\n",
    "        'metrics': metrics_xgb,\n",
    "        'best_hpo_params': best_params_xgb,\n",
    "        'best_hpo_method': best_hpo_method_xgb, # Added\n",
    "        'final_n_estimators': final_best_params_xgb.get('n_estimators'),\n",
    "        'hpo_f1_score': best_score_hpo_xgb,\n",
    "        'hpo_duration_s': hpo_duration_xgb,\n",
    "        'calibration_duration_s': calibration_duration_xgb,\n",
    "        'evaluation_duration_s': eval_duration_xgb,\n",
    "        'timestamp': timestamp_xgb,\n",
    "        'group': group,\n",
    "        'calibration_method': CALIBRATOR\n",
    "    }}\n",
    "    logging.info(f\"===== Finished Workflow for {model_name_xgb} ({group}) =====\")\n",
    "    return results_for_this_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2740d857",
   "metadata": {},
   "source": [
    "### 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d90d844f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Feature Groups:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting feature set group 'group_1' with 8 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javym\\miniconda3\\envs\\phys-tfg\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "Processing Feature Groups:  33%|███▎      | 1/3 [06:17<12:34, 377.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting feature set group 'group_2' with 49 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javym\\miniconda3\\envs\\phys-tfg\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "Processing Feature Groups:  67%|██████▋   | 2/3 [23:51<12:55, 775.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting feature set group 'group_3' with 56 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javym\\miniconda3\\envs\\phys-tfg\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "Processing Feature Groups: 100%|██████████| 3/3 [45:51<00:00, 917.20s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Workflow loop finished. Results saved to 'results\\all_group_results_20250610_104134.json'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# Workflow Loop - Run Models with Best Parameters and Evaluate #\n",
    "################################################################\n",
    "\n",
    "# --- Configuration ---\n",
    "BEST_PARAMS_DIR = \"best_params\"\n",
    "RESULTS_DIR = \"results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "RESULTS_FILENAME = os.path.join(RESULTS_DIR, f\"all_group_results_{datetime.now():%Y%m%d_%H%M%S}.json\")\n",
    "\n",
    "GROUP_NAMES_TO_PROCESS = list(groups.keys())\n",
    "#GROUP_NAMES_TO_PROCESS = ['group_1'] # Example for faster testing\n",
    "\n",
    "all_group_results = {}\n",
    "\n",
    "logging.info(f\"STARTING MAIN WORKFLOW LOOP for groups: {GROUP_NAMES_TO_PROCESS}\")\n",
    "outer_loop_start_time = time.time()\n",
    "\n",
    "for group_name in tqdm(GROUP_NAMES_TO_PROCESS, desc=\"Processing Feature Groups\"):\n",
    "    logging.info(f\"\\n{'='*30} Processing Feature Group: {group_name} {'='*30}\")\n",
    "    group_start_time = time.time()\n",
    "    group_results = {}\n",
    "\n",
    "    # --- Load Best Params for the Group ---\n",
    "    params_filepath = os.path.join(BEST_PARAMS_DIR, f\"{group_name}_best_params.json\")\n",
    "    group_best_params = {}\n",
    "    params_were_loaded = False # Flag to track if we need to save params later\n",
    "    if os.path.exists(params_filepath):\n",
    "        try:\n",
    "            with open(params_filepath, 'r') as f:\n",
    "                group_best_params = json.load(f)\n",
    "            params_were_loaded = True # parameters loaded\n",
    "            logging.info(f\"[{group_name}] Successfully loaded best parameters from {params_filepath}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] Failed to load best parameters from {params_filepath}: {e}. HPO will run.\")\n",
    "            params_were_loaded = False\n",
    "    else:\n",
    "        logging.warning(f\"[{group_name}] Best parameters file not found: {params_filepath}. HPO will run.\")\n",
    "        params_were_loaded = False # File doesn't exist, so we'll run HPO.\n",
    "\n",
    "    best_params_svm = group_best_params.get(model_name_svm)\n",
    "    # best_params_cart = group_best_params.get(model_name_cart)\n",
    "    best_params_rf = group_best_params.get(model_name_rf)\n",
    "    best_params_xgb = group_best_params.get(model_name_xgb)\n",
    "    # best_params_lgbm = group_best_params.get(model_name_lgbm)\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "    X, y, df_clean = clean_data(df, group_name, TARGET_COLUMN, logger=logging)\n",
    "    if X.empty or y.empty:\n",
    "        logging.warning(f\"[{group_name}] Skipping group due to insufficient data.\")\n",
    "        all_group_results[group_name] = {'status': 'skipped_insufficient_data', 'results': {}}\n",
    "        continue\n",
    "\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, X_cal, y_cal = split_data(X, y)\n",
    "    if X_train.empty or y_train.empty or X_test.empty or y_test.empty:\n",
    "         logging.warning(f\"[{group_name}] Skipping group due to empty train or test set.\")\n",
    "         all_group_results[group_name] = {'status': 'skipped_empty_splits', 'results': {}}\n",
    "         continue\n",
    "\n",
    "    X_train_scaled, X_val_scaled, X_test_scaled, X_cal_scaled, _ = apply_feature_scaling(\n",
    "        X_train, X_val, X_test, X_cal, 1, 1, 1, 1, MODEL_DIR, save_scaler=True, group_name=group_name\n",
    "    )\n",
    "\n",
    "    # --- Run Workflows ---\n",
    "    \n",
    "    # 1. SVM Workflow\n",
    "    try:\n",
    "        svm_results = svm_workflow(X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test, MAX_RESOURCE_SVM, MIN_RESOURCE_SVM, ETA_SVM, RESOURCE_TYPE_SVM, model_name_svm, CALIBRATOR, RANDOM_SEED, SVC, loguniform, f1_score, logging, best_params_svm=best_params_svm, group=group_name, model_dir=MODEL_DIR)\n",
    "        group_results.update(svm_results if svm_results else {model_name_svm: {\"status\": \"failed\"}})\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[{group_name}] SVM Workflow failed: {e}\", exc_info=True)\n",
    "        group_results[model_name_svm] = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "    # 2. Random Forest Workflow\n",
    "    try:\n",
    "        rf_results = random_forest_workflow(X_train, y_train, X_val, y_val, X_test, y_test, model_name_rf, MAX_RESOURCE_RF, MIN_RESOURCE_RF, ETA_RF, RESOURCE_TYPE_RF, CALIBRATOR, RANDOM_SEED, RandomForestClassifier, randint, loguniform, f1_score, hyperband_hpo, train_calibrate_model, calculate_metrics, logging, np, datetime, best_params_rf=best_params_rf, group=group_name, model_dir=MODEL_DIR)\n",
    "        group_results.update(rf_results if rf_results else {model_name_rf: {\"status\": \"failed\"}})\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[{group_name}] Random Forest Workflow failed: {e}\", exc_info=True)\n",
    "        group_results[model_name_rf] = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "    # 3. XGBoost Workflow\n",
    "    try:\n",
    "        xgb_results = xgb_workflow(X_train, y_train, X_val, y_val, X_test, y_test, {}, model_name_xgb, MAX_RESOURCE_XGB, MIN_RESOURCE_XGB, ETA_XGB, RESOURCE_TYPE_XGB, ROUNDS, CALIBRATOR, RANDOM_SEED, loguniform, randint, uniform, xgb, hyperband_hpo, f1_score, EarlyStopping, train_calibrate_model, calculate_metrics, logging, np, best_params_xgb=best_params_xgb, group=group_name, model_dir=MODEL_DIR)\n",
    "        group_results.update(xgb_results if xgb_results else {model_name_xgb: {\"status\": \"failed\"}})\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[{group_name}] XGBoost Workflow failed: {e}\", exc_info=True)\n",
    "        group_results[model_name_xgb] = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "    \n",
    "    # Save the newly found parameters if they were not loaded from a file ---\n",
    "    if not params_were_loaded and group_results:\n",
    "        logging.info(f\"[{group_name}] HPO was run. Saving newly found best parameters to {params_filepath}\")\n",
    "        best_params_to_save = {}\n",
    "        for model_name, result_data in group_results.items():\n",
    "            if result_data and 'best_hpo_params' in result_data:\n",
    "                best_params_to_save[model_name] = result_data['best_hpo_params']\n",
    "\n",
    "        if best_params_to_save:\n",
    "            try:\n",
    "                # This serialization logic handles numpy types correctly\n",
    "                serializable_params = {}\n",
    "                for model, params in best_params_to_save.items():\n",
    "                    if params is not None:\n",
    "                        serializable_params[model] = {k: (int(v) if isinstance(v, np.integer) else\n",
    "                                                          float(v) if isinstance(v, np.floating) else\n",
    "                                                          v)\n",
    "                                                      for k, v in params.items()}\n",
    "                    else:\n",
    "                        serializable_params[model] = None\n",
    "                \n",
    "                with open(params_filepath, 'w') as f:\n",
    "                    json.dump(serializable_params, f, indent=4)\n",
    "                logging.info(f\"[{group_name}] Successfully saved best parameters.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"[{group_name}] Failed to save best parameters to {params_filepath}: {e}\", exc_info=True)\n",
    "        else:\n",
    "            logging.warning(f\"[{group_name}] HPO was run, but no best parameters were found to save.\")\n",
    "\n",
    "\n",
    "    # --- Store and Save Results for the Group ---\n",
    "    all_group_results[group_name] = {'status': 'completed', 'results': group_results}\n",
    "    group_duration = time.time() - group_start_time\n",
    "    logging.info(f\"--- Finished processing Group: {group_name} in {group_duration:.2f} seconds ---\")\n",
    "\n",
    "outer_loop_duration = time.time() - outer_loop_start_time\n",
    "logging.info(f\"\\n{'='*30} FINISHED PROCESSING ALL GROUPS in {outer_loop_duration:.2f} seconds {'='*30}\")\n",
    "\n",
    "# --- Serialize Final Results to JSON ---\n",
    "# (The serialization logic from your original cell [30] is correct and remains unchanged)\n",
    "try:\n",
    "    def default_serializer(o):\n",
    "        if isinstance(o, (np.integer, np.int64)): return int(o)\n",
    "        if isinstance(o, (np.floating, np.float64)): return float(o)\n",
    "        if isinstance(o, np.ndarray): return o.tolist()\n",
    "        if isinstance(o, (datetime, pd.Timestamp)): return o.isoformat()\n",
    "        if isinstance(o, Exception): return f\"Error: {str(o)}\"\n",
    "        try: return str(o)\n",
    "        except: return f\"Unserializable type: {type(o)}\"\n",
    "\n",
    "    with open(RESULTS_FILENAME, 'w') as f:\n",
    "        json.dump(all_group_results, f, indent=4, default=default_serializer)\n",
    "\n",
    "    logging.info(f\"Successfully saved all group results to {RESULTS_FILENAME}\")\n",
    "    print(f\"\\nWorkflow loop finished. Results saved to '{RESULTS_FILENAME}'.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to serialize final results to {RESULTS_FILENAME}: {e}\", exc_info=True)\n",
    "    print(\"\\nWorkflow loop finished, but failed to save results to JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7047b6c",
   "metadata": {},
   "source": [
    "### 6. Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "819b3809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Performance Metrics Summary (All Groups) =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall_tpr</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>specificity_tnr</th>\n",
       "      <th>g_mean</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>brier_score</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>HPO F1</th>\n",
       "      <th>Best HPO Method</th>\n",
       "      <th>Final Estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>group_1</td>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>0.9367</td>\n",
       "      <td>0.9750</td>\n",
       "      <td>0.1013</td>\n",
       "      <td>0.1835</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>0.8130</td>\n",
       "      <td>0.3005</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>5092</td>\n",
       "      <td>1</td>\n",
       "      <td>346</td>\n",
       "      <td>39</td>\n",
       "      <td>0.4561</td>\n",
       "      <td>Hyperband</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>group_1</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.9547</td>\n",
       "      <td>0.7175</td>\n",
       "      <td>0.5870</td>\n",
       "      <td>0.6457</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.7594</td>\n",
       "      <td>0.8866</td>\n",
       "      <td>0.6006</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>5004</td>\n",
       "      <td>89</td>\n",
       "      <td>159</td>\n",
       "      <td>226</td>\n",
       "      <td>0.3298</td>\n",
       "      <td>Hyperband</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>group_1</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.9626</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.5143</td>\n",
       "      <td>0.6589</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.7159</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>0.6840</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>5075</td>\n",
       "      <td>18</td>\n",
       "      <td>187</td>\n",
       "      <td>198</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>Hyperband</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>group_2</td>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>0.9304</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0206</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.8397</td>\n",
       "      <td>0.2476</td>\n",
       "      <td>0.0633</td>\n",
       "      <td>5093</td>\n",
       "      <td>0</td>\n",
       "      <td>381</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5753</td>\n",
       "      <td>Hyperband</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>group_2</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.3214</td>\n",
       "      <td>0.0234</td>\n",
       "      <td>0.0436</td>\n",
       "      <td>0.9963</td>\n",
       "      <td>0.1526</td>\n",
       "      <td>0.8252</td>\n",
       "      <td>0.2849</td>\n",
       "      <td>0.0593</td>\n",
       "      <td>5074</td>\n",
       "      <td>19</td>\n",
       "      <td>376</td>\n",
       "      <td>9</td>\n",
       "      <td>0.1934</td>\n",
       "      <td>Hyperband</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>group_2</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>0.8980</td>\n",
       "      <td>0.5948</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>0.7693</td>\n",
       "      <td>0.9269</td>\n",
       "      <td>0.7708</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>5067</td>\n",
       "      <td>26</td>\n",
       "      <td>156</td>\n",
       "      <td>229</td>\n",
       "      <td>0.6105</td>\n",
       "      <td>Hyperband</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>group_3</td>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>0.9465</td>\n",
       "      <td>0.9894</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.3883</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.4914</td>\n",
       "      <td>0.9425</td>\n",
       "      <td>0.7743</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>5092</td>\n",
       "      <td>1</td>\n",
       "      <td>292</td>\n",
       "      <td>93</td>\n",
       "      <td>0.6424</td>\n",
       "      <td>Hyperband</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>group_3</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.9556</td>\n",
       "      <td>0.7983</td>\n",
       "      <td>0.4935</td>\n",
       "      <td>0.6100</td>\n",
       "      <td>0.9906</td>\n",
       "      <td>0.6992</td>\n",
       "      <td>0.9028</td>\n",
       "      <td>0.6623</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>5045</td>\n",
       "      <td>48</td>\n",
       "      <td>195</td>\n",
       "      <td>190</td>\n",
       "      <td>0.3745</td>\n",
       "      <td>Hyperband</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>group_3</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9261</td>\n",
       "      <td>0.6831</td>\n",
       "      <td>0.7862</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>0.8248</td>\n",
       "      <td>0.9488</td>\n",
       "      <td>0.8249</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>5072</td>\n",
       "      <td>21</td>\n",
       "      <td>122</td>\n",
       "      <td>263</td>\n",
       "      <td>0.6772</td>\n",
       "      <td>Hyperband</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group          Model  accuracy  precision  recall_tpr  f1_score  \\\n",
       "0  group_1  Random_Forest    0.9367     0.9750      0.1013    0.1835   \n",
       "1  group_1            SVM    0.9547     0.7175      0.5870    0.6457   \n",
       "2  group_1        XGBoost    0.9626     0.9167      0.5143    0.6589   \n",
       "3  group_2  Random_Forest    0.9304     1.0000      0.0104    0.0206   \n",
       "4  group_2            SVM    0.9279     0.3214      0.0234    0.0436   \n",
       "5  group_2        XGBoost    0.9668     0.8980      0.5948    0.7156   \n",
       "6  group_3  Random_Forest    0.9465     0.9894      0.2416    0.3883   \n",
       "7  group_3            SVM    0.9556     0.7983      0.4935    0.6100   \n",
       "8  group_3        XGBoost    0.9739     0.9261      0.6831    0.7862   \n",
       "\n",
       "   specificity_tnr  g_mean  roc_auc  pr_auc  brier_score    TN  FP   FN   TP  \\\n",
       "0           0.9998  0.3182   0.8130  0.3005       0.0577  5092   1  346   39   \n",
       "1           0.9825  0.7594   0.8866  0.6006       0.0401  5004  89  159  226   \n",
       "2           0.9965  0.7159   0.8752  0.6840       0.0356  5075  18  187  198   \n",
       "3           1.0000  0.1019   0.8397  0.2476       0.0633  5093   0  381    4   \n",
       "4           0.9963  0.1526   0.8252  0.2849       0.0593  5074  19  376    9   \n",
       "5           0.9949  0.7693   0.9269  0.7708       0.0298  5067  26  156  229   \n",
       "6           0.9998  0.4914   0.9425  0.7743       0.0369  5092   1  292   93   \n",
       "7           0.9906  0.6992   0.9028  0.6623       0.0391  5045  48  195  190   \n",
       "8           0.9959  0.8248   0.9488  0.8249       0.0236  5072  21  122  263   \n",
       "\n",
       "   HPO F1 Best HPO Method Final Estimators  \n",
       "0  0.4561       Hyperband              N/A  \n",
       "1  0.3298       Hyperband              N/A  \n",
       "2  0.4877       Hyperband              455  \n",
       "3  0.5753       Hyperband              N/A  \n",
       "4  0.1934       Hyperband              N/A  \n",
       "5  0.6105       Hyperband              160  \n",
       "6  0.6424       Hyperband              N/A  \n",
       "7  0.3745       Hyperband              N/A  \n",
       "8  0.6772       Hyperband              447  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shortened floats in the JSON string above for brevity, use your full precision data\n",
    "# Load the JSON file from disk, not as a raw string\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "filename = (r\"results\\all_group_results_20250608_140210.json\" \n",
    "            if os.path.exists(r\"results\\all_group_results_20250608_140210.json\") \n",
    "            else max(glob.glob(r\"results\\all_group_results_*.json\"), key=os.path.getmtime))\n",
    "\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    all_group_results = json.load(f)\n",
    "\n",
    "# --- Modified Processing Logic for all_group_results ---\n",
    "results_summary_all_groups = []\n",
    "\n",
    "# Outer loop: Iterate through each group (e.g., group_1, group_2)\n",
    "for group_name, group_data in all_group_results.items():\n",
    "    status = group_data.get(\"status\")\n",
    "    all_results_for_group = group_data.get(\"results\") # This is the dict for models in this group\n",
    "\n",
    "    if status != \"completed\":\n",
    "        logging.warning(f\"Skipping group '{group_name}' due to status: {status}\")\n",
    "        continue # Skip to the next group if this one didn't complete\n",
    "\n",
    "    if not all_results_for_group:\n",
    "        logging.warning(f\"Skipping group '{group_name}': No 'results' data found even though status is completed.\")\n",
    "        continue # Skip if results dict is missing\n",
    "\n",
    "    # Inner loop: Iterate through each model's results within the current group\n",
    "    # This part is similar to your original code, but operates on all_results_for_group\n",
    "    for model_name, results_data in all_results_for_group.items():\n",
    "        # Start summary dict, adding the Group identifier\n",
    "        summary = {'Group': group_name, 'Model': model_name}\n",
    "\n",
    "        metrics = results_data.get('metrics')\n",
    "        if metrics:\n",
    "            # Add all metrics first\n",
    "            summary.update(metrics)\n",
    "            # Pop confusion matrix and extract TN, FP, FN, TP\n",
    "            cm = summary.pop('confusion_matrix', None)\n",
    "            if cm:\n",
    "                summary['TN'] = cm.get('tn')\n",
    "                summary['FP'] = cm.get('fp')\n",
    "                summary['FN'] = cm.get('fn')\n",
    "                summary['TP'] = cm.get('tp')\n",
    "            else:\n",
    "                 # Ensure these columns exist even if CM is missing\n",
    "                 summary['TN'] = None\n",
    "                 summary['FP'] = None\n",
    "                 summary['FN'] = None\n",
    "                 summary['TP'] = None\n",
    "\n",
    "        summary['HPO F1'] = results_data.get('hpo_f1_score')\n",
    "        summary['Best HPO Method'] = results_data.get('best_hpo_method', 'N/A')\n",
    "        summary['Final Estimators'] = results_data.get('final_n_estimators', 'N/A')\n",
    "\n",
    "        # --- Optional: Add best HPO parameters as a string column ---\n",
    "        # best_params = results_data.get('best_hpo_params')\n",
    "        # summary['Best HPO Params'] = str(best_params) if best_params else 'N/A'\n",
    "\n",
    "        results_summary_all_groups.append(summary)\n",
    "    # End inner loop (models)\n",
    "# End outer loop (groups)\n",
    "\n",
    "# --- Create the final DataFrame from the list of summaries ---\n",
    "results_df = pd.DataFrame(results_summary_all_groups)\n",
    "\n",
    "# Set display options for float formatting\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}' if isinstance(x, float) else x)\n",
    "\n",
    "print(\"\\n===== Performance Metrics Summary (All Groups) =====\")\n",
    "if not results_df.empty:\n",
    "    # Sort by Group and Model for better organization before displaying/saving\n",
    "    results_df = results_df.sort_values(by=['Group', 'Model']).reset_index(drop=True)\n",
    "\n",
    "    # Ensure all columns and all rows are displayed\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        display(results_df)\n",
    "else:\n",
    "    print(\"No results to display.\")\n",
    "\n",
    "# --- Update CSV saving ---\n",
    "# Use a filename indicating it contains results from all groups\n",
    "results_csv_path = os.path.join(\".\", f\"model_comparison_summary_all_groups_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "try:\n",
    "    # Save the sorted DataFrame\n",
    "    results_df.to_csv(results_csv_path, index=False, float_format='%.6f') # Control float precision in CSV\n",
    "    logging.info(f\"Results summary DataFrame saved to {results_csv_path}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to save results summary CSV: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys-tfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
