{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92c47e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter nbconvert --to script full_notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2cc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any existing log files\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "# Reset logger to avoid any issues with permissions\n",
    "logging.shutdown()\n",
    "# Remove loggers\n",
    "for log_file in glob.glob(\"*.log\"):\n",
    "    os.remove(log_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0f56a",
   "metadata": {},
   "source": [
    "# 0. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba63ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # For confusion matrix heatmap\n",
    "from scipy.stats import randint, uniform, loguniform # Ensure loguniform is imported if used\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "import logging\n",
    "from tqdm import tqdm  \n",
    "from IPython.display import display\n",
    "#from tqdm.notebook import tqdm # Needs pip install ipywidgets\n",
    "#from tqdm.auto import tqdm\n",
    "import joblib # For saving/loading models efficiently\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, ParameterSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from scipy.stats import loguniform # For hyperparameter distributions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone, BaseEstimator, ClassifierMixin\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support, roc_auc_score,\n",
    "    brier_score_loss, precision_recall_curve, auc, f1_score\n",
    ")   \n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "# Boosting models\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "\n",
    "# Configure logging\n",
    "logging.shutdown()\n",
    "logging.basicConfig(\n",
    "    filename=f'models_{datetime.now().strftime(\"%d_%H-%M-%S\")}.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True\n",
    ")\n",
    "# Prevent logs from being printed to console\n",
    "logging.getLogger().handlers = [h for h in logging.getLogger().handlers if isinstance(h, logging.FileHandler)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a8e78",
   "metadata": {},
   "source": [
    "## 1. Loading Dataset & Feature Selection\n",
    "\n",
    "- Group 1: Morphology (magnitudes with errors).\n",
    "- Group 2: Photometry (magnitudes with errors).\n",
    "- Group 3: Combined morphology and photometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2edcbbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the df\n",
    "df = pd.read_csv('data/match_alhambra_cosmos2020_ACS_class_0.8arcsec.csv')\n",
    "logging.info(f\"DataFrame created with shape: {df.shape}\")\n",
    "# Map ACS classification: 1 (Galaxy, Majority) -> 0, 2 (Star, minority) -> 1, 3 (Fake) -> drop\n",
    "logging.info(\"Original class counts:\")\n",
    "logging.info(df['acs_mu_class'].value_counts().to_string())\n",
    "\n",
    "# Drop fake detections (class 3)\n",
    "# Drop fake detections\n",
    "n_fakes = (df['acs_mu_class'] == 3).sum()\n",
    "logging.info(f\"Number of fake detections (class 3): {n_fakes}\")\n",
    "df = df[df['acs_mu_class'] != 3]\n",
    "\n",
    "# Map classifications\n",
    "df['acs_mu_class'] = df['acs_mu_class'].map({1: 0, 2: 1})\n",
    "\n",
    "logging.info(\"After dropping fakes and mapping classes (0: Galaxy, 1: Star):\")\n",
    "logging.info(df['acs_mu_class'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4a03fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features\n",
    "\n",
    "# --- Modeling features ---\n",
    "\n",
    "# 1. ALHAMBRA Morphology Features (SExtractor-based)\n",
    "morphology_features = [\n",
    "    'area', 'fwhm', 'stell', \n",
    "    'ell', #'a', 'b', #'theta', #a,b son fwhm y ell. Theta no da info.\n",
    "    'rk', \n",
    "    'rf'\n",
    "]\n",
    "\n",
    "morphology_err = [\n",
    "    's2n'\n",
    "]\n",
    "\n",
    "morphology_mags_errors = morphology_features + morphology_err\n",
    "\n",
    "# 2. ALHAMBRA Photometry (Optical + NIR + Synthetic)\n",
    "OPTICAL_MAG_COLS = [\n",
    "    'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W',\n",
    "    'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W',\n",
    "    'F799W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W'\n",
    "]\n",
    "photometry_magnitudes = (\n",
    "    OPTICAL_MAG_COLS +\n",
    "    ['J', 'H', 'KS', 'F814W']\n",
    ")\n",
    "\n",
    "OPTICAL_ERR_COLS = [\n",
    "    'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W',\n",
    "    'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W',\n",
    "    'dF799W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W'\n",
    "]\n",
    "photometry_uncertainties = (\n",
    "    OPTICAL_ERR_COLS +\n",
    "    ['dJ', 'dH', 'dKS', 'dF814W']\n",
    ")\n",
    "\n",
    "photometry_mags_errors = photometry_magnitudes + photometry_uncertainties\n",
    "\n",
    "# Target from acs\n",
    "target_variable = ['acs_mu_class'] # The COSMOS classification label to predict\n",
    "\n",
    "# --- Non-Modeling features ---\n",
    "\n",
    "# ALHAMBRA Photometric Redshift & Derived Features (BPZ-based)\n",
    "redshift_magnitudes = [\n",
    "    'zb_1', 'zb_Min_1', 'zb_Max_1', 'Tb_1',\n",
    "    'z_ml', 't_ml',\n",
    "    'Stell_Mass_1', 'M_Abs_1', 'MagPrior'\n",
    "]\n",
    "\n",
    "redshift_uncertainties = [\n",
    "    'Odds_1', 'Chi2'\n",
    "]\n",
    "\n",
    "redshift_mags_errors = redshift_magnitudes + redshift_uncertainties\n",
    "\n",
    "# ALHAMBRA Quality/Auxiliary Features (per-band quality etc.)\n",
    "OPTICAL_IRMS_COLS = [\n",
    "    'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W',\n",
    "    'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W',\n",
    "    'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W',\n",
    "    'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W'\n",
    "]\n",
    "quality_aux_features = (\n",
    "    ['nfobs'] +\n",
    "    OPTICAL_IRMS_COLS +\n",
    "    ['irms_J', 'irms_H', 'irms_KS', 'irms_F814W']\n",
    ")\n",
    "\n",
    "# Flags and identifiers\n",
    "\n",
    "non_modeling_identifiers = ['ID_1', 'id_2'] # ALHAMBRA ID, COSMOS ID\n",
    "\n",
    "non_modeling_astrometry = [\n",
    "    'RA_1', 'Dec_1', 'x', 'y', # ALHAMBRA Astrometry\n",
    "    'ra_2', 'dec_2',          # COSMOS Astrometry\n",
    "    'Separation'              # Matching Quality\n",
    "]\n",
    "\n",
    "non_modeling_flags = [\n",
    "    'photoflag', 'xray', 'PercW', 'Satur_Flag', # ALHAMBRA Object/Photometry Flags\n",
    "    'irms_OPT_Flag', 'irms_NIR_Flag'           # ALHAMBRA Overall Quality Flags\n",
    "]\n",
    "\n",
    "alhambra_prediction = ['Stellar_Flag'] # ALHAMBRA's own classification\n",
    "\n",
    "non_modeling_aperture_mags = [ # Specific aperture mags, usually use total mags\n",
    "    'F814W_3arcs', 'dF814W_3arcs', 'F814W_3arcs_corr'\n",
    "]\n",
    "\n",
    "non_modeling_cosmos_features = [ # Measurements/flags derived from COSMOS data (HST, HSC, VISTA...)\n",
    "    'model_flag',\n",
    "    'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista',\n",
    "    'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid',\n",
    "    'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid',\n",
    "    'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid',\n",
    "    'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid',\n",
    "    'acs_f814w_mag', 'acs_f814w_magerr',\n",
    "    'acs_fwhm_world', 'acs_mu_max',\n",
    "    'solution_model' # This is categorical, but still COSMOS-derived info\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Storing features\n",
    "feature_sets = {\n",
    "        # --- Potential Input Feature Sets ---\n",
    "        'morphology_only': morphology_mags_errors,\n",
    "        'photometry_magnitudes_only': photometry_magnitudes,\n",
    "        'photometry_mags_errors': photometry_mags_errors,\n",
    "        'photometry_plus_morphology': photometry_mags_errors + morphology_mags_errors,\n",
    "        'photometry_no_redshift': photometry_mags_errors + morphology_mags_errors + quality_aux_features,\n",
    "        'redshift_only': redshift_mags_errors,\n",
    "        'full_alhambra_all': (morphology_mags_errors +\n",
    "                            photometry_mags_errors +\n",
    "                            redshift_mags_errors + \n",
    "                            quality_aux_features),\n",
    "\n",
    "        # --- Excluded Feature Sets ---\n",
    "        'non_modeling_identifiers': non_modeling_identifiers,\n",
    "        'non_modeling_astrometry': non_modeling_astrometry,\n",
    "        'non_modeling_flags': non_modeling_flags,\n",
    "        'non_modeling_aperture_mags': non_modeling_aperture_mags,\n",
    "        'non_modeling_cosmos_features': non_modeling_cosmos_features,\n",
    "        'alhambra_prediction': alhambra_prediction,\n",
    "        'target_variable': target_variable\n",
    "    }\n",
    "\n",
    "# Groups of features for modeling\n",
    "groups = {\n",
    "        'group_1': feature_sets.get('morphology_only', []) + feature_sets.get('target_variable', []),\n",
    "        'group_2': feature_sets.get('photometry_mags_errors', []) + feature_sets.get('target_variable', []),\n",
    "        'group_3': (feature_sets.get('morphology_only', []) +\n",
    "                   feature_sets.get('photometry_mags_errors', []) +\n",
    "                   feature_sets.get('target_variable', []))\n",
    "    }\n",
    "\n",
    "# --- Function to get a specific feature set ---\n",
    "\n",
    "def get_feature_set(df, set_name, groups = groups):\n",
    "    \"\"\"\n",
    "    Selects columns from a DataFrame based on a predefined feature set name,\n",
    "    including six specific groups defined by combinations of morphology,\n",
    "    photometry magnitudes, uncertainties, and redshift features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        set_name (str): The name of the desired feature set group:\n",
    "                        'group_1' to 'group_6' as defined below.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing only the columns\n",
    "                      belonging to the specified feature set group.\n",
    "                      Returns an empty DataFrame if no columns are found.\n",
    "    \"\"\"\n",
    "\n",
    "    if set_name not in groups:\n",
    "        raise ValueError(f\"Feature set group '{set_name}' not defined. \"\n",
    "                         f\"Available groups: {list(groups.keys())}\")\n",
    "\n",
    "    required_cols_in_set = groups[set_name]\n",
    "\n",
    "    # Find which of these columns actually exist in the DataFrame\n",
    "    available_cols = [col for col in required_cols_in_set if col in df.columns]\n",
    "\n",
    "    # Warn if some columns from the set definition are missing\n",
    "    missing_cols = [col for col in required_cols_in_set if col not in available_cols]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: The following columns defined for feature set group '{set_name}'\"\n",
    "              f\" were not found in the DataFrame and will be excluded: {missing_cols}\")\n",
    "\n",
    "    if not available_cols:\n",
    "        print(f\"Warning: No columns for feature set group '{set_name}' found in the DataFrame.\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "\n",
    "    print(f\"Selecting feature set group '{set_name}' with {len(available_cols)} columns.\")\n",
    "    return df[available_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca368d9",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b117b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting parameters\n",
    "TEST_SIZE = 0.20 # Test set proportion\n",
    "VAL_SIZE = 0.10 # Validation set proportion\n",
    "CAL_SIZE = 0.00 # Calibration set proportion\n",
    "# Train size will be 1 - (TEST_SIZE + VAL_SIZE + CAL_SIZE)\n",
    "\n",
    "TARGET_COLUMN = feature_sets.get('target_variable', [])[0]\n",
    "RANDOM_SEED = 33 # For reproducibility\n",
    "\n",
    "# Model saving directory\n",
    "MODEL_DIR = \"trained_models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Data splitting strategy ('stratified' or 'random')\n",
    "SPLIT_STRATEGY = 'stratified' # Recommended for imbalanced datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "561ab2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Cleaning ---\n",
    "def clean_data(df, feature_group, target_column, logger=logging):\n",
    "    \"\"\"\n",
    "    Cleans the input DataFrame by selecting features for the given group,\n",
    "    dropping NaNs, and separating features and target.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        feature_group (str): The feature group to use (e.g., 'group_1', 'group_2', etc.).\n",
    "        target_column (str): The name of the target column.\n",
    "        logger (logging.Logger): Logger for info and error messages.\n",
    "\n",
    "    Returns:\n",
    "        X (pd.DataFrame): Cleaned feature DataFrame.\n",
    "        y (pd.Series): Target variable.\n",
    "        df_clean (pd.DataFrame): The cleaned DataFrame (features + target).\n",
    "    \"\"\"\n",
    "    logger.info(f\"Original dataset size: {df.shape}\")\n",
    "\n",
    "    # Get the feature columns for the selected group using get_feature_set\n",
    "    df_clean = get_feature_set(df, feature_group).dropna().copy()\n",
    "    logger.info(f\"Dataset size after dropping NaNs: {df_clean.shape}\")\n",
    "\n",
    "    # Ensure target_column is defined correctly\n",
    "    if target_column not in df_clean.columns:\n",
    "        raise KeyError(f\"Target column '{target_column}' not found in the cleaned DataFrame columns: {df_clean.columns.tolist()}\")\n",
    "\n",
    "    # Log value counts for target\n",
    "    logger.info(f\"Value counts for target:\\n1 (Star): {(df_clean[target_column] == 1).sum()}\\n0 (Galaxy): {(df_clean[target_column] == 0).sum()}\")\n",
    "\n",
    "    # Separate features (X) and target (y) for the cleaned DataFrame\n",
    "    X = df_clean.drop(columns=[target_column])\n",
    "    y = df_clean[target_column]\n",
    "    return X, y, df_clean\n",
    "\n",
    "# Example usage:\n",
    "# X, y, df_clean = clean_data(df, feature_group='group_7', target_column=TARGET_COLUMN, logger=logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4576d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Splitting ---\n",
    "def split_data(X, y):\n",
    "    \"\"\"\n",
    "    Splits the data into train, validation, test, and calibration sets according to the global\n",
    "    split proportions and strategy. Uses global variables:\n",
    "        - TEST_SIZE, VAL_SIZE, CAL_SIZE, SPLIT_STRATEGY, RANDOM_SEED\n",
    "\n",
    "    The logic and split order is identical to the original code.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature DataFrame.\n",
    "        y (pd.Series): Target variable.\n",
    "\n",
    "    Returns:\n",
    "        (X_train, y_train, X_val, y_val, X_test, y_test, X_cal, y_cal): tuple of splits.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Splitting data using '{SPLIT_STRATEGY}' strategy...\")\n",
    "\n",
    "    # --- Validate Proportions ---\n",
    "    if not (0 <= TEST_SIZE <= 1 and 0 <= VAL_SIZE <= 1 and 0 <= CAL_SIZE <= 1):\n",
    "        raise ValueError(\"Split proportions (TEST_SIZE, VAL_SIZE, CAL_SIZE) must be between 0 and 1.\")\n",
    "\n",
    "    TRAIN_SIZE = 1.0 - TEST_SIZE - VAL_SIZE - CAL_SIZE\n",
    "    if not (0 <= TRAIN_SIZE <= 1):\n",
    "        raise ValueError(f\"Calculated TRAIN_SIZE ({TRAIN_SIZE:.3f}) is invalid. Sum of TEST_SIZE, VAL_SIZE, and CAL_SIZE must be between 0 and 1.\")\n",
    "\n",
    "    if not np.isclose(TRAIN_SIZE + TEST_SIZE + VAL_SIZE + CAL_SIZE, 1.0):\n",
    "        # This check might be redundant given the calculation of TRAIN_SIZE, but good for safety.\n",
    "        raise ValueError(\"Sum of split proportions must be equal to 1.\")\n",
    "\n",
    "    if np.isclose(TRAIN_SIZE, 0) and (np.isclose(VAL_SIZE, 0) or np.isclose(TEST_SIZE, 0) or np.isclose(CAL_SIZE, 0)):\n",
    "        # Avoid scenarios where train is 0 but other splits are also 0, leading to ambiguity.\n",
    "        # If only train is 0, it might be valid in some rare cases, but usually requires at least one other non-zero split.\n",
    "        # Let's enforce Train > 0 for typical ML workflows.\n",
    "        # If you need zero training data, adjust this check.\n",
    "        logging.warning(\"TRAIN_SIZE is zero or near zero. Ensure this is intended.\")\n",
    "        if TRAIN_SIZE < 0: # Definitely an error\n",
    "            raise ValueError(\"TRAIN_SIZE cannot be negative.\")\n",
    "        # Allow TRAIN_SIZE = 0 only if explicitly handled later, otherwise raise error?\n",
    "        # For now, let's proceed but log a warning. If TRAIN_SIZE must be > 0, uncomment the raise below.\n",
    "        # raise ValueError(\"TRAIN_SIZE must be greater than 0 for typical model training.\")\n",
    "\n",
    "    logging.info(f\"Target split ratios: Train={TRAIN_SIZE:.2f}, Val={VAL_SIZE:.2f}, Test={TEST_SIZE:.2f}, Cal={CAL_SIZE:.2f}\")\n",
    "\n",
    "    # --- Initialize Splits ---\n",
    "    # Use iloc[0:0] to create empty DataFrames/Series with the same columns/dtype\n",
    "    empty_X = X.iloc[0:0]\n",
    "    empty_y = y.iloc[0:0]\n",
    "    X_train, y_train = empty_X.copy(), empty_y.copy()\n",
    "    X_val, y_val = empty_X.copy(), empty_y.copy()\n",
    "    X_test, y_test = empty_X.copy(), empty_y.copy()\n",
    "    X_cal, y_cal = empty_X.copy(), empty_y.copy()\n",
    "\n",
    "    # Temporary variables for sequential splitting\n",
    "    X_remaining, y_remaining = X.copy(), y.copy() # Use copies to avoid modifying original X, y\n",
    "\n",
    "    # --- Stratification Option ---\n",
    "    # Define stratify_func only once\n",
    "    def get_stratify_array(y_arr):\n",
    "        return y_arr if SPLIT_STRATEGY == 'stratified' and not y_arr.empty else None\n",
    "\n",
    "    # --- First Split: Train vs. Remainder (Val + Test + Cal) ---\n",
    "    val_test_cal_size = VAL_SIZE + TEST_SIZE + CAL_SIZE\n",
    "\n",
    "    if np.isclose(val_test_cal_size, 0): # Only Train set needed\n",
    "        X_train, y_train = X_remaining, y_remaining\n",
    "        logging.info(\"All data assigned to Train set (Val, Test, Cal sizes are 0).\")\n",
    "        X_remaining, y_remaining = empty_X.copy(), empty_y.copy() # No remainder\n",
    "    elif np.isclose(TRAIN_SIZE, 0): # No Train set needed\n",
    "        logging.info(\"Train set is empty (TRAIN_SIZE=0). Remainder passed to next splits.\")\n",
    "        # X_remaining, y_remaining already hold all data\n",
    "    else: # Split Train vs Remainder\n",
    "        split_test_size = val_test_cal_size # Proportion of remainder relative to total (1.0)\n",
    "        X_train, X_remaining, y_train, y_remaining = train_test_split(\n",
    "            X_remaining, y_remaining,\n",
    "            test_size=split_test_size,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=get_stratify_array(y_remaining)\n",
    "        )\n",
    "    logging.info(f\"Train set shape: {X_train.shape}\")\n",
    "\n",
    "    # --- Second Split: Val vs. Remainder (Test + Cal) ---\n",
    "    if not X_remaining.empty:\n",
    "        test_cal_size = TEST_SIZE + CAL_SIZE\n",
    "        # Denominator for relative size calculation: size of the current remaining pool\n",
    "        current_remaining_size_frac = VAL_SIZE + test_cal_size # = val_test_cal_size\n",
    "\n",
    "        if np.isclose(VAL_SIZE, 0): # No Val set, pass remainder to next stage\n",
    "            X_temp2, y_temp2 = X_remaining, y_remaining # Remainder is Test + Cal\n",
    "            logging.info(\"Validation set is empty (VAL_SIZE=0).\")\n",
    "        elif np.isclose(test_cal_size, 0): # Only Val set left in remainder\n",
    "            X_val, y_val = X_remaining, y_remaining\n",
    "            X_temp2, y_temp2 = empty_X.copy(), empty_y.copy() # No data left for Test/Cal\n",
    "            logging.info(f\"Validation set shape: {X_val.shape}\")\n",
    "        else: # Split Val vs (Test + Cal)\n",
    "            # Proportion of (Test + Cal) relative to (Val + Test + Cal)\n",
    "            split_test_size = test_cal_size / current_remaining_size_frac\n",
    "            X_val, X_temp2, y_val, y_temp2 = train_test_split(\n",
    "                X_remaining, y_remaining,\n",
    "                test_size=split_test_size,\n",
    "                random_state=RANDOM_SEED,\n",
    "                stratify=get_stratify_array(y_remaining)\n",
    "            )\n",
    "            logging.info(f\"Validation set shape: {X_val.shape}\")\n",
    "    else: # No data remaining after train split\n",
    "        X_temp2, y_temp2 = empty_X.copy(), empty_y.copy()\n",
    "        if not np.isclose(VAL_SIZE, 0): # Log only if Val set was expected\n",
    "            logging.info(\"Validation set is empty (no data remaining after train split).\")\n",
    "\n",
    "    # --- Third Split: Test vs. Cal ---\n",
    "    if not X_temp2.empty:\n",
    "        # Denominator for relative size calculation: size of the current remaining pool\n",
    "        current_remaining_size_frac = TEST_SIZE + CAL_SIZE # = test_cal_size\n",
    "\n",
    "        if np.isclose(CAL_SIZE, 0): # No Cal set, remainder is Test\n",
    "            X_test, y_test = X_temp2, y_temp2\n",
    "            logging.info(\"Calibration set is empty (CAL_SIZE=0).\")\n",
    "        elif np.isclose(TEST_SIZE, 0): # Only Cal set left in remainder\n",
    "            X_cal, y_cal = X_temp2, y_temp2\n",
    "            logging.info(\"Test set is empty (TEST_SIZE=0).\")\n",
    "        else: # Split Test vs Cal\n",
    "            # Proportion of Cal relative to (Test + Cal)\n",
    "            split_test_size = CAL_SIZE / current_remaining_size_frac\n",
    "            X_test, X_cal, y_test, y_cal = train_test_split(\n",
    "                X_temp2, y_temp2,\n",
    "                test_size=split_test_size,\n",
    "                random_state=RANDOM_SEED,\n",
    "                stratify=get_stratify_array(y_temp2)\n",
    "            )\n",
    "            # Logging shapes done after the if/else block\n",
    "    else: # No data remaining for Test/Cal split\n",
    "        if not (np.isclose(TEST_SIZE, 0) and np.isclose(CAL_SIZE, 0)): # Log only if Test or Cal were expected\n",
    "            logging.info(\"Test and Calibration sets are empty (no data remaining for final split).\")\n",
    "\n",
    "    # Log final shapes for Test and Cal\n",
    "    logging.info(f\"Test set shape: {X_test.shape}\")\n",
    "    logging.info(f\"Calibration set shape: {X_cal.shape}\")\n",
    "\n",
    "    # --- Verification and Final Logging ---\n",
    "    total_len = len(X_train) + len(X_val) + len(X_test) + len(X_cal)\n",
    "    original_len = len(X)\n",
    "\n",
    "    if total_len != original_len:\n",
    "        # Calculate actual proportions based on lengths\n",
    "        actual_train = len(X_train) / original_len if original_len > 0 else 0\n",
    "        actual_val = len(X_val) / original_len if original_len > 0 else 0\n",
    "        actual_test = len(X_test) / original_len if original_len > 0 else 0\n",
    "        actual_cal = len(X_cal) / original_len if original_len > 0 else 0\n",
    "        logging.warning(f\"Total split length ({total_len}) does not exactly match original length ({original_len}). \"\n",
    "                        f\"This can happen with stratification or rounding. \"\n",
    "                        f\"Target proportions: Train={TRAIN_SIZE:.3f}, Val={VAL_SIZE:.3f}, Test={TEST_SIZE:.3f}, Cal={CAL_SIZE:.3f}. \"\n",
    "                        f\"Actual proportions: Train={actual_train:.3f}, Val={actual_val:.3f}, Test={actual_test:.3f}, Cal={actual_cal:.3f}\")\n",
    "    else:\n",
    "        logging.info(\"Split lengths verification successful.\")\n",
    "\n",
    "    logging.info(\"Data splitting complete.\")\n",
    "\n",
    "    # Log distributions, handling empty sets\n",
    "    def log_distribution(name, y_set):\n",
    "        if y_set.empty:\n",
    "            logging.info(f\"{name} target distribution: Set is empty.\")\n",
    "        else:\n",
    "            try:\n",
    "                # Use normalize=True, handle potential division by zero if counts are zero (though unlikely if not empty)\n",
    "                counts = y_set.value_counts()\n",
    "                dist = counts / counts.sum() if counts.sum() > 0 else counts\n",
    "                logging.info(f\"{name} target distribution:\\n{dist}\")\n",
    "                # Log absolute counts as well for clarity\n",
    "                logging.info(f\"{name} target counts:\\n{counts}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Could not calculate distribution for {name}: {e}\")\n",
    "                # Attempt to log raw value counts even if normalization fails\n",
    "                try:\n",
    "                    logging.info(f\"{name} raw value counts:\\n{y_set.value_counts()}\")\n",
    "                except Exception as e_raw:\n",
    "                    logging.error(f\"Could not get raw value counts for {name}: {e_raw}\")\n",
    "\n",
    "    log_distribution(\"Train\", y_train)\n",
    "    log_distribution(\"Validation\", y_val)\n",
    "    log_distribution(\"Test\", y_test)\n",
    "    log_distribution(\"Calibration\", y_cal)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_cal, y_cal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1aa63",
   "metadata": {},
   "source": [
    "# 3. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25e5249",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization via Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fc67c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Internal Helper ---\n",
    "def _train_and_eval(model_class, params,\n",
    "                    X_train, y_train, X_val, y_val,\n",
    "                    resource, resource_type,\n",
    "                    scoring_func, random_state):\n",
    "    \"\"\"Internal helper function to train and evaluate a single configuration.\"\"\"\n",
    "    try:\n",
    "        # Instantiate the base model without iteration-specific params first\n",
    "        # Iteration param (e.g., n_estimators) will be handled later if needed\n",
    "        model = model_class(**params)\n",
    "\n",
    "        fit_duration = 0.0\n",
    "        eval_duration = 0.0\n",
    "        start_fit = time.time() # Start timing fit process\n",
    "\n",
    "        if resource_type == 'data_fraction':\n",
    "            # --- FIX 1: Implement data subsetting ---\n",
    "            if resource < 1.0:\n",
    "                # Use train_test_split to get a stratified fraction\n",
    "                # We only need the 'train' part of this split for the subset\n",
    "                try:\n",
    "                    X_subset, _, y_subset, _ = train_test_split(\n",
    "                        X_train, y_train,\n",
    "                        train_size=resource,\n",
    "                        random_state=random_state, # Use provided random state\n",
    "                        stratify=y_train # Stratify based on original train labels\n",
    "                    )\n",
    "                except ValueError as e:\n",
    "                    # Handle cases where stratification is not possible (e.g., too few samples)\n",
    "                    logging.warning(f\"Stratification failed for resource {resource:.2f}: {e}. Falling back to non-stratified split.\")\n",
    "                    X_subset, _, y_subset, _ = train_test_split(\n",
    "                        X_train, y_train,\n",
    "                        train_size=resource,\n",
    "                        random_state=random_state\n",
    "                    )\n",
    "            else:\n",
    "                # Use the full training data if resource is 1.0\n",
    "                X_subset, y_subset = X_train, y_train\n",
    "\n",
    "            # Ensure y_subset is numpy for fitting if needed by model\n",
    "            y_subset_np = y_subset.values if isinstance(y_subset, pd.Series) else y_subset\n",
    "\n",
    "            # Fit the model \n",
    "            model.fit(X_subset, y_subset_np)\n",
    "            fit_duration = time.time() - start_fit\n",
    "            # -----------------------------------------\n",
    "\n",
    "        elif resource_type == 'iterations':\n",
    "            # Resource represents n_estimators or similar iteration parameter\n",
    "            params_iter = params.copy() # Avoid modifying original params dict\n",
    "            iter_param_name = 'n_estimators' # Common case for RF, XGB, LGBM\n",
    "\n",
    "            # Ensure resource is an integer for iterations\n",
    "            params_iter[iter_param_name] = int(max(1, resource)) # Ensure at least 1 iteration\n",
    "            model = model_class(**params_iter) # Re-instantiate with correct n_estimators\n",
    "\n",
    "            # --- FIX 2 & 3: Conditional Fit Parameters ---\n",
    "            current_fit_args = {} # Dictionary for specific fit arguments\n",
    "            eval_set_for_fit = [(X_val, y_val)] # Common eval set\n",
    "\n",
    "            if model_class is xgb.XGBClassifier:\n",
    "                current_fit_args['eval_set'] = eval_set_for_fit\n",
    "                current_fit_args['verbose'] = False\n",
    "\n",
    "            elif model_class is lgb.LGBMClassifier:\n",
    "                current_fit_args['eval_set'] = eval_set_for_fit\n",
    "                if 'metric' in params_iter: # Get metric from HPO params\n",
    "                     current_fit_args['eval_metric'] = params_iter['metric']\n",
    "                elif isinstance(model.metric, str): # Get metric from model instance if set\n",
    "                     current_fit_args['eval_metric'] = model.metric\n",
    "                else: # Default if not found (might cause issues if early stopping expects it)\n",
    "                     logging.warning(f\"LGBM eval_metric not found in HPO params or model instance for config {params_iter}. Early stopping might fail.\")\n",
    "                     # You might need to add a default like 'logloss' or raise an error\n",
    "                     # current_fit_args['eval_metric'] = 'logloss' # Example default\n",
    "\n",
    "            # For models like RandomForest or DecisionTree, current_fit_args remains empty {}\n",
    "            # as they don't use eval_set or callbacks in their standard fit method\n",
    "\n",
    "            # Fit the model with appropriate arguments\n",
    "            # Ensure y_train is numpy if needed\n",
    "            y_train_np = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "            try:\n",
    "                model.fit(X_train, y_train_np, **current_fit_args)\n",
    "            except Exception as fit_error:\n",
    "                 logging.error(f\"Fit failed for config {params_iter} with resource {resource}: {fit_error}\")\n",
    "                 # logging.exception(\"Fit Traceback:\") # Uncomment for full traceback\n",
    "                 return -1.0 # Indicate failure\n",
    "            fit_duration = time.time() - start_fit\n",
    "            # -----------------------------------------\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid resource_type. Choose 'data_fraction' or 'iterations'.\")\n",
    "\n",
    "        # Evaluate on the full validation set (common part)\n",
    "        start_eval = time.time()\n",
    "        try:\n",
    "             y_pred_val = model.predict(X_val)\n",
    "             # Ensure y_val is numpy if needed by scoring_func\n",
    "             y_val_np = y_val.values if isinstance(y_val, pd.Series) else y_val\n",
    "             score = scoring_func(y_val_np, y_pred_val)\n",
    "        except Exception as eval_error:\n",
    "             logging.error(f\"Predict/Score failed for config {params} with resource {resource}: {eval_error}\")\n",
    "             score = -1.0 # Indicate failure\n",
    "        eval_duration = time.time() - start_eval\n",
    "\n",
    "        logging.debug(f\"Evaluated config: {params} | Resource: {resource:.2f} | Score: {score:.4f} | Fit: {fit_duration:.2f}s | Eval: {eval_duration:.2f}s\")\n",
    "        return score\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the configuration that caused the error\n",
    "        logging.error(f\"Error training/evaluating config {params} with resource {resource}: {e}\", exc_info=False) # Set exc_info=True for traceback if needed\n",
    "        return -1.0 # Return a clearly bad score\n",
    "\n",
    "\n",
    "def hyperband_hpo(model_class, param_space,\n",
    "                  X_train, y_train, X_val, y_val,\n",
    "                  max_resource, eta=3, resource_type='iterations',\n",
    "                  min_resource=1, # Min iterations or min data fraction\n",
    "                  scoring_func=f1_score, # Function accepting (y_true, y_pred)\n",
    "                  random_state=None): # For early stopping etc. passed to .fit()\n",
    "    \"\"\"\n",
    "    Performs Hyperband Hyperparameter Optimization.\n",
    "\n",
    "    Args:\n",
    "        model_class: The model class (e.g., SVC, RandomForestClassifier).\n",
    "        param_space (dict): Dictionary defining the hyperparameter search space\n",
    "                           compatible with ParameterSampler.\n",
    "        X_train, y_train: Training data and labels.\n",
    "        X_val, y_val: Validation data and labels for evaluation.\n",
    "        max_resource (float/int): Maximum resource allocation\n",
    "                                 (e.g., max n_estimators or 1.0 for data fraction).\n",
    "        eta (int): Reduction factor for successive halving (>= 2).\n",
    "        resource_type (str): How resource is allocated:\n",
    "                             'iterations' -> resource sets n_estimators (or similar).\n",
    "                             'data_fraction' -> resource is fraction of training data used (stratified).\n",
    "        min_resource (float/int): Minimum resource for the first iteration.\n",
    "                                 Must be >= 1 for 'iterations', > 0 for 'data_fraction'.\n",
    "        scoring_func (callable): Function to evaluate performance (e.g., f1_score).\n",
    "                                Higher score is assumed better.\n",
    "        random_state (int): Seed for reproducibility of parameter sampling and data subsetting.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_params, best_score)\n",
    "               best_params (dict): The hyperparameters of the best performing configuration.\n",
    "               best_score (float): The score achieved by the best configuration on the validation set\n",
    "                                  using the maximum resource.\n",
    "    \"\"\"\n",
    "\n",
    "    log_max_r = math.log(max_resource / min_resource, eta) if max_resource > min_resource and min_resource > 0 else 0\n",
    "    s_max = int(log_max_r)\n",
    "    B = (s_max + 1) * max_resource # Approximate total resource budget\n",
    "\n",
    "    logging.info(f\"--- Starting Hyperband HPO ---\")\n",
    "    logging.info(f\"Model: {model_class.__name__}\")\n",
    "    logging.info(f\"Resource Type: {resource_type}\")\n",
    "    logging.info(f\"Resource Range: [{min_resource}, {max_resource}]\")\n",
    "    logging.info(f\"Eta: {eta}\")\n",
    "    logging.info(f\"Max Brackets (s_max): {s_max}\")\n",
    "    logging.info(f\"Approx. Budget (B): {B:.2f}\")\n",
    "    logging.info(f\"Scoring: {scoring_func.__name__}\")\n",
    "\n",
    "    best_params = None\n",
    "    best_score = -1.0\n",
    "    total_configs_evaluated = 0\n",
    "    outer_tqdm = tqdm(range(s_max, -1, -1), desc=\"Hyperband Brackets (s)\")\n",
    "\n",
    "    # Outer loop: Iterate through brackets (s values)\n",
    "    for s in outer_tqdm:\n",
    "        n_configs = int(math.ceil(int(B / max_resource / (s + 1)) * eta**s)) # Number of configs in this bracket\n",
    "        r_initial = max_resource * eta**(-s) # Initial resource for this bracket\n",
    "        # Ensure initial resource is not less than min_resource\n",
    "        r_initial = max(r_initial, min_resource)\n",
    "\n",
    "        outer_tqdm.set_description(f\"Bracket s={s} (n={n_configs}, r0={r_initial:.2f})\")\n",
    "        logging.info(f\"\\n>> Bracket s={s}: n_configs={n_configs}, r_initial={r_initial:.2f}\")\n",
    "\n",
    "        # Sample configurations for this bracket\n",
    "        param_list = list(ParameterSampler(param_space, n_iter=n_configs, random_state=random_state + s if random_state is not None else None))\n",
    "        \n",
    "        # --- Add common fixed parameters ---\n",
    "        # Calculate scale_pos_weight once if needed\n",
    "        scale_pos_weight_val = None\n",
    "        if model_class in [xgb.XGBClassifier, lgb.LGBMClassifier]:\n",
    "             neg_count = (y_train == 0).sum()\n",
    "             pos_count = (y_train == 1).sum()\n",
    "             if pos_count > 0:\n",
    "                 scale_pos_weight_val = neg_count / pos_count\n",
    "\n",
    "        for p in param_list:\n",
    "             # Add random_state if model supports it and it's not sampled\n",
    "             if 'random_state' not in p and hasattr(model_class(random_state=1), 'random_state'): # Check if attr exists\n",
    "                 p['random_state'] = random_state\n",
    "             # Add class_weight='balanced' for relevant sklearn models if not sampled\n",
    "             if model_class in [SVC, RandomForestClassifier, DecisionTreeClassifier] and 'class_weight' not in p:\n",
    "                 p['class_weight'] = 'balanced'\n",
    "             # Add scale_pos_weight for boosting models if not sampled and calculated\n",
    "             if model_class in [xgb.XGBClassifier, lgb.LGBMClassifier] and 'scale_pos_weight' not in p and scale_pos_weight_val is not None:\n",
    "                  p['scale_pos_weight'] = scale_pos_weight_val\n",
    "             # For LightGBM, also consider adding 'objective': 'binary' if not sampled\n",
    "             if model_class is lgb.LGBMClassifier and 'objective' not in p:\n",
    "                  p['objective'] = 'binary'\n",
    "        # -----------------------------------\n",
    "\n",
    "        # Inner loop: Successive halving rounds\n",
    "        inner_tqdm = tqdm(range(s + 1), desc=f\"SH Round (s={s})\", leave=False)\n",
    "        for i in inner_tqdm:\n",
    "            current_resource = r_initial * eta**i\n",
    "            # Ensure resource doesn't exceed max_resource due to floating point/rounding\n",
    "            current_resource = min(current_resource, max_resource)\n",
    "\n",
    "            n_configs_in_round = len(param_list)\n",
    "            inner_tqdm.set_description(f\"SH Round i={i} (n={n_configs_in_round}, r={current_resource:.2f})\")\n",
    "            logging.info(f\"  -- Round i={i}: Evaluating {n_configs_in_round} configs with resource={current_resource:.2f} --\")\n",
    "\n",
    "            round_scores = []\n",
    "            # Use tqdm for the configurations within the round\n",
    "            eval_tqdm = tqdm(param_list, desc=f\"Evaluating Configs (i={i})\", leave=False)\n",
    "            for params in eval_tqdm:\n",
    "                score = _train_and_eval(model_class, params, X_train, y_train, X_val, y_val,\n",
    "                                        current_resource, resource_type, scoring_func,\n",
    "                                        random_state)\n",
    "                round_scores.append((score, params))\n",
    "                total_configs_evaluated += 1 # Count unique evaluations\n",
    "\n",
    "            # Sort by score (descending, higher is better)\n",
    "            round_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            # Track the best overall score and params seen so far *at max resource*\n",
    "            # Only update if we are actually at max resource in this round\n",
    "            if abs(current_resource - max_resource) < 1e-6: # Check if we are at max resource\n",
    "                 if round_scores and round_scores[0][0] > best_score:\n",
    "                      best_score = round_scores[0][0]\n",
    "                      best_params = round_scores[0][1]\n",
    "                      logging.info(f\"  ** New Best Found (Score: {best_score:.4f}) at max resource ** Params: {best_params}\")\n",
    "                      # Update outer tqdm description with best score found so far\n",
    "                      outer_tqdm.set_postfix_str(f\"Best F1: {best_score:.4f}\", refresh=True)\n",
    "\n",
    "\n",
    "            # --- Halving Step ---\n",
    "            n_keep = int(n_configs_in_round / eta)\n",
    "            logging.info(f\"  -- Round i={i}: Completed {len(round_scores)} evaluations. Keeping top {n_keep} configs. --\")\n",
    "\n",
    "            if n_keep < 1 or i == s: # Keep at least one, or if it's the last round\n",
    "                # If it's the last round, ensure the best score from *this bracket* at *max resource* is considered\n",
    "                if abs(current_resource - max_resource) < 1e-6 and round_scores:\n",
    "                     bracket_best_score = round_scores[0][0]\n",
    "                     bracket_best_params = round_scores[0][1]\n",
    "                     logging.info(f\"  Bracket s={s} final best score: {bracket_best_score:.4f}\")\n",
    "                     # No need to update global best here, already done above\n",
    "                break # Exit inner loop\n",
    "\n",
    "            # Prepare parameter list for the next round\n",
    "            param_list = [params for score, params in round_scores[:n_keep]]\n",
    "            if not param_list: # Safety break if list becomes empty unexpectedly\n",
    "                 logging.warning(f\"  Param list empty after halving round i={i}. Stopping bracket.\")\n",
    "                 break\n",
    "\n",
    "    logging.info(f\"\\n--- Hyperband HPO Finished ---\")\n",
    "    logging.info(f\"Total configurations evaluated (approx): {total_configs_evaluated}\") # Might overcount if errors happened\n",
    "    if best_params:\n",
    "        logging.info(f\"Best Overall Score ({scoring_func.__name__}): {best_score:.4f}\")\n",
    "        logging.info(f\"Best Params: {best_params}\")\n",
    "    else:\n",
    "        logging.warning(\"No best parameters found. Check logs for errors or increase resources/configs.\")\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe9f2d",
   "metadata": {},
   "source": [
    "### Calibration (Platt Scaling/Isotonic Regression/Cross Venn Abers Predictors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4a1430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raw_cvap import RawVennAbers, CVAPPredictorRaw\n",
    "\n",
    "# --- Helper Function to Get Scores ---\n",
    "def get_scores(estimator, X, score_method):\n",
    "    \"\"\"Gets scores from an estimator based on the specified method.\"\"\"\n",
    "    if score_method == 'decision_function':\n",
    "        if hasattr(estimator, 'decision_function'):\n",
    "            scores = estimator.decision_function(X)\n",
    "            # Ensure scores are 1D for binary classification\n",
    "            if scores.ndim == 2 and scores.shape[1] == 1:\n",
    "                 scores = scores.flatten()\n",
    "            elif scores.ndim > 1:\n",
    "                 # For binary, decision_function should be 1D. If not, maybe multiclass? Raise error.\n",
    "                 raise ValueError(f\"decision_function returned shape {scores.shape}, expected 1D for binary classification.\")\n",
    "            return scores\n",
    "        else:\n",
    "            raise AttributeError(f\"{estimator.__class__.__name__} does not have 'decision_function' method.\")\n",
    "    elif score_method == 'predict_proba':\n",
    "        if hasattr(estimator, 'predict_proba'):\n",
    "            # Return probability of the positive class (class 1)\n",
    "            proba = estimator.predict_proba(X)\n",
    "            if proba.shape[1] != 2:\n",
    "                 raise ValueError(f\"predict_proba returned shape {proba.shape}, expected (n_samples, 2)\")\n",
    "            return proba[:, 1]\n",
    "        else:\n",
    "            raise AttributeError(f\"{estimator.__class__.__name__} does not have 'predict_proba' method.\")\n",
    "    elif score_method == 'raw_margin_xgb':\n",
    "        # Check if it looks like an XGBoost model (basic check)\n",
    "        # A more robust check might involve isinstance(estimator, xgboost.XGBModel) after importing xgboost\n",
    "        if hasattr(estimator, 'predict') and hasattr(estimator, 'get_params') and 'objective' in estimator.get_params():\n",
    "             try:\n",
    "                 # XGBoost convention: predict with output_margin=True gives raw scores\n",
    "                 # For binary classification, this is usually a single value per instance\n",
    "                 scores = estimator.predict(X, output_margin=True)\n",
    "                 return scores.flatten() # Ensure 1D\n",
    "             except TypeError as e:\n",
    "                  # Check if the error message specifically mentions 'output_margin'\n",
    "                  if 'output_margin' in str(e):\n",
    "                      raise TypeError(f\"'output_margin' might not be a valid parameter for predict in this XGBoost version or configuration. Error: {e}\")\n",
    "                  else:\n",
    "                      raise TypeError(f\"Error calling predict with output_margin=True on {estimator.__class__.__name__}. Is it an XGBoost model? Original error: {e}\")\n",
    "\n",
    "        else:\n",
    "             raise AttributeError(f\"{estimator.__class__.__name__} might not be an XGBoost model supporting 'output_margin'.\")\n",
    "    elif score_method == 'raw_score_lgbm':\n",
    "         # Check if it looks like a LightGBM model (basic check)\n",
    "         # A more robust check might involve isinstance(estimator, lightgbm.LGBMModel) after importing lightgbm\n",
    "        if hasattr(estimator, 'predict') and hasattr(estimator, 'get_params') and 'objective' in estimator.get_params():\n",
    "             try:\n",
    "                 # LightGBM convention: predict with raw_score=True gives raw scores\n",
    "                 # For binary classification, output shape might depend on objective.\n",
    "                 # Often (n_samples,) or (n_samples, 1) for binary logloss/cross_entropy\n",
    "                 scores = estimator.predict(X, raw_score=True)\n",
    "                 # Handle potential (n_samples, 1) output for binary\n",
    "                 if scores.ndim == 2 and scores.shape[1] == 1:\n",
    "                     scores = scores.flatten()\n",
    "                 elif scores.ndim != 1:\n",
    "                      # If multiclass raw_score=True might return (n_samples, n_classes)\n",
    "                      raise ValueError(f\"LightGBM raw_score returned shape {scores.shape}. Expected 1D for binary.\")\n",
    "                 return scores\n",
    "             except TypeError as e:\n",
    "                 # Check if the error message specifically mentions 'raw_score'\n",
    "                 if 'raw_score' in str(e):\n",
    "                      raise TypeError(f\"'raw_score' might not be a valid parameter for predict in this LightGBM version or configuration. Error: {e}\")\n",
    "                 else:\n",
    "                      raise TypeError(f\"Error calling predict with raw_score=True on {estimator.__class__.__name__}. Is it a LightGBM model? Original error: {e}\")\n",
    "        else:\n",
    "             raise AttributeError(f\"{estimator.__class__.__name__} might not be a LightGBM model supporting 'raw_score'.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported score_method: {score_method}. Choose 'decision_function', 'predict_proba', 'raw_margin_xgb', or 'raw_score_lgbm'.\")\n",
    "\n",
    "# --- Loading and saving models ---\n",
    "def _get_save_path(model_dir: str, model_name: str, group_name: str, calibration_method: str) -> str:\n",
    "    \"\"\"\n",
    "    Constructs the standardized file path including a group subfolder\n",
    "    and ensures the target directory exists.\n",
    "\n",
    "    Args:\n",
    "        model_dir (str): The base directory for saving models.\n",
    "        model_name (str): Name of the model (e.g., 'SVM').\n",
    "        group_name (str): Name of the data group (e.g., 'group_1'). Used for subfolder.\n",
    "        calibration_method (str): The calibration method ('platt', 'isotonic', 'cvap').\n",
    "\n",
    "    Returns:\n",
    "        str: The full, absolute path to the save file.\n",
    "             Example: /path/to/models/group_1/SVM_group_1_platt.joblib\n",
    "\n",
    "    Raises:\n",
    "        OSError: If the directory cannot be created due to permissions or other OS issues.\n",
    "    \"\"\"\n",
    "    # Define the filename (remains the same as before)\n",
    "    filename = f\"{model_name}_{group_name}_{calibration_method}.joblib\"\n",
    "\n",
    "    # Define the target directory path including the group subfolder\n",
    "    # Use group_name as the subfolder name\n",
    "    target_directory = os.path.join(model_dir, group_name)\n",
    "\n",
    "    # Ensure the target directory exists; create it if necessary\n",
    "    try:\n",
    "        # os.makedirs creates parent directories as needed (like model_dir if it doesn't exist)\n",
    "        # exist_ok=True prevents an error if the directory already exists\n",
    "        os.makedirs(target_directory, exist_ok=True)\n",
    "        logging.debug(f\"Ensured directory exists: {target_directory}\")\n",
    "    except OSError as e:\n",
    "        # Log the error and re-raise it so the calling function knows something went wrong\n",
    "        logging.error(f\"Could not create directory {target_directory}: {e}\", exc_info=True)\n",
    "        raise  # Re-raise the exception to halt execution if directory creation fails\n",
    "\n",
    "    # Construct the full file path by joining the target directory and filename\n",
    "    full_file_path = os.path.join(target_directory, filename)\n",
    "\n",
    "    # Optional: Return absolute path for clarity, especially if model_dir might be relative\n",
    "    return os.path.abspath(full_file_path)\n",
    "\n",
    "def save_predictor(predictor_object, file_path: str, model_name: str):\n",
    "    \"\"\"Saves the fitted predictor object (CalibratedClassifierCV or CVAPPredictorRaw) to a file.\"\"\"\n",
    "    try:\n",
    "        joblib.dump(predictor_object, file_path)\n",
    "        logging.info(f\"--- [{model_name}] Model with calibrator saved successfully to {file_path} ---\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"--- [{model_name}] Failed to save model with calibrator to {file_path}: {e} ---\", exc_info=True)\n",
    "\n",
    "def load_predictor(file_path: str, model_name: str):\n",
    "    \"\"\"Loads the fitted predictor object from a file.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        logging.info(f\"--- [{model_name}] Predictor save file not found at {file_path}. Proceeding with training. ---\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        loaded_predictor = joblib.load(file_path)\n",
    "        # Basic checks: Is it an object? Does it have predict_proba? Is it fitted?\n",
    "        if loaded_predictor is not None and hasattr(loaded_predictor, 'predict_proba'):\n",
    "            # Check if it appears fitted (works for CalibratedClassifierCV, need similar for CVAPPredictorRaw if available)\n",
    "            try:\n",
    "                # Check if the base estimator inside CalibratedClassifierCV is fitted\n",
    "                if isinstance(loaded_predictor, CalibratedClassifierCV):\n",
    "                     # Access the fitted base estimator(s)\n",
    "                     estimators_to_check = []\n",
    "                     if hasattr(loaded_predictor, 'calibrated_classifiers_') and loaded_predictor.calibrated_classifiers_:\n",
    "                         estimators_to_check = [cc.base_estimator for cc in loaded_predictor.calibrated_classifiers_]\n",
    "                     elif hasattr(loaded_predictor, 'base_estimator_'): # For older sklearn? Or if fitted directly without CV part?\n",
    "                          estimators_to_check = [loaded_predictor.base_estimator_]\n",
    "\n",
    "                     if not estimators_to_check:\n",
    "                          raise ValueError(\"Could not find base estimator(s) in loaded CalibratedClassifierCV\")\n",
    "\n",
    "                     # Check if *all* base estimators are fitted\n",
    "                     all_fitted = all(hasattr(est, \"classes_\") or isinstance(getattr(est, \"_is_fitted\", lambda: False)(), bool) for est in estimators_to_check) # Use _is_fitted where available\n",
    "                     if not all_fitted:\n",
    "                          logging.warning(f\"--- [{model_name}] Loaded predictor from {file_path} has unfitted base estimator(s). Retraining. ---\")\n",
    "                          return None\n",
    "                # Add check for CVAPPredictorRaw if it has a standard fitted attribute or method\n",
    "                elif isinstance(loaded_predictor, CVAPPredictorRaw):\n",
    "                     # Assuming CVAPPredictorRaw has a final_estimator_ that should be fitted\n",
    "                     if not hasattr(loaded_predictor, 'final_estimator_') or not hasattr(loaded_predictor.final_estimator_, \"classes_\"):\n",
    "                           logging.warning(f\"--- [{model_name}] Loaded CVAPPredictorRaw from {file_path} seems incomplete or unfitted. Retraining. ---\")\n",
    "                           return None\n",
    "                else:\n",
    "                     # For other types, maybe just rely on predict_proba existing\n",
    "                     pass\n",
    "\n",
    "                logging.info(f\"--- [{model_name}] Predictor loaded successfully from {file_path} ---\")\n",
    "                return loaded_predictor\n",
    "\n",
    "            except (AttributeError, ValueError, TypeError) as fit_check_err:\n",
    "                 logging.warning(f\"--- [{model_name}] Loaded predictor from {file_path} raised error during fitted check ({fit_check_err}). Assuming invalid. Retraining. ---\")\n",
    "                 return None\n",
    "        else:\n",
    "            logging.warning(f\"--- [{model_name}] Loaded object from {file_path} is not a valid predictor (None or no predict_proba). Proceeding with training. ---\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"--- [{model_name}] Failed to load predictor from {file_path}: {e}. Proceeding with training. ---\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main Unified Function ---\n",
    "def train_calibrate_model(base_estimator_class, best_params, X_train, y_train,\n",
    "                          model_name: str,\n",
    "                          group_name: str,\n",
    "                          MODEL_DIR: str,\n",
    "                          calibration_method='platt', # 'platt', 'isotonic', 'cvap'\n",
    "                          n_splits=5, random_state=None,\n",
    "                          # CVAP specific params\n",
    "                          score_method='decision_function', # 'decision_function', 'predict_proba', 'raw_margin_xgb', 'raw_score_lgbm'\n",
    "                          cvap_loss='log', # 'log' or 'brier' for aggregation\n",
    "                          cvap_precision=None, # Precision for rounding scores in VA fit\n",
    "                          # Platt/Isotonic specific params (CalibratedClassifierCV handles score method)\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Trains a base estimator and calibrates it, or loads a pre-trained/calibrated predictor.\n",
    "\n",
    "    Saves/Loads the single fitted predictor object (CalibratedClassifierCV or CVAPPredictorRaw)\n",
    "    into a file specific to the calibration method.\n",
    "\n",
    "    Args:\n",
    "        base_estimator_class: Class of the base estimator (e.g., SVC, RandomForestClassifier).\n",
    "        best_params (dict): Dictionary of best hyperparameters for the base estimator.\n",
    "        X_train (pd.DataFrame or np.ndarray): Training features.\n",
    "        y_train (pd.Series or np.ndarray): Training labels (binary 0/1).\n",
    "        model_name (str): Name of the model (e.g., 'SVM').\n",
    "        group_name (str): Name of the data group (e.g., 'group_1').\n",
    "        MODEL_DIR (str): Directory to save/load models.\n",
    "        calibration_method (str): 'platt', 'isotonic', or 'cvap'.\n",
    "        n_splits (int): Number of folds for cross-validation.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "        score_method (str): Method for CVAP scores ('decision_function', 'predict_proba', etc.).\n",
    "        cvap_loss (str): Aggregation loss for CVAP ('log' or 'brier').\n",
    "        cvap_precision (int, optional): Precision for CVAP VennAbers fit.\n",
    "\n",
    "    Returns:\n",
    "        object or None:\n",
    "            - Fitted predictor object with a `predict_proba` method\n",
    "              (either CalibratedClassifierCV or CVAPPredictorRaw).\n",
    "            - Returns None if an error occurs during training/loading or if loading fails validation.\n",
    "    \"\"\"\n",
    "    logging.info(f\"--- Preparing Model Training & Calibration ({calibration_method}) for {model_name} ({group_name}) ---\")\n",
    "\n",
    "    # --- Check for existing saved predictor first ---\n",
    "    save_path = _get_save_path(MODEL_DIR, model_name, group_name, calibration_method)\n",
    "    loaded_predictor = load_predictor(save_path, model_name)\n",
    "    if loaded_predictor is not None:\n",
    "        logging.info(f\"--- Using pre-trained and calibrated predictor from {save_path} ---\")\n",
    "        return loaded_predictor\n",
    "    # If loaded_predictor is None, load_predictor already logged the reason (not found or invalid)\n",
    "\n",
    "    # --- Proceed with training if predictor not loaded ---\n",
    "    logging.info(f\"--- Starting Model Training & Calibration ({calibration_method}) ---\")\n",
    "\n",
    "    # Input Type Handling (same as before)\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        X_train_np = X_train.values\n",
    "    else:\n",
    "        X_train_np = np.asarray(X_train)\n",
    "    if isinstance(y_train, pd.Series):\n",
    "        y_train_np = y_train.values\n",
    "    else:\n",
    "        y_train_np = np.asarray(y_train)\n",
    "    if len(np.unique(y_train_np)) != 2:\n",
    "        logging.error(f\"This function currently supports only binary classification. Found labels: {np.unique(y_train_np)}\")\n",
    "        raise ValueError(f\"Binary classification required. Found labels: {np.unique(y_train_np)}\")\n",
    "\n",
    "    # Instantiate the base estimator (same as before, including SVC probability logic)\n",
    "    try:\n",
    "        current_params = best_params.copy()\n",
    "        is_svc = issubclass(base_estimator_class, SVC)\n",
    "\n",
    "        needs_proba = False\n",
    "        # Check if base estimator *needs* predict_proba for the chosen calibration approach\n",
    "        if calibration_method in ['platt', 'isotonic']:\n",
    "            # CalibratedClassifierCV can use decision_function OR predict_proba.\n",
    "            # It automatically uses decision_function if available, unless method='isotonic'\n",
    "            # AND the base estimator only has predict_proba.\n",
    "            # Forcing probability=True for SVC with Platt is common practice, though not strictly required by CCCV if decision_function exists.\n",
    "            # Let's keep the original logic for SVC to be safe.\n",
    "            if is_svc: needs_proba = True\n",
    "        elif calibration_method == 'cvap' and score_method == 'predict_proba':\n",
    "             needs_proba = True # Only if base estimator needs probability for predict_proba\n",
    "\n",
    "        if needs_proba and is_svc and not current_params.get('probability', False):\n",
    "             logging.warning(f\"Setting probability=True for SVC as required by calibration setup.\")\n",
    "             current_params['probability'] = True\n",
    "        elif needs_proba and not hasattr(base_estimator_class(**current_params), 'predict_proba'):\n",
    "             # If we absolutely need predict_proba (e.g., CVAP with score_method='predict_proba')\n",
    "             # and the estimator doesn't have it, it's an issue.\n",
    "             logging.error(f\"Configuration requires 'predict_proba' (score_method='{score_method}'), \"\n",
    "                            f\"but {base_estimator_class.__name__} with params {current_params} might not support it.\")\n",
    "             # Decide whether to raise an error or just warn\n",
    "             # Raising an error is safer:\n",
    "             raise AttributeError(f\"{base_estimator_class.__name__} does not support 'predict_proba' needed for this configuration.\")\n",
    "             # Alternatively, warn and proceed, hoping get_scores handles it (though it will likely fail there):\n",
    "             # logging.warning(f\"Score method '{score_method}' requires 'predict_proba', but {base_estimator_class.__name__} might not support it with current params. Proceeding, but check estimator capabilities.\")\n",
    "\n",
    "        base_estimator = base_estimator_class(**current_params)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error instantiating base estimator {base_estimator_class.__name__} with params {current_params}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "    # --- Calibration Method Logic ---\n",
    "    final_predictor = None # This will hold the object to be returned/saved\n",
    "\n",
    "    try:\n",
    "        if calibration_method in ['platt', 'isotonic']:\n",
    "            logging.info(f\"Using CalibratedClassifierCV with method='{'sigmoid' if calibration_method == 'platt' else 'isotonic'}'\")\n",
    "\n",
    "            cv_strategy = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "            # Use the instantiated base_estimator. CalibratedClassifierCV clones it internally for CV.\n",
    "            # It then refits a final base estimator on all data.\n",
    "            calibrator = CalibratedClassifierCV(\n",
    "                base_estimator, # Pass the configured instance\n",
    "                method='sigmoid' if calibration_method == 'platt' else 'isotonic',\n",
    "                cv=cv_strategy,\n",
    "                n_jobs=-1,\n",
    "                #!ensemble=False # Default, ensures one final model refit on all data\n",
    "            )\n",
    "\n",
    "            logging.info(\"Fitting CalibratedClassifierCV...\")\n",
    "            calibrator.fit(X_train_np, y_train_np)\n",
    "            # No need to set fitted_ manually, sklearn does this.\n",
    "            logging.info(\"CalibratedClassifierCV fitting complete.\")\n",
    "\n",
    "            # The 'calibrator' object IS the final predictor we want.\n",
    "            final_predictor = calibrator\n",
    "\n",
    "            logging.info(f\"--- {calibration_method.capitalize()} Calibration Training Complete ---\")\n",
    "            # Save the single CalibratedClassifierCV object\n",
    "            save_predictor(final_predictor, save_path, model_name)\n",
    "            return final_predictor\n",
    "\n",
    "\n",
    "        elif calibration_method == 'cvap':\n",
    "            logging.info(f\"Using Cross Venn-Abers Prediction (CVAP) with score_method='{score_method}'\")\n",
    "\n",
    "            cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "            oof_scores_list, oof_y_cal_list = [], []\n",
    "            fold_estimators = [] # Store fold estimators if needed later, otherwise remove\n",
    "\n",
    "            logging.info(\"Generating out-of-fold scores for CVAP...\")\n",
    "            for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_np, y_train_np)):\n",
    "                logging.debug(f\"Fitting fold {fold+1}/{n_splits}\")\n",
    "                # Clone the original base estimator config for each fold\n",
    "                est = clone(base_estimator).fit(X_train_np[train_idx], y_train_np[train_idx])\n",
    "                scores = get_scores(est, X_train_np[val_idx], score_method)\n",
    "                oof_scores_list.append(scores)\n",
    "                oof_y_cal_list.append(y_train_np[val_idx])\n",
    "                fold_estimators.append(est) # Optional: keep if needed elsewhere\n",
    "            logging.info(\"Out-of-fold score generation complete.\")\n",
    "\n",
    "            logging.info(\"Fitting final base model on all data...\")\n",
    "            # Clone again to ensure a fresh fit on all data\n",
    "            final_base_estimator = clone(base_estimator).fit(X_train_np, y_train_np)\n",
    "            logging.info(\"Final base model fitting complete.\")\n",
    "\n",
    "            logging.info(\"Fitting VennAbers calibrators for each fold...\")\n",
    "            calibrators = [\n",
    "                RawVennAbers(precision=cvap_precision).fit(scores, y_cal)\n",
    "                for scores, y_cal in zip(oof_scores_list, oof_y_cal_list)\n",
    "            ]\n",
    "            logging.info(\"VennAbers fitting complete.\")\n",
    "\n",
    "            # Wrap them into CVAPPredictorRaw\n",
    "            cvap_predictor = CVAPPredictorRaw(\n",
    "                final_estimator_=final_base_estimator,\n",
    "                calibrators_=calibrators,\n",
    "                loss_=cvap_loss,\n",
    "                score_method_=score_method\n",
    "                # Note: CVAPPredictorRaw should internally set fitted_=True or provide predict_proba\n",
    "            )\n",
    "            # Manually set fitted_ flag if CVAPPredictorRaw doesn't do it\n",
    "            if not hasattr(cvap_predictor, 'fitted_'):\n",
    "                cvap_predictor.fitted_ = True\n",
    "\n",
    "            final_predictor = cvap_predictor\n",
    "            logging.info(\"--- CVAP Training Complete ---\")\n",
    "\n",
    "            # Save the single CVAPPredictorRaw object\n",
    "            save_predictor(final_predictor, save_path, model_name)\n",
    "            return final_predictor\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown calibration_method: {calibration_method}. Choose 'platt', 'isotonic', or 'cvap'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during {calibration_method} calibration for {model_name}: {e}\", exc_info=True)\n",
    "        return None # Return None on any error during training/calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca834ca",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9538c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Comprehensive Metrics ---\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_proba, model_name=\"Model\", conf_mat = False):\n",
    "    \"\"\"\n",
    "    Calculates a comprehensive set of classification metrics.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Ground truth labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        y_proba (array-like): Predicted probabilities for the positive class (class 1).\n",
    "        model_name (str): Name of the model for logging.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing calculated metrics.\n",
    "              Returns None if input arrays are empty or invalid.\n",
    "    \"\"\"\n",
    "    if len(y_true) == 0 or len(y_pred) == 0 or len(y_proba) == 0:\n",
    "        logging.error(f\"[{model_name}] Empty input arrays provided for metric calculation.\")\n",
    "        return None\n",
    "    if len(y_true) != len(y_pred) or len(y_true) != len(y_proba):\n",
    "        logging.error(f\"[{model_name}] Mismatched lengths in input arrays for metric calculation.\")\n",
    "        return None\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # --- Threshold-based Metrics (using y_pred) ---\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['precision'] = precision\n",
    "    metrics['recall_tpr'] = recall # True Positive Rate (Sensitivity)\n",
    "    metrics['f1_score'] = f1\n",
    "\n",
    "    # Specificity (True Negative Rate)\n",
    "    metrics['specificity_tnr'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "    # Geometric Mean\n",
    "    metrics['g_mean'] = np.sqrt(metrics['recall_tpr'] * metrics['specificity_tnr'])\n",
    "\n",
    "    # Confusion Matrix\n",
    "    metrics['confusion_matrix'] = {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp}\n",
    "\n",
    "    # --- Ranking/Probabilistic Metrics (using y_proba) ---\n",
    "    try:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_true, y_proba)\n",
    "    except ValueError as e:\n",
    "        logging.warning(f\"[{model_name}] Could not calculate ROC AUC: {e}. Setting to 0.0.\")\n",
    "        metrics['roc_auc'] = 0.0 # Handle cases with only one class present\n",
    "\n",
    "    # PR AUC\n",
    "    pr_curve_precision, pr_curve_recall, _ = precision_recall_curve(y_true, y_proba)\n",
    "    metrics['pr_auc'] = auc(pr_curve_recall, pr_curve_precision) # Note order: recall is x, precision is y\n",
    "\n",
    "    # Brier Score\n",
    "    metrics['brier_score'] = brier_score_loss(y_true, y_proba)\n",
    "\n",
    "    logging.info(f\"--- {model_name} Metrics ---\")\n",
    "    logging.info(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    logging.info(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    logging.info(f\"Recall (TPR): {metrics['recall_tpr']:.4f}\")\n",
    "    logging.info(f\"Specificity (TNR): {metrics['specificity_tnr']:.4f}\")\n",
    "    logging.info(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    logging.info(f\"G-Mean: {metrics['g_mean']:.4f}\")\n",
    "    logging.info(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    logging.info(f\"PR AUC: {metrics['pr_auc']:.4f}\")\n",
    "    logging.info(f\"Brier Score: {metrics['brier_score']:.4f}\")\n",
    "    logging.info(f\"Confusion Matrix (TN, FP, FN, TP): ({tn}, {fp}, {fn}, {tp})\")\n",
    "\n",
    "    # Optional: Plot Confusion Matrix\n",
    "    if conf_mat:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap([[tn, fp], [fn, tp]], annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['Predicted Galaxy (0)', 'Predicted Star (1)'],\n",
    "                    yticklabels=['Actual Galaxy (0)', 'Actual Star (1)'])\n",
    "        plt.title(f'{model_name} Confusion Matrix')\n",
    "        plt.ylabel('Actual Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        cm_filename = os.path.join(MODEL_DIR, f\"{model_name}_confusion_matrix_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\")\n",
    "        plt.savefig(cm_filename)\n",
    "        plt.close()\n",
    "        logging.info(f\"Confusion matrix plot saved to {cm_filename}\")\n",
    "\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf85e1b",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68e877b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Scaling ---\n",
    "# Important for SVM, not used for the other models.\n",
    "# Fit scaler ONLY on training data, then transform all sets.\n",
    "\n",
    "# Check if training set and other datasets are non-empty before scaling\n",
    "def apply_feature_scaling(\n",
    "    X_train, X_val, X_test, X_cal,\n",
    "    TRAIN_SIZE, VAL_SIZE, TEST_SIZE, CAL_SIZE,\n",
    "    MODEL_DIR,\n",
    "    save_scaler=True,  # New param: whether to save the scaler to disk\n",
    "    group_name=None    # New param: optional, for unique scaler filename per group\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies StandardScaler to the provided datasets if training data is available.\n",
    "    Optionally saves the fitted scaler to disk.\n",
    "    If group_name is provided, attempts to load the scaler from disk.\n",
    "    Returns: X_train_scaled, X_val_scaled, X_test_scaled, X_cal_scaled, scaler (or None)\n",
    "    \"\"\"\n",
    "\n",
    "    scaler = None\n",
    "    scaler_loaded = False\n",
    "\n",
    "    # If group_name is provided, try to load the scaler from disk\n",
    "    if group_name is not None:\n",
    "        scaler_filename = os.path.join(MODEL_DIR, f\"scaler_{group_name}.joblib\")\n",
    "        if os.path.exists(scaler_filename):\n",
    "            scaler = joblib.load(scaler_filename)\n",
    "            scaler_loaded = True\n",
    "            logging.info(f\"Scaler loaded from {scaler_filename}\")\n",
    "        else:\n",
    "            logging.info(f\"No existing scaler found for group '{group_name}', will fit a new one if possible.\")\n",
    "\n",
    "    # If scaler was not loaded, fit a new one if possible\n",
    "    if not scaler_loaded:\n",
    "        if len(X_train) > 0 and TRAIN_SIZE > 0:\n",
    "            logging.info(\"Applying StandardScaler to features...\")\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "        else:\n",
    "            logging.info(\"Empty training set, NOT able to apply StandardScaler!\")\n",
    "            X_train_scaled = X_train\n",
    "    else:\n",
    "        # If scaler was loaded, just transform\n",
    "        if len(X_train) > 0 and TRAIN_SIZE > 0:\n",
    "            X_train_scaled = scaler.transform(X_train)\n",
    "        else:\n",
    "            X_train_scaled = X_train\n",
    "\n",
    "    if len(X_val) > 0 and VAL_SIZE > 0 and scaler is not None:\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "    else:\n",
    "        X_val_scaled = X_val\n",
    "\n",
    "    if len(X_test) > 0 and TEST_SIZE > 0 and scaler is not None:\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_test_scaled = X_test\n",
    "\n",
    "    if len(X_cal) > 0 and CAL_SIZE > 0 and scaler is not None:\n",
    "        X_cal_scaled = scaler.transform(X_cal)\n",
    "    else:\n",
    "        X_cal_scaled = X_cal\n",
    "\n",
    "    # Save the scaler if it was fitted and requested, and not already loaded\n",
    "    if scaler is not None and save_scaler and not scaler_loaded:\n",
    "        if group_name is not None:\n",
    "            scaler_filename = os.path.join(\n",
    "                MODEL_DIR, f\"scaler_{group_name}.joblib\"\n",
    "            )\n",
    "        else:\n",
    "            scaler_filename = os.path.join(\n",
    "                MODEL_DIR, f\"scaler_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib\"\n",
    "            )\n",
    "        joblib.dump(scaler, scaler_filename)\n",
    "        logging.info(f\"Scaler saved to {scaler_filename}\")\n",
    "\n",
    "    logging.info(\"Feature scaling complete.\")\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, X_cal_scaled, scaler\n",
    "\n",
    "# Example usage:\n",
    "# X_train_scaled, X_val_scaled, X_test_scaled, X_cal_scaled, scaler = apply_feature_scaling(\n",
    "#     X_train, X_val, X_test, X_cal, TRAIN_SIZE, VAL_SIZE, TEST_SIZE, CAL_SIZE, MODEL_DIR,\n",
    "#     save_scaler=True, # Ensure scaler is saved\n",
    "#     group_name=group_name # Pass group name for potentially unique scaler filename\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfa8f49",
   "metadata": {},
   "source": [
    "## 4. Model Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa3665cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {} # Dictionary to store metrics for each model\n",
    "\n",
    "CALIBRATOR = 'cvap' #'cvap' 'platt' 'isotonic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca2d2ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for SVM (using data fraction)\n",
    "MAX_RESOURCE_SVM = 1.0  # Max data fraction\n",
    "MIN_RESOURCE_SVM = 0.1  # Min data fraction (adjust based on minority class size)\n",
    "ETA_SVM = 3\n",
    "RESOURCE_TYPE_SVM = 'data_fraction'\n",
    "model_name_svm = \"SVM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b95e2f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for CART (using data fraction)\n",
    "MAX_RESOURCE_CART = 1.0\n",
    "MIN_RESOURCE_CART = 0.1 # Can start with smaller fraction for trees\n",
    "ETA_CART = 3\n",
    "RESOURCE_TYPE_CART = 'data_fraction'\n",
    "model_name_cart = \"CART\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6971c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for RF (using iterations)\n",
    "MAX_RESOURCE_RF = 300  # Max n_estimators\n",
    "MIN_RESOURCE_RF = 20   # Min n_estimators\n",
    "ETA_RF = 3\n",
    "RESOURCE_TYPE_RF = 'iterations'\n",
    "model_name_rf = \"Random_Forest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f957357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for XGB (using iterations)\n",
    "MAX_RESOURCE_XGB = 500 # Max n_estimators\n",
    "MIN_RESOURCE_XGB = 30  # Min n_estimators\n",
    "ETA_XGB = 3\n",
    "RESOURCE_TYPE_XGB = 'iterations'\n",
    "model_name_xgb = \"XGBoost\"\n",
    "ROUNDS = 20        # Number of rounds to wait for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93f0744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for LGBM (using iterations)\n",
    "MAX_RESOURCE_LGBM = 500 # Max n_estimators\n",
    "MIN_RESOURCE_LGBM = 30  # Min n_estimators\n",
    "ETA_LGBM = 3\n",
    "RESOURCE_TYPE_LGBM = 'iterations'\n",
    "model_name_lgbm = \"LightGBM\"\n",
    "ROUNDS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7859c",
   "metadata": {},
   "source": [
    "### 3.1 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18317c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_hpo(\n",
    "    X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "    MAX_RESOURCE_SVM,\n",
    "    MIN_RESOURCE_SVM,\n",
    "    ETA_SVM,\n",
    "    RESOURCE_TYPE_SVM,\n",
    "    RANDOM_SEED,\n",
    "    SVC,\n",
    "    loguniform,\n",
    "    hyperband_hpo,\n",
    "    f1_score,\n",
    "    logging,\n",
    "    model_name_svm\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Hyperband HPO for SVM and return best parameters and best score.\n",
    "    \"\"\"\n",
    "    # --- SVM: Define Search Space and HPO Params ---\n",
    "    param_space_svm = {\n",
    "        #   Covers 10  10  ( 2  2)  \n",
    "        \"C\":     loguniform(1e-3, 1e3),\n",
    "\n",
    "        #   Covers 10  10  ( 2  2)  \n",
    "        \"gamma\": loguniform(1e-6, 1e1),\n",
    "\n",
    "        #    discrete choices \n",
    "        \"kernel\": [\"rbf\", \"linear\", \"poly\"],\n",
    "\n",
    "        #   Only used when kernel=\"poly\"\n",
    "        \"degree\": randint(2, 6),       # {2,3,4,5}\n",
    "\n",
    "        #   Only used for \"poly\" and \"sigmoid\"\n",
    "        \"coef0\": uniform(-1.0, 3.0),   # [-1,2]\n",
    "    }\n",
    "\n",
    "    logging.info(f\"--- [{model_name_svm}] Running Hyperband HPO ---\")\n",
    "    best_params_svm, best_score_hpo_svm = hyperband_hpo(\n",
    "        model_class=SVC,\n",
    "        param_space=param_space_svm,\n",
    "        X_train=X_train_scaled, # USE SCALED DATA\n",
    "        y_train=y_train,\n",
    "        X_val=X_val_scaled,     # USE SCALED DATA\n",
    "        y_val=y_val,\n",
    "        max_resource=MAX_RESOURCE_SVM,\n",
    "        eta=ETA_SVM,\n",
    "        resource_type=RESOURCE_TYPE_SVM,\n",
    "        min_resource=MIN_RESOURCE_SVM,\n",
    "        scoring_func=f1_score,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    return best_params_svm, best_score_hpo_svm\n",
    "\n",
    "def svm_workflow(\n",
    "    X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test,\n",
    "    MAX_RESOURCE_SVM, MIN_RESOURCE_SVM, ETA_SVM, RESOURCE_TYPE_SVM,\n",
    "    model_name_svm, CALIBRATOR,  RANDOM_SEED,\n",
    "    SVC=SVC,\n",
    "    loguniform=loguniform,\n",
    "    hyperband_hpo=hyperband_hpo,\n",
    "    f1_score=f1_score,\n",
    "    logging=logging,\n",
    "    best_params_svm=None,  # <-- Allow passing best_params directly\n",
    "    best_score_hpo_svm=None,\n",
    "    group = \"\",\n",
    "    model_dir = \"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete SVM workflow: HPO, calibration and evaluation.\n",
    "\n",
    "    This function performs the following steps for SVM:\n",
    "      1. Defines the hyperparameter search space.\n",
    "      2. Runs Hyperband HPO using the provided data splits.\n",
    "      3. Trains the final SVM model and calibrates it (Platt/Isotonic/CVAP).\n",
    "      4. Stores results in the provided all_results dictionary.\n",
    "\n",
    "    All logic is preserved from the original notebook cell.\n",
    "\n",
    "    Args:\n",
    "        X_train_scaled, y_train: Training data (scaled, labels).\n",
    "        X_val_scaled, y_val: Validation data (scaled, labels).\n",
    "        X_cal_scaled, y_cal: Calibration data (scaled, labels).\n",
    "        X_test_scaled, y_test: Test data (scaled, labels).\n",
    "        \n",
    "        MAX_RESOURCE_SVM, MIN_RESOURCE_SVM, ETA_SVM, RESOURCE_TYPE_SVM: HPO settings.\n",
    "        model_name_svm: Name for the model (e.g., \"SVM\").\n",
    "        CALIBRATOR: Calibration method ('platt', 'isotonic', 'cvap').\n",
    "        ALPHA: Conformal prediction significance level.\n",
    "        RANDOM_SEED: Random seed for reproducibility.\n",
    "        best_params_svm: (Optional) If provided, skips HPO and uses these params\n",
    "        best_score_hpo_svm: (Optional) If provided, uses this as HPO score\n",
    "\n",
    "    Returns:\n",
    "        dict or None: Dictionary containing results for this model, or None if HPO fails.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    logging.info(f\"\\n\\n===== Starting Workflow for {model_name_svm} ({group}) =====\")\n",
    "    timestamp_svm = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    all_results = {} # Initialize results dict for this run\n",
    "\n",
    "    # --- HPO Step ---\n",
    "    if best_params_svm is None:\n",
    "        hpo_start_time_svm = time.time()\n",
    "        best_params_svm, best_score_hpo_svm = svm_hpo(\n",
    "            X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "            MAX_RESOURCE_SVM,\n",
    "            MIN_RESOURCE_SVM,\n",
    "            ETA_SVM,\n",
    "            RESOURCE_TYPE_SVM,\n",
    "            RANDOM_SEED,\n",
    "            SVC,\n",
    "            loguniform,\n",
    "            hyperband_hpo,\n",
    "            f1_score,\n",
    "            logging,\n",
    "            model_name_svm\n",
    "        )\n",
    "        hpo_duration_svm = time.time() - hpo_start_time_svm\n",
    "        logging.info(f\"--- [{model_name_svm}] HPO finished in {hpo_duration_svm:.2f} seconds ---\")\n",
    "    else:\n",
    "        hpo_duration_svm = 0.0\n",
    "        logging.info(f\"--- [{model_name_svm}] Using provided hyperparameters, skipping HPO. ---\")\n",
    "\n",
    "    # --- Training & Calibration Step ---\n",
    "    final_svm_predictor = None # Initialize predictor variable\n",
    "    calibration_duration_svm = 0.0\n",
    "    if best_params_svm:\n",
    "        logging.info(f\"--- [{model_name_svm}] Training final model & calibrator ({CALIBRATOR}) ---\")\n",
    "        calibration_start_time_svm = time.time()\n",
    "\n",
    "        # Ensure necessary fixed parameters are present for the final fit\n",
    "        best_params_svm['random_state'] = RANDOM_SEED\n",
    "        if 'class_weight' not in best_params_svm: best_params_svm['class_weight'] = 'balanced'\n",
    "        if 'probability' in best_params_svm: del best_params_svm['probability'] # Use decision_function\n",
    "\n",
    "\n",
    "        final_svm_predictor = train_calibrate_model(\n",
    "            base_estimator_class=SVC,          # Pass the class\n",
    "            best_params=best_params_svm,  # Pass the potentially modified HPO params\n",
    "            X_train=X_train_scaled,            # Use scaled training data\n",
    "            y_train=y_train,                   # Use original y_train for CV indexing\n",
    "            model_name = model_name_svm,\n",
    "            group_name = group,\n",
    "            MODEL_DIR = model_dir,\n",
    "            calibration_method=CALIBRATOR,     # Use chosen calibration method\n",
    "            n_splits=5,                        # Folds\n",
    "            random_state=RANDOM_SEED,\n",
    "            score_method='decision_function', # SVC supports decision_function\n",
    "            cvap_loss='log',          # Use log-loss aggregation for CVAP\n",
    "            cvap_precision=None       # Default precision\n",
    "        )\n",
    "        calibration_duration_svm = time.time() - calibration_start_time_svm\n",
    "        if final_svm_predictor:\n",
    "            logging.info(f\"--- [{model_name_svm}] Training & Calibration with {CALIBRATOR} finished in {calibration_duration_svm:.2f} seconds ---\")\n",
    "        else:\n",
    "            logging.error(f\"[{model_name_svm}] Failed to train or calibrate the model.\")\n",
    "            logging.info(f\"===== Aborting Workflow for {model_name_svm} due to training/calibration failure =====\")\n",
    "            return None # Stop the workflow here\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_svm}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "        logging.info(f\"===== Aborting Workflow for {model_name_svm} due to HPO failure =====\")\n",
    "        return None # Stop the workflow here\n",
    "\n",
    "    # --- Final Evaluation Step ---\n",
    "    metrics_svm = None\n",
    "    eval_duration_svm = 0.0\n",
    "\n",
    "    # Proceed only if the final predictor exists\n",
    "    if final_svm_predictor:\n",
    "        logging.info(f\"--- [{model_name_svm}] Final Evaluation on Test Set ---\")\n",
    "        eval_start_time_svm = time.time()\n",
    "\n",
    "        try:\n",
    "            # --- Calculate Base Metrics ---\n",
    "            probs_test_svm_full = final_svm_predictor.predict_proba(X_test_scaled) # (n_test, 2)\n",
    "\n",
    "            # Derive probabilities and predictions from the calibrated output\n",
    "            y_proba_test_svm = probs_test_svm_full[:, 1] # Prob positive class\n",
    "            y_pred_test_svm = (y_proba_test_svm >= 0.5).astype(int) # Threshold calibrated probs\n",
    "            metrics_svm = calculate_metrics(y_test, y_pred_test_svm, y_proba_test_svm, model_name=model_name_svm)\n",
    "\n",
    "            eval_duration_svm = time.time() - eval_start_time_svm # Total eval time\n",
    "            logging.info(f\"--- [{model_name_svm}] Total Evaluation finished in {eval_duration_svm:.2f} seconds ---\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{model_name_svm}] Error during final evaluation: {e}\", exc_info=True)\n",
    "            # Reset metrics if evaluation failed\n",
    "            metrics_svm = None\n",
    "\n",
    "    # --- Store results ---\n",
    "    # Store results even if some parts failed (values will be None)\n",
    "    all_results[model_name_svm] = {\n",
    "        'metrics': metrics_svm,\n",
    "        'best_hpo_params': best_params_svm,\n",
    "        'hpo_f1_score': best_score_hpo_svm,\n",
    "        'hpo_duration_s': hpo_duration_svm,\n",
    "        'calibration_duration_s': calibration_duration_svm,\n",
    "        'evaluation_duration_s': eval_duration_svm,\n",
    "        'timestamp': timestamp_svm,\n",
    "        'group': group,\n",
    "        'calibration_method': CALIBRATOR\n",
    "    }\n",
    "\n",
    "    logging.info(f\"===== Finished Workflow for {model_name_svm} ({group}) =====\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc764a22",
   "metadata": {},
   "source": [
    "### 3.2 CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart_hpo(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    MAX_RESOURCE_CART,\n",
    "    MIN_RESOURCE_CART,\n",
    "    ETA_CART,\n",
    "    RESOURCE_TYPE_CART,\n",
    "    RANDOM_SEED,\n",
    "    DecisionTreeClassifier,\n",
    "    randint,\n",
    "    f1_score,\n",
    "    hyperband_hpo,\n",
    "    logging,\n",
    "    model_name_cart\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Hyperband HPO for CART (DecisionTreeClassifier) and return best parameters and best score.\n",
    "    \"\"\"\n",
    "    # --- CART: Define Search Space and HPO Params ---\n",
    "    param_space_cart = {\n",
    "        # qualitative choices\n",
    "        \"criterion\": [\"gini\", \"entropy\"],     # impurity / log-loss\n",
    "        \"splitter\":  [\"best\", \"random\"],                 # deterministic vs. stochastic\n",
    "\n",
    "        # tree size & shape (integers are *inclusive*: randint(a,b+1) in SciPy)\n",
    "        \"max_depth\":          randint(5, 80),            \n",
    "        \"min_samples_split\":  randint(2, 51),            \n",
    "        \"min_samples_leaf\":   randint(1, 30),            \n",
    "\n",
    "        # subsampling & pruning\n",
    "        \"max_features\": [\"sqrt\", \"log2\", None],          # -, p, logp\n",
    "        \"ccp_alpha\":    loguniform(1e-6, 1e-1),          # 10  10\n",
    "    }\n",
    "\n",
    "    logging.info(f\"--- [{model_name_cart}] Running Hyperband HPO ---\")\n",
    "    best_params_cart, best_score_hpo_cart = hyperband_hpo(\n",
    "        model_class=DecisionTreeClassifier,\n",
    "        param_space=param_space_cart,\n",
    "        X_train=X_train, # USE UNSCALED DATA\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,     # USE UNSCALED DATA\n",
    "        y_val=y_val,\n",
    "        max_resource=MAX_RESOURCE_CART,\n",
    "        eta=ETA_CART,\n",
    "        resource_type=RESOURCE_TYPE_CART,\n",
    "        min_resource=MIN_RESOURCE_CART,\n",
    "        scoring_func=f1_score,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    return best_params_cart, best_score_hpo_cart\n",
    "\n",
    "def cart_workflow(\n",
    "    X_train, y_train, X_val, y_val, X_cal, y_cal, X_test, y_test,\n",
    "    model_name_cart,\n",
    "    MAX_RESOURCE_CART,\n",
    "    MIN_RESOURCE_CART,\n",
    "    ETA_CART,\n",
    "    RESOURCE_TYPE_CART,\n",
    "    RANDOM_SEED,\n",
    "    CALIBRATOR,\n",
    "    \n",
    "    DecisionTreeClassifier,\n",
    "    randint,\n",
    "    f1_score,\n",
    "    hyperband_hpo,\n",
    "    logging,\n",
    "    best_params_cart=None,  # <-- Allow passing best_params directly\n",
    "    best_score_hpo_cart=None,\n",
    "    group = \"\",\n",
    "    model_dir = \"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete workflow for CART (DecisionTreeClassifier) model:\n",
    "    - Hyperparameter optimization using Hyperband\n",
    "    - Final model training and calibration\n",
    "    - Results are stored in the returned dictionary\n",
    "\n",
    "    Args:\n",
    "        X_train, y_train: Training data (unscaled)\n",
    "        X_val, y_val: Validation data (unscaled)\n",
    "        X_cal, y_cal: Calibration data (unscaled)\n",
    "        X_test, y_test: Test data (unscaled)\n",
    "        model_name_cart: str, name for the model (e.g. \"CART\")\n",
    "        MAX_RESOURCE_CART, MIN_RESOURCE_CART, ETA_CART, RESOURCE_TYPE_CART: HPO settings\n",
    "        RANDOM_SEED: int, random seed for reproducibility\n",
    "        CALIBRATOR: str, calibration method ('cvap', 'platt', etc.)\n",
    "        ALPHA: float, significance level for conformal prediction\n",
    "\n",
    "    Returns:\n",
    "        dict or None: Dictionary containing results for this model, or None if HPO/training fails.\n",
    "    \"\"\"\n",
    "    logging.info(f\"\\n\\n===== Starting Workflow for {model_name_cart} ({group}) =====\")\n",
    "    timestamp_cart = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    all_results = {}\n",
    "\n",
    "    # --- HPO Step ---\n",
    "    if best_params_cart is None:\n",
    "        hpo_start_time_cart = time.time()\n",
    "        best_params_cart, best_score_hpo_cart = cart_hpo(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            MAX_RESOURCE_CART,\n",
    "            MIN_RESOURCE_CART,\n",
    "            ETA_CART,\n",
    "            RESOURCE_TYPE_CART,\n",
    "            RANDOM_SEED,\n",
    "            DecisionTreeClassifier,\n",
    "            randint,\n",
    "            f1_score,\n",
    "            hyperband_hpo,\n",
    "            logging,\n",
    "            model_name_cart\n",
    "        )\n",
    "        hpo_duration_cart = time.time() - hpo_start_time_cart\n",
    "        logging.info(f\"--- [{model_name_cart}] HPO finished in {hpo_duration_cart:.2f} seconds ---\")\n",
    "    else:\n",
    "        hpo_duration_cart = 0.0\n",
    "        logging.info(f\"--- [{model_name_cart}] Using provided hyperparameters, skipping HPO. ---\")\n",
    "\n",
    "    # --- Training & Calibration Step ---\n",
    "    final_cart_predictor = None\n",
    "    calibration_duration_cart = 0.0\n",
    "    if best_params_cart:\n",
    "        logging.info(f\"--- [{model_name_cart}] Training final model & calibrator ({CALIBRATOR}) ---\")\n",
    "        calibration_start_time_cart = time.time()\n",
    "        # Ensure necessary fixed parameters are present for the final fit\n",
    "        best_params_cart['random_state'] = RANDOM_SEED\n",
    "        if 'class_weight' not in best_params_cart: best_params_cart['class_weight'] = 'balanced'\n",
    "\n",
    "        # Only a single object is returned (the calibrated predictor)\n",
    "        final_cart_predictor = train_calibrate_model(\n",
    "            base_estimator_class=DecisionTreeClassifier,\n",
    "            best_params=best_params_cart,\n",
    "            X_train=X_train,  # Use UNSCALED training data\n",
    "            y_train=y_train,\n",
    "            model_name = model_name_cart,\n",
    "            group_name = group,\n",
    "            MODEL_DIR = model_dir,\n",
    "            calibration_method=CALIBRATOR,  # Use chosen calibration method (e.g., 'cvap', 'platt', etc.)\n",
    "            n_splits=5,\n",
    "            random_state=RANDOM_SEED,\n",
    "            score_method='predict_proba',  # For CART, use predict_proba\n",
    "            cvap_loss='brier',  # or 'brier' if desired\n",
    "            cvap_precision=None  # or set as needed\n",
    "        )\n",
    "        calibration_duration_cart = time.time() - calibration_start_time_cart\n",
    "        if final_cart_predictor:\n",
    "            logging.info(f\"--- [{model_name_cart}] Training & Calibration with {CALIBRATOR} finished in {calibration_duration_cart:.2f} seconds ---\")\n",
    "        else:\n",
    "            logging.error(f\"[{model_name_cart}] Failed to train or calibrate the model.\")\n",
    "            logging.info(f\"===== Aborting Workflow for {model_name_cart} due to training/calibration failure =====\")\n",
    "            return None\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_cart}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "        return None\n",
    "\n",
    "    # --- Final Evaluation Step ---\n",
    "    metrics_cart = None\n",
    "    eval_duration_cart = 0.0\n",
    "\n",
    "    if final_cart_predictor:\n",
    "        logging.info(f\"--- [{model_name_cart}] Final Evaluation on Test Set ---\")\n",
    "        eval_start_time_cart = time.time()\n",
    "        try:\n",
    "            # Get calibrated probabilities from the predictor\n",
    "            calibrated_probs_test_cart = final_cart_predictor.predict_proba(X_test)\n",
    "            y_proba_test_cart = calibrated_probs_test_cart[:, 1]  # Probability of positive class\n",
    "            logging.info(f\"[{model_name_cart}] Calibrated test probabilities for class 1 (sample): {y_proba_test_cart[:20]}\")\n",
    "            logging.info(f\"[{model_name_cart}] Calibrated test probabilities range: min={np.min(y_proba_test_cart):.4f}, max={np.max(y_proba_test_cart):.4f}, mean={np.mean(y_proba_test_cart):.4f}\")\n",
    "            y_pred_test_cart = (y_proba_test_cart >= 0.5).astype(int)  # Threshold calibrated probabilities\n",
    "\n",
    "            metrics_cart = calculate_metrics(y_test, y_pred_test_cart, y_proba_test_cart, model_name=model_name_cart)\n",
    "\n",
    "            eval_duration_cart = time.time() - eval_start_time_cart\n",
    "            logging.info(f\"--- [{model_name_cart}] Total Evaluation finished in {eval_duration_cart:.2f} seconds ---\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{model_name_cart}] Error during final evaluation: {e}\", exc_info=True)\n",
    "            metrics_cart = None\n",
    "\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_cart}] Skipping final evaluation (Final predictor not available).\")\n",
    "\n",
    "    # --- Store results ---\n",
    "    all_results[model_name_cart] = {\n",
    "        'metrics': metrics_cart,\n",
    "        'best_hpo_params': best_params_cart,\n",
    "        'hpo_f1_score': best_score_hpo_cart,\n",
    "        'hpo_duration_s': hpo_duration_cart,\n",
    "        'calibration_duration_s': calibration_duration_cart,\n",
    "        'evaluation_duration_s': eval_duration_cart,\n",
    "        'timestamp': timestamp_cart,\n",
    "        'group': group,\n",
    "        'calibration_method': CALIBRATOR\n",
    "    }\n",
    "\n",
    "    logging.info(f\"===== Finished Workflow for {model_name_cart} ({group}) =====\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0a169",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_hpo(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    MAX_RESOURCE_RF,\n",
    "    MIN_RESOURCE_RF,\n",
    "    ETA_RF,\n",
    "    RESOURCE_TYPE_RF,\n",
    "    RANDOM_SEED,\n",
    "    RandomForestClassifier,\n",
    "    randint,\n",
    "    f1_score,\n",
    "    hyperband_hpo,\n",
    "    logging,\n",
    "    model_name_rf\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Hyperband HPO for Random Forest and return best parameters and best score.\n",
    "    \"\"\"\n",
    "    # --- RF: Define Search Space and HPO Params ---\n",
    "    param_space_rf = {\n",
    "        # qualitative choices ------------------------------------------------------\n",
    "        \"criterion\":  [\"gini\", \"entropy\", \"log_loss\"],      # all three supported\n",
    "        \"bootstrap\":  [True, False],                        # bagging on/off\n",
    "\n",
    "        # tree-shape & regularisation ---------------------------------------------\n",
    "        \"max_depth\":         randint(3, 40),   # 3  39  (SciPys randint is [low,high) )\n",
    "        \"min_samples_split\": randint(2, 50),   # 2  49\n",
    "        \"min_samples_leaf\":  randint(1, 20),   # 1  19\n",
    "        \"ccp_alpha\":         loguniform(1e-7, 1e-2),        # cost-complexity pruning\n",
    "\n",
    "        # feature sub-sampling -----------------------------------------------------\n",
    "        \"max_features\": [\"sqrt\", \"log2\", None, 0.3, 0.5, 0.8],  # categorical + float\n",
    "        # n_estimators is driven by Hyperbands resource schedule (20300)\n",
    "    }\n",
    "\n",
    "    logging.info(f\"--- [{model_name_rf}] Running Hyperband HPO ---\")\n",
    "    best_params_rf, best_score_hpo_rf = hyperband_hpo(\n",
    "        model_class=RandomForestClassifier,\n",
    "        param_space=param_space_rf,\n",
    "        X_train=X_train, # USE UNSCALED DATA\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,     # USE UNSCALED DATA\n",
    "        y_val=y_val,\n",
    "        max_resource=MAX_RESOURCE_RF,\n",
    "        eta=ETA_RF,\n",
    "        resource_type=RESOURCE_TYPE_RF,\n",
    "        min_resource=MIN_RESOURCE_RF,\n",
    "        scoring_func=f1_score,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    return best_params_rf, best_score_hpo_rf\n",
    "\n",
    "def random_forest_workflow(\n",
    "    X_train, y_train, X_val, y_val, X_cal, y_cal, X_test, y_test,\n",
    "    model_name_rf,\n",
    "    MAX_RESOURCE_RF,\n",
    "    MIN_RESOURCE_RF,\n",
    "    ETA_RF,\n",
    "    RESOURCE_TYPE_RF,\n",
    "    CALIBRATOR,\n",
    "    \n",
    "    RANDOM_SEED,\n",
    "    RandomForestClassifier,\n",
    "    randint,\n",
    "    f1_score,\n",
    "    hyperband_hpo,\n",
    "    train_calibrate_model,\n",
    "    calculate_metrics,\n",
    "    logging,\n",
    "    np,\n",
    "    datetime,\n",
    "    best_params_rf=None,  # <-- Allow passing best_params directly\n",
    "    best_score_hpo_rf=None,\n",
    "    group = \"\",\n",
    "    model_dir = \"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Full Random Forest workflow: HPO, training, calibration and evaluation.\n",
    "    This function is a direct conversion of the notebook workflow to a callable function.\n",
    "    All logic is preserved exactly as in the original notebook cell.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train, y_train : Training data and labels\n",
    "    X_val, y_val     : Validation data and labels\n",
    "    X_cal, y_cal     : Calibration data and labels\n",
    "    X_test, y_test   : Test data and labels\n",
    "    model_name_rf    : Name of the model (string)\n",
    "    MAX_RESOURCE_RF, MIN_RESOURCE_RF, ETA_RF, RESOURCE_TYPE_RF : HPO params\n",
    "    CALIBRATOR       : Calibration method (e.g., 'cvap', 'platt', etc.)\n",
    "    ALPHA            : Conformal prediction significance level\n",
    "    RANDOM_SEED      : Random seed for reproducibility\n",
    "    RandomForestClassifier : RF class\n",
    "    randint          : Distribution for HPO\n",
    "    f1_score         : F1 scoring function\n",
    "    hyperband_hpo    : Hyperband HPO function\n",
    "    train_calibrate_model : Model training and calibration function\n",
    "    calculate_metrics : Function to calculate metrics\n",
    "    logging          : Logging module\n",
    "    np               : Numpy module\n",
    "    datetime         : Datetime module\n",
    "    best_params_rf   : (Optional) If provided, skips HPO and uses these params\n",
    "    best_score_hpo_rf: (Optional) If provided, uses this as HPO score\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict or None: Dictionary containing results for this model, or None if HPO fails.\n",
    "    \"\"\"\n",
    "    logging.info(f\"\\n\\n===== Starting Workflow for {model_name_rf} ({group}) =====\")\n",
    "    timestamp_rf = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    all_results = {}  # Initialize results dict for this run\n",
    "\n",
    "    # --- HPO Step ---\n",
    "    if best_params_rf is None:\n",
    "        hpo_start_time_rf = time.time()\n",
    "        best_params_rf, best_score_hpo_rf = rf_hpo(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            MAX_RESOURCE_RF,\n",
    "            MIN_RESOURCE_RF,\n",
    "            ETA_RF,\n",
    "            RESOURCE_TYPE_RF,\n",
    "            RANDOM_SEED,\n",
    "            RandomForestClassifier,\n",
    "            randint,\n",
    "            f1_score,\n",
    "            hyperband_hpo,\n",
    "            logging,\n",
    "            model_name_rf\n",
    "        )\n",
    "        hpo_duration_rf = time.time() - hpo_start_time_rf\n",
    "        logging.info(f\"--- [{model_name_rf}] HPO finished in {hpo_duration_rf:.2f} seconds ---\")\n",
    "    else:\n",
    "        hpo_duration_rf = 0.0\n",
    "        logging.info(f\"--- [{model_name_rf}] Using provided hyperparameters, skipping HPO. ---\")\n",
    "\n",
    "    # --- Training & Calibration Step ---\n",
    "    final_rf_predictor = None  # This will be the single predictor object (calibrated)\n",
    "    calibration_duration_rf = 0.0\n",
    "    if best_params_rf:\n",
    "        logging.info(f\"--- [{model_name_rf}] Training final model & calibrator ({CALIBRATOR}) ---\")\n",
    "        calibration_start_time_rf = time.time()\n",
    "        # Ensure necessary fixed parameters are present for the final fit\n",
    "        best_params_rf['random_state'] = RANDOM_SEED\n",
    "        if 'class_weight' not in best_params_rf:\n",
    "            best_params_rf['class_weight'] = 'balanced'\n",
    "        best_params_rf['n_jobs'] = -1  # Use all cores\n",
    "\n",
    "        # Only one object is returned (the calibrated predictor)\n",
    "        final_rf_predictor = train_calibrate_model(\n",
    "            base_estimator_class=RandomForestClassifier,\n",
    "            best_params=best_params_rf,\n",
    "            X_train=X_train,  # Use UNSCALED training data\n",
    "            y_train=y_train,\n",
    "            model_name=model_name_rf,\n",
    "            group_name=group,\n",
    "            MODEL_DIR=model_dir,\n",
    "            calibration_method=CALIBRATOR,\n",
    "            n_splits=5,\n",
    "            random_state=RANDOM_SEED,\n",
    "            score_method='predict_proba',\n",
    "            cvap_loss='log',\n",
    "            cvap_precision=None\n",
    "        )\n",
    "        calibration_duration_rf = time.time() - calibration_start_time_rf\n",
    "        if final_rf_predictor:\n",
    "            logging.info(f\"--- [{model_name_rf}] Calibration with {CALIBRATOR} finished in {calibration_duration_rf:.2f} seconds ---\")\n",
    "            # Optional: Save model\n",
    "            # joblib.dump(...)\n",
    "        else:\n",
    "            logging.error(f\"[{model_name_rf}] Failed to train and calibrate the final model.\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_rf}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "\n",
    "    # --- Final Evaluation Step ---\n",
    "    metrics_rf = None\n",
    "    cp_coverage_mond_rf, cp_avg_set_size_mond_rf = None, None\n",
    "    class_coverage_dict = None\n",
    "    eval_duration_rf = 0.0\n",
    "\n",
    "    if final_rf_predictor:\n",
    "        logging.info(f\"--- [{model_name_rf}] Final Evaluation on Test Set ---\")\n",
    "        eval_start_time_rf = time.time()\n",
    "        try:\n",
    "            # Get calibrated probabilities from the final predictor\n",
    "            calibrated_probs_test_rf = final_rf_predictor.predict_proba(X_test)\n",
    "            y_proba_test_rf = calibrated_probs_test_rf[:, 1]  # Probability of positive class\n",
    "            y_pred_test_rf = (y_proba_test_rf >= 0.5).astype(int)  # Threshold calibrated probabilities\n",
    "\n",
    "            metrics_rf = calculate_metrics(y_test, y_pred_test_rf, y_proba_test_rf, model_name=model_name_rf)\n",
    "\n",
    "            eval_duration_rf = time.time() - eval_start_time_rf\n",
    "            logging.info(f\"--- [{model_name_rf}] Total Evaluation finished in {eval_duration_rf:.2f} seconds ---\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{model_name_rf}] Error during final evaluation: {e}\", exc_info=True)\n",
    "            metrics_rf = None\n",
    "            cp_coverage_mond_rf, cp_avg_set_size_mond_rf = None, None\n",
    "            class_coverage_dict = None\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_rf}] Skipping final evaluation (Final predictor not available).\")\n",
    "\n",
    "    # --- Store results ---\n",
    "    all_results[model_name_rf] = {\n",
    "        'metrics': metrics_rf,\n",
    "        'best_hpo_params': best_params_rf,\n",
    "        'hpo_f1_score': best_score_hpo_rf,\n",
    "        'hpo_duration_s': hpo_duration_rf,\n",
    "        'calibration_duration_s': calibration_duration_rf,\n",
    "        'evaluation_duration_s': eval_duration_rf,\n",
    "        'timestamp': timestamp_rf,\n",
    "        'group': group,\n",
    "        'calibration_method': CALIBRATOR\n",
    "    }\n",
    "\n",
    "    logging.info(f\"===== Finished Workflow for {model_name_rf} ({group}) =====\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c03f0",
   "metadata": {},
   "source": [
    "### 3.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_hpo(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    MAX_RESOURCE_XGB,\n",
    "    MIN_RESOURCE_XGB,\n",
    "    ETA_XGB,\n",
    "    RESOURCE_TYPE_XGB,\n",
    "    RANDOM_SEED,\n",
    "    xgb,\n",
    "    loguniform,\n",
    "    randint,\n",
    "    uniform,\n",
    "    f1_score,\n",
    "    hyperband_hpo,\n",
    "    logging,\n",
    "    model_name_xgb\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Hyperband HPO for XGBoost and return best parameters and best score.\n",
    "    \"\"\"\n",
    "    # --- XGB: Define Search Space and HPO Params ---\n",
    "    param_space_xgb = {\n",
    "        # n_estimators is driven by the Hyperband resource schedule (30500)\n",
    "        \"learning_rate\":  loguniform(1e-3, 0.2),       # 0.001  0.20\n",
    "        \"max_depth\":      randint(3, 11),              # 3  10  (inclusive)\n",
    "        \"min_child_weight\": loguniform(5e-1, 10),      # 0.5  10\n",
    "        \"subsample\":      uniform(0.5, 0.5),           # 0.5  <1.0\n",
    "        \"colsample_bytree\": uniform(0.5, 0.5),         # 0.5  <1.0\n",
    "        \"gamma\":          loguniform(1e-4, 5),         # 0.0001  5\n",
    "        \"reg_alpha\":      loguniform(1e-4, 10),        # 0.0001  10   (L1)\n",
    "        \"reg_lambda\":     loguniform(1e-3, 10),        # 0.001   10   (L2)\n",
    "\n",
    "        # fixed for a binary-classification use-case\n",
    "        \"objective\":  [\"binary:logistic\"],\n",
    "        \"eval_metric\":[\"logloss\"],\n",
    "    }\n",
    "\n",
    "    logging.info(f\"--- [{model_name_xgb}] Running Hyperband HPO ---\")\n",
    "    best_params_xgb, best_score_hpo_xgb = hyperband_hpo(\n",
    "        model_class=xgb.XGBClassifier,\n",
    "        param_space=param_space_xgb,\n",
    "        X_train=X_train, # USE UNSCALED DATA\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,     # USE UNSCALED DATA\n",
    "        y_val=y_val,\n",
    "        max_resource=MAX_RESOURCE_XGB,\n",
    "        eta=ETA_XGB,\n",
    "        resource_type=RESOURCE_TYPE_XGB,\n",
    "        min_resource=MIN_RESOURCE_XGB,\n",
    "        scoring_func=f1_score,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    return best_params_xgb, best_score_hpo_xgb\n",
    "\n",
    "def xgb_workflow(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "    all_results,\n",
    "    model_name_xgb,\n",
    "    MAX_RESOURCE_XGB,\n",
    "    MIN_RESOURCE_XGB,\n",
    "    ETA_XGB,\n",
    "    RESOURCE_TYPE_XGB,\n",
    "    ROUNDS,\n",
    "    CALIBRATOR,\n",
    "    \n",
    "    RANDOM_SEED,\n",
    "    loguniform,\n",
    "    randint,\n",
    "    uniform,\n",
    "    xgb,\n",
    "    hyperband_hpo,\n",
    "    f1_score,\n",
    "    EarlyStopping,\n",
    "    train_calibrate_model,\n",
    "    calculate_metrics,\n",
    "    logging,\n",
    "    np,\n",
    "    best_params_xgb=None,  # <-- Allow passing best_params directly\n",
    "    best_score_hpo_xgb=None,\n",
    "    group = \"\", \n",
    "    model_dir = \"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete XGBoost workflow: HPO, calibration and evaluation.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Defines the XGBoost hyperparameter search space.\n",
    "    2. Runs Hyperband HPO to find the best hyperparameters.\n",
    "    3. Determines the best number of boosting rounds using early stopping.\n",
    "    4. Trains the final XGBoost model and calibrator.\n",
    "    6. Evaluates the model and conformal predictor on the test set.\n",
    "    7. Stores all results in the provided all_results dictionary.\n",
    "\n",
    "    All logic is preserved exactly as in the original notebook workflow.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train, y_train, X_val, y_val, X_cal, y_cal, X_test, y_test : pd.DataFrame/np.ndarray\n",
    "        Data splits for training, validation, calibration, and testing.\n",
    "    model_name_xgb : str\n",
    "        Name of the model (e.g., \"XGBoost\").\n",
    "    MAX_RESOURCE_XGB, MIN_RESOURCE_XGB, ETA_XGB, RESOURCE_TYPE_XGB, ROUNDS : int/str\n",
    "        Hyperband and early stopping parameters.\n",
    "    CALIBRATOR : str\n",
    "        Calibration method to use.\n",
    "    ALPHA : float\n",
    "        Significance level for conformal prediction.\n",
    "    RANDOM_SEED : int\n",
    "        Random seed for reproducibility.\n",
    "    loguniform, randint, uniform : scipy.stats distributions\n",
    "        Distributions for hyperparameter sampling.\n",
    "    xgb : module\n",
    "        XGBoost module.\n",
    "    hyperband_hpo : function\n",
    "        Function to run Hyperband HPO.\n",
    "    f1_score : function\n",
    "        F1 scoring function.\n",
    "    EarlyStopping : class\n",
    "        XGBoost early stopping callback.\n",
    "    train_calibrate_model : function\n",
    "        Function to train and calibrate the model.\n",
    "    calculate_metrics : function\n",
    "        Function to calculate base metrics.\n",
    "    logging : module\n",
    "        Logging module.\n",
    "    np : module\n",
    "        Numpy module.\n",
    "    best_params_xgb : (Optional) If provided, skips HPO and uses these params\n",
    "    best_score_hpo_xgb : (Optional) If provided, uses this as HPO score\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Results are stored in all_results[model_name_xgb].\n",
    "    \"\"\"\n",
    "    logging.info(f\"\\n\\n===== Starting Workflow for {model_name_xgb} =====\")\n",
    "    timestamp_xgb = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if best_params_xgb is None:\n",
    "        hpo_start_time_xgb = time.time()\n",
    "        best_params_xgb, best_score_hpo_xgb = xgb_hpo(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            MAX_RESOURCE_XGB,\n",
    "            MIN_RESOURCE_XGB,\n",
    "            ETA_XGB,\n",
    "            RESOURCE_TYPE_XGB,\n",
    "            RANDOM_SEED,\n",
    "            xgb,\n",
    "            loguniform,\n",
    "            randint,\n",
    "            uniform,\n",
    "            f1_score,\n",
    "            hyperband_hpo,\n",
    "            logging,\n",
    "            model_name_xgb\n",
    "        )\n",
    "        hpo_duration_xgb = time.time() - hpo_start_time_xgb\n",
    "        logging.info(f\"--- [{model_name_xgb}] HPO finished in {hpo_duration_xgb:.2f} seconds ---\")\n",
    "    else:\n",
    "        hpo_duration_xgb = 0.0\n",
    "        logging.info(f\"--- [{model_name_xgb}] Using provided hyperparameters, skipping HPO. ---\")\n",
    "\n",
    "    # --- 4.3 XGB: Train Final Model & Calibration (using Full Training Set) ---\n",
    "    final_xgb_predictor = None\n",
    "    final_best_params_xgb = None  # Initialize\n",
    "\n",
    "    if best_params_xgb:\n",
    "        logging.info(f\"--- [{model_name_xgb}] Determining best iteration and training calibrator ---\")\n",
    "        calibration_start_time_xgb = time.time()\n",
    "\n",
    "        # 1. Determine best iteration using early stopping on validation set\n",
    "        temp_best_params_xgb = best_params_xgb.copy()  # Work with a copy\n",
    "        temp_best_params_xgb['random_state'] = RANDOM_SEED\n",
    "        if 'objective' not in temp_best_params_xgb: temp_best_params_xgb['objective'] = 'binary:logistic'\n",
    "        if 'eval_metric' not in temp_best_params_xgb: temp_best_params_xgb['eval_metric'] = 'logloss'\n",
    "        if 'n_jobs' not in temp_best_params_xgb: temp_best_params_xgb['n_jobs'] = -1\n",
    "        if 'scale_pos_weight' not in temp_best_params_xgb:\n",
    "            neg_count = (y_train == 0).sum()\n",
    "            pos_count = (y_train == 1).sum()\n",
    "            if pos_count > 0:\n",
    "                temp_best_params_xgb['scale_pos_weight'] = neg_count / pos_count\n",
    "        # Define callbacks for the fit that determines the best iteration\n",
    "        xgb_final_iteration_callbacks = [\n",
    "                EarlyStopping(rounds=ROUNDS,        # Number of rounds to wait for improvement\n",
    "                                save_best=True,   # Saves the model from the best iteration\n",
    "                                metric_name='logloss', # Explicitly state the metric to monitor (optional but good practice)\n",
    "                                maximize=False)    # We want to minimize logloss\n",
    "            ]\n",
    "\n",
    "        logging.info(\"Training temporary XGBoost with early stopping to find best iteration...\")\n",
    "        temp_xgb_model = xgb.XGBClassifier(**temp_best_params_xgb, callbacks=xgb_final_iteration_callbacks)\n",
    "        eval_set_final = [(X_val, y_val)]\n",
    "        temp_xgb_model.fit(X_train, y_train, eval_set=eval_set_final, verbose=False)\n",
    "\n",
    "        # Retrieve the best iteration\n",
    "        best_iteration = temp_xgb_model.best_iteration\n",
    "        if best_iteration is None or best_iteration <= 0:\n",
    "            logging.warning(f\"Early stopping did not trigger or returned invalid iteration ({best_iteration}). Using max_resource ({MAX_RESOURCE_XGB}) as n_estimators.\")\n",
    "            best_iteration = MAX_RESOURCE_XGB\n",
    "        logging.info(f\"Best iteration found: {best_iteration}\")\n",
    "\n",
    "        # Update best_params with the optimal number of estimators found\n",
    "        final_best_params_xgb = temp_best_params_xgb.copy()\n",
    "        final_best_params_xgb['n_estimators'] = best_iteration\n",
    "\n",
    "        # 2. Train final model and calibrator using train_calibrate_model (returns a single predictor)\n",
    "        logging.info(f\"--- [{model_name_xgb}] Training final model ({final_best_params_xgb['n_estimators']} est.) and calibrator ---\")\n",
    "        final_xgb_predictor = train_calibrate_model(\n",
    "            base_estimator_class=xgb.XGBClassifier,\n",
    "            best_params=final_best_params_xgb,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            model_name = model_name_xgb,\n",
    "            group_name = group,\n",
    "            MODEL_DIR = model_dir,\n",
    "            calibration_method=CALIBRATOR,  # Use chosen calibration method (e.g., 'cvap', 'platt', etc.)\n",
    "            n_splits=5,\n",
    "            random_state=RANDOM_SEED,\n",
    "            score_method='raw_margin_xgb',  # For XGB, use raw margin for Platt/CVAP\n",
    "            cvap_loss='log',                # or 'brier' if desired\n",
    "            cvap_precision=None             # or set as needed\n",
    "        )\n",
    "        calibration_duration_xgb = time.time() - calibration_start_time_xgb\n",
    "        if final_xgb_predictor:\n",
    "            logging.info(f\"--- [{model_name_xgb}] Calibration with {CALIBRATOR} finished in {calibration_duration_xgb:.2f} seconds ---\")\n",
    "            # Optional: Save models\n",
    "            # joblib.dump(...)\n",
    "        else:\n",
    "            logging.error(f\"[{model_name_xgb}] Failed to train/calibrate the final model.\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_xgb}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "\n",
    "    # --- 4.4 XGB: Final Evaluation (using Test Set) ---\n",
    "    metrics_xgb = None\n",
    "    if final_xgb_predictor:\n",
    "        logging.info(f\"--- [{model_name_xgb}] Final Evaluation on Test Set ---\")\n",
    "        eval_start_time_xgb = time.time()\n",
    "\n",
    "        # --- Calculate Base Metrics (Same as before) ---\n",
    "        probs_test_xgb_full = final_xgb_predictor.predict_proba(X_test)  # Calibrated probs for BOTH classes\n",
    "        y_proba_test_xgb = probs_test_xgb_full[:, 1]\n",
    "        y_pred_test_xgb = (y_proba_test_xgb >= 0.5).astype(int)\n",
    "        metrics_xgb = calculate_metrics(y_test, y_pred_test_xgb, y_proba_test_xgb, model_name=model_name_xgb)\n",
    "\n",
    "        eval_duration_xgb = time.time() - eval_start_time_xgb  # Total eval time\n",
    "        logging.info(f\"--- [{model_name_xgb}] Total Evaluation finished in {eval_duration_xgb:.2f} seconds ---\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_xgb}] Skipping final evaluation (Final predictor not available).\")\n",
    "\n",
    "    # --- Store results (comprehensive) ---\n",
    "    all_results = {model_name_xgb : {\n",
    "        'metrics': metrics_xgb,\n",
    "        'best_hpo_params': best_params_xgb,\n",
    "        'final_n_estimators': final_best_params_xgb.get('n_estimators', None) if final_best_params_xgb else None,\n",
    "        'hpo_f1_score': best_score_hpo_xgb,\n",
    "        'hpo_duration_s': hpo_duration_xgb,\n",
    "        'final_xgb_predictor': final_xgb_predictor,\n",
    "        'timestamp': timestamp_xgb,\n",
    "    }}\n",
    "\n",
    "    logging.info(f\"===== Finished Workflow for {model_name_xgb} =====\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d64e5",
   "metadata": {},
   "source": [
    "### 3.5 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fbb4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_hpo(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    MAX_RESOURCE_LGBM,\n",
    "    MIN_RESOURCE_LGBM,\n",
    "    ETA_LGBM,\n",
    "    RESOURCE_TYPE_LGBM,\n",
    "    RANDOM_SEED,\n",
    "    lgb,\n",
    "    loguniform,\n",
    "    randint,\n",
    "    uniform,\n",
    "    f1_score,\n",
    "    hyperband_hpo,\n",
    "    logging,\n",
    "    model_name_lgb\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Hyperband HPO for LightGBM and return best parameters and best score.\n",
    "    \"\"\"\n",
    "    # --- LGBM: Define Search Space and HPO Params ---\n",
    "    param_space_lgbm = {\n",
    "        # n_estimators is handled by the Hyperband schedule (30500)\n",
    "        \"learning_rate\":     loguniform(1e-3, 0.2),      # 0.001  0.20\n",
    "        \"num_leaves\":        randint(16, 128),           # 16  127  ( 27  21)\n",
    "        \"max_depth\":         randint(3, 13),             # 3  12   (1 handled by  num_leaves)\n",
    "        \"min_child_samples\": randint(5, 60),             # 5  59   (aka min_data_in_leaf)\n",
    "        \"subsample\":         uniform(0.5, 0.5),          # 0.5  <1   (bagging_fraction)\n",
    "        \"colsample_bytree\":  uniform(0.5, 0.5),          # 0.5  <1   (feature_fraction)\n",
    "        \"reg_alpha\":         loguniform(1e-4, 10),       # 0.0001  10   ()\n",
    "        \"reg_lambda\":        loguniform(1e-3, 10),       # 0.001   10   ()\n",
    "\n",
    "        # fixed for binary classification\n",
    "        \"objective\":  [\"binary\"],\n",
    "        \"metric\":     [\"logloss\"],\n",
    "        \"verbose\":    [-1]\n",
    "    }\n",
    "\n",
    "    logging.info(f\"--- [{model_name_lgbm}] Running Hyperband HPO ---\")\n",
    "    best_params_lgbm, best_score_hpo_lgbm = hyperband_hpo(\n",
    "        model_class=lgb.LGBMClassifier,\n",
    "        param_space=param_space_lgbm,\n",
    "        X_train=X_train, # USE UNSCALED DATA\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,     # USE UNSCALED DATA\n",
    "        y_val=y_val,\n",
    "        max_resource=MAX_RESOURCE_LGBM,\n",
    "        eta=ETA_LGBM,\n",
    "        resource_type=RESOURCE_TYPE_LGBM,\n",
    "        min_resource=MIN_RESOURCE_LGBM,\n",
    "        scoring_func=f1_score,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    return best_params_lgbm, best_score_hpo_lgbm\n",
    "\n",
    "def lgbm_workflow(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "    model_name_lgbm,\n",
    "    MAX_RESOURCE_LGBM,\n",
    "    MIN_RESOURCE_LGBM,\n",
    "    ETA_LGBM,\n",
    "    RESOURCE_TYPE_LGBM,\n",
    "    ROUNDS,\n",
    "    CALIBRATOR,\n",
    "    \n",
    "    RANDOM_SEED,\n",
    "    lgb,\n",
    "    loguniform,\n",
    "    randint,\n",
    "    uniform,\n",
    "    f1_score,\n",
    "    hyperband_hpo,\n",
    "    early_stopping,\n",
    "    train_calibrate_model,\n",
    "    calculate_metrics,\n",
    "    logging,\n",
    "    np,\n",
    "    datetime,\n",
    "    best_params_lgbm=None,  # <-- Allow passing best_params directly\n",
    "    best_score_hpo_lgbm=None,\n",
    "    group = \"\",\n",
    "    model_dir = \"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Full LightGBM workflow: HPO, training, calibration and evaluation.\n",
    "    This function is a direct conversion of the notebook workflow to a callable function.\n",
    "    All logic is preserved exactly as in the original notebook cell.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train, y_train : Training data and labels\n",
    "    X_val, y_val     : Validation data and labels\n",
    "    X_cal, y_cal     : Calibration data and labels\n",
    "    X_test, y_test   : Test data and labels\n",
    "    model_name_lgbm  : Name of the model (string)\n",
    "    MAX_RESOURCE_LGBM, MIN_RESOURCE_LGBM, ETA_LGBM, RESOURCE_TYPE_LGBM, ROUNDS : HPO and early stopping params\n",
    "    CALIBRATOR       : Calibration method (e.g., 'cvap', 'platt', etc.)\n",
    "    ALPHA            : Conformal prediction significance level\n",
    "    RANDOM_SEED      : Random seed for reproducibility\n",
    "    lgb              : LightGBM module\n",
    "    loguniform, randint, uniform : Distributions for HPO\n",
    "    f1_score         : F1 scoring function\n",
    "    hyperband_hpo    : Hyperband HPO function\n",
    "    early_stopping   : LightGBM early stopping callback\n",
    "    train_calibrate_model : Model training and calibration function\n",
    "    calculate_metrics : Function to calculate metrics\n",
    "    logging          : Logging module\n",
    "    np               : Numpy module\n",
    "    datetime         : Datetime module\n",
    "    best_params_lgbm : (Optional) If provided, skips HPO and uses these params\n",
    "    best_score_hpo_lgbm : (Optional) If provided, uses this as HPO score\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None (results are stored in all_results[model_name_lgbm])\n",
    "    \"\"\"\n",
    "    logging.info(f\"\\n\\n===== Starting Workflow for {model_name_lgbm} =====\")\n",
    "    timestamp_lgbm = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if best_params_lgbm is None:\n",
    "        hpo_start_time_lgbm = time.time()\n",
    "        best_params_lgbm, best_score_hpo_lgbm = lgbm_hpo(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            MAX_RESOURCE_LGBM,\n",
    "            MIN_RESOURCE_LGBM,\n",
    "            ETA_LGBM,\n",
    "            RESOURCE_TYPE_LGBM,\n",
    "            RANDOM_SEED,\n",
    "            lgb,\n",
    "            loguniform,\n",
    "            randint,\n",
    "            uniform,\n",
    "            f1_score,\n",
    "            hyperband_hpo,\n",
    "            logging,\n",
    "            model_name_lgbm\n",
    "        )\n",
    "        hpo_duration_lgbm = time.time() - hpo_start_time_lgbm\n",
    "        logging.info(f\"--- [{model_name_lgbm}] HPO finished in {hpo_duration_lgbm:.2f} seconds ---\")\n",
    "    else:\n",
    "        hpo_duration_lgbm = 0.0\n",
    "        logging.info(f\"--- [{model_name_lgbm}] Using provided hyperparameters, skipping HPO. ---\")\n",
    "\n",
    "    # --- 5.3 LGBM: Train Final Model & Calibration (using Full Training Set) ---\n",
    "    final_lgbm_predictor = None\n",
    "    final_best_params_lgbm = None  # Initialize\n",
    "\n",
    "    if best_params_lgbm:\n",
    "        logging.info(f\"--- [{model_name_lgbm}] Determining best iteration and training calibrator ---\")\n",
    "        calibration_start_time_lgbm = time.time()\n",
    "\n",
    "        # 1. Determine best iteration using early stopping on validation set\n",
    "        temp_best_params_lgbm = best_params_lgbm.copy()  # Work with a copy\n",
    "        temp_best_params_lgbm['random_state'] = RANDOM_SEED\n",
    "        if 'objective' not in temp_best_params_lgbm: temp_best_params_lgbm['objective'] = 'binary'\n",
    "        if 'metric' not in temp_best_params_lgbm: temp_best_params_lgbm['metric'] = 'logloss'\n",
    "        if 'n_jobs' not in temp_best_params_lgbm: temp_best_params_lgbm['n_jobs'] = -1\n",
    "        if 'verbose' not in temp_best_params_lgbm: temp_best_params_lgbm['verbose'] = -1  # Control verbosity\n",
    "        if 'scale_pos_weight' not in temp_best_params_lgbm:\n",
    "            neg_count = (y_train == 0).sum()\n",
    "            pos_count = (y_train == 1).sum()\n",
    "            if pos_count > 0:\n",
    "                temp_best_params_lgbm['scale_pos_weight'] = neg_count / pos_count\n",
    "                if 'is_unbalance' in temp_best_params_lgbm: del temp_best_params_lgbm['is_unbalance']\n",
    "            elif 'is_unbalance' not in temp_best_params_lgbm:\n",
    "                temp_best_params_lgbm['is_unbalance'] = True\n",
    "\n",
    "        # Define callbacks for the fit that determines the best iteration\n",
    "        callbacks_final = [\n",
    "            early_stopping(stopping_rounds=ROUNDS, verbose=False)\n",
    "        ]\n",
    "        metric_to_monitor = 'logloss'\n",
    "\n",
    "        logging.info(f\"Training temporary LightGBM with early stopping (monitoring '{metric_to_monitor}') to find best iteration...\")\n",
    "        temp_lgbm_model = lgb.LGBMClassifier(**temp_best_params_lgbm)\n",
    "        eval_set_final_lgbm = [(X_val, y_val)]\n",
    "\n",
    "        # Check if eval_set is valid before fitting\n",
    "        if not eval_set_final_lgbm or not isinstance(eval_set_final_lgbm, list) or not eval_set_final_lgbm[0]:\n",
    "            raise ValueError(\"eval_set_final_lgbm is not correctly defined before fitting.\")\n",
    "        if len(eval_set_final_lgbm[0]) != 2:\n",
    "            raise ValueError(\"Each element in eval_set must be a tuple (X, y).\")\n",
    "\n",
    "        try:\n",
    "            temp_lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set_final_lgbm,\n",
    "                eval_metric=metric_to_monitor,\n",
    "                callbacks=callbacks_final\n",
    "            )\n",
    "\n",
    "            # Retrieve the best iteration\n",
    "            best_iteration_lgbm = temp_lgbm_model.best_iteration_\n",
    "            if best_iteration_lgbm is None or best_iteration_lgbm <= 0:\n",
    "                logging.warning(f\"Early stopping did not trigger or returned invalid iteration ({best_iteration_lgbm}). Using max_resource ({MAX_RESOURCE_LGBM}) as n_estimators.\")\n",
    "                best_iteration_lgbm = MAX_RESOURCE_LGBM\n",
    "            logging.info(f\"Best iteration found: {best_iteration_lgbm}\")\n",
    "\n",
    "            # Update best_params with the optimal number of estimators found\n",
    "            final_best_params_lgbm = temp_best_params_lgbm.copy()\n",
    "            final_best_params_lgbm['n_estimators'] = best_iteration_lgbm\n",
    "\n",
    "            # 2. Train final model and calibrator using train_calibrate_model (like XGB/SVM)\n",
    "            logging.info(f\"--- [{model_name_lgbm}] Training final model ({final_best_params_lgbm['n_estimators']} est.) and calibrator ---\")\n",
    "            final_lgbm_predictor = train_calibrate_model(\n",
    "                base_estimator_class=lgb.LGBMClassifier,\n",
    "                best_params=final_best_params_lgbm,\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                model_name = model_name_lgbm,\n",
    "                group_name = group,\n",
    "                MODEL_DIR = model_dir,\n",
    "                calibration_method=CALIBRATOR,  # Use chosen calibration method (e.g., 'cvap', 'platt', etc.)\n",
    "                n_splits=5,\n",
    "                random_state=RANDOM_SEED,\n",
    "                score_method='raw_score_lgbm',  # Specify score method for LGBM if needed\n",
    "                cvap_loss='log',                # or 'brier' if desired\n",
    "                cvap_precision=None             # or set as needed\n",
    "            )\n",
    "            calibration_duration_lgbm = time.time() - calibration_start_time_lgbm\n",
    "            if final_lgbm_predictor:\n",
    "                logging.info(f\"--- [{model_name_lgbm}] Calibration finished in {calibration_duration_lgbm:.2f} seconds ---\")\n",
    "                # Optional: Save models\n",
    "                # joblib.dump(...)\n",
    "            else:\n",
    "                logging.error(f\"[{model_name_lgbm}] Failed to train final model or calibrator.\")\n",
    "        except ValueError as ve:\n",
    "            logging.error(f\"ValueError during temp_lgbm_model.fit: {ve}\")\n",
    "            logging.error(f\"Shapes: X_train={X_train.shape}, y_train={y_train.shape}, X_val={X_val.shape}, y_val={y_val.shape}\")\n",
    "            raise\n",
    "\n",
    "    # --- 5.5 LGBM: Final Evaluation ---\n",
    "    metrics_lgbm = None\n",
    "    if final_lgbm_predictor:\n",
    "        logging.info(f\"--- [{model_name_lgbm}] Final Evaluation on Test Set ---\")\n",
    "        eval_start_time_lgbm = time.time()\n",
    "\n",
    "        # --- Calculate Base Metrics (Same as before) ---\n",
    "        probs_test_lgbm_full = final_lgbm_predictor.predict_proba(X_test)  # Calibrated probs for BOTH classes\n",
    "        y_proba_test_lgbm = probs_test_lgbm_full[:, 1]\n",
    "        y_pred_test_lgbm = (y_proba_test_lgbm >= 0.5).astype(int)\n",
    "        metrics_lgbm = calculate_metrics(y_test, y_pred_test_lgbm, y_proba_test_lgbm, model_name=model_name_lgbm)\n",
    "\n",
    "        eval_duration_lgbm = time.time() - eval_start_time_lgbm  # Total eval time\n",
    "        logging.info(f\"--- [{model_name_lgbm}] Total Evaluation finished in {eval_duration_lgbm:.2f} seconds ---\")\n",
    "\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_lgbm}] Skipping final evaluation (Final model or calibrator not available).\")\n",
    "\n",
    "    # --- Store results (comprehensive) ---\n",
    "    all_results = {model_name_lgbm : {\n",
    "        'metrics': metrics_lgbm,\n",
    "        'best_hpo_params': best_params_lgbm, # Original HPO params\n",
    "        # Store actual used estimators if available\n",
    "        'final_n_estimators': final_best_params_lgbm.get('n_estimators', None) if final_best_params_lgbm else None,\n",
    "        'hpo_f1_score': best_score_hpo_lgbm,\n",
    "        'hpo_duration_s': hpo_duration_lgbm,\n",
    "        'final_model': final_lgbm_predictor,\n",
    "        'timestamp': timestamp_lgbm,\n",
    "    }}\n",
    "\n",
    "    logging.info(f\"===== Finished Workflow for {model_name_lgbm} =====\")\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2740d857",
   "metadata": {},
   "source": [
    "### 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c286e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running HPO:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting feature set group 'group_1' with 8 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Bracket s=0 (n=3, r0=1.00): 100%|| 3/3 [19:28<00:00, 389.58s/it, Best F1: 0.5931]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Bracket s=0 (n=3, r0=1.00): 100%|| 3/3 [00:00<00:00,  6.74it/s, Best F1: 0.5912]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Bracket s=0 (n=3, r0=300.00): 100%|| 3/3 [02:09<00:00, 43.06s/it, Best F1: 0.6461]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Bracket s=0 (n=3, r0=500.00): 100%|| 3/3 [00:07<00:00,  2.43s/it, Best F1: 0.6507]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Bracket s=0 (n=3, r0=500.00): 100%|| 3/3 [00:07<00:00,  2.42s/it, Best F1: 0.6183]\n",
      "Running HPO: 100%|| 1/1 [21:53<00:00, 1313.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HPO loop finished. Best parameters saved in 'best_params' directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "# HPO Loop - Find and Save Best Hyperparameters #\n",
    "#################################################\n",
    "\n",
    "# --- Configuration ---\n",
    "BEST_PARAMS_DIR = \"best_params\"\n",
    "os.makedirs(BEST_PARAMS_DIR, exist_ok=True)\n",
    "\n",
    "# List of feature groups to process (keys from the 'groups' dictionary)\n",
    "# Adjust this list if you only want to run specific groups\n",
    "GROUP_NAMES_TO_PROCESS = list(groups.keys()) # Process all defined groups\n",
    "GROUP_NAMES_TO_PROCESS = ['group_1'] # Example: Process only specific groups\n",
    "\n",
    "# Ensure necessary variables are defined in the global scope before running this cell:\n",
    "# groups, df, TARGET_COLUMN, clean_data, split_data, apply_feature_scaling,\n",
    "# MAX_RESOURCE_*, MIN_RESOURCE_*, ETA_*, RESOURCE_TYPE_*, RANDOM_SEED, MODEL_DIR,\n",
    "# SVC, loguniform, hyperband_hpo, f1_score, model_name_svm,\n",
    "# DecisionTreeClassifier, randint, model_name_cart,\n",
    "# RandomForestClassifier, model_name_rf,\n",
    "# xgb, uniform, model_name_xgb,\n",
    "# lgb, model_name_lgbm,\n",
    "# svm_hpo, cart_hpo, rf_hpo, xgb_hpo, lgbm_hpo\n",
    "\n",
    "# Dictionary to store best params found (optional, mainly for logging/debugging here)\n",
    "all_best_params_found = {}\n",
    "\n",
    "logging.info(f\"Starting HPO loop for groups: {GROUP_NAMES_TO_PROCESS}\")\n",
    "hpo_loop_start_time = time.time()\n",
    "\n",
    "# Use tqdm for the outer loop to show progress over groups\n",
    "for group_name in tqdm(GROUP_NAMES_TO_PROCESS, desc=\"Running HPO\"):\n",
    "    logging.info(f\"\\n{'='*30} Running HPO for Group: {group_name} {'='*30}\")\n",
    "    group_hpo_start_time = time.time()\n",
    "    group_best_params = {} # Store best params for the current group\n",
    "\n",
    "    try:\n",
    "        # --- Data Preparation ---\n",
    "        logging.info(f\"[{group_name}] Cleaning data...\")\n",
    "        X, y, df_clean = clean_data(df, group_name, TARGET_COLUMN, logger=logging)\n",
    "        if X.empty or y.empty:\n",
    "            logging.warning(f\"[{group_name}] Skipping HPO due to insufficient data after cleaning.\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"[{group_name}] Splitting data (Train/Val only for HPO)...\")\n",
    "        # Only need train/val splits for Hyperparameter Optimization\n",
    "        X_train, y_train, X_val, y_val, _, _, _, _ = split_data(X, y)\n",
    "\n",
    "        if X_train.empty or y_train.empty or X_val.empty or y_val.empty:\n",
    "             logging.warning(f\"[{group_name}] Skipping HPO due to empty train or validation set after splitting.\")\n",
    "             continue\n",
    "\n",
    "        logging.info(f\"[{group_name}] Applying feature scaling (for SVM HPO)...\")\n",
    "        # Note: Only scale train/val needed for HPO. Don't save scaler yet.\n",
    "        X_train_scaled, X_val_scaled, _, _, _ = apply_feature_scaling(\n",
    "            X_train, X_val, pd.DataFrame(), pd.DataFrame(), # Pass empty DFs for test/cal\n",
    "            len(X_train) / len(X) if len(X) > 0 else 0,\n",
    "            len(X_val) / len(X) if len(X) > 0 else 0,\n",
    "            0, 0,\n",
    "            MODEL_DIR, save_scaler=False # Don't save scaler during HPO phase\n",
    "        )\n",
    "\n",
    "        # --- Run HPO for each model ---\n",
    "\n",
    "        # SVM HPO\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running SVM HPO ---\")\n",
    "            best_params_svm, best_score_hpo_svm = svm_hpo(\n",
    "                X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "                MAX_RESOURCE_SVM, MIN_RESOURCE_SVM, ETA_SVM, RESOURCE_TYPE_SVM,\n",
    "                RANDOM_SEED, SVC, loguniform, hyperband_hpo, f1_score, logging, model_name_svm\n",
    "            )\n",
    "            group_best_params[model_name_svm] = best_params_svm\n",
    "            logging.info(f\"[{group_name}] SVM Best Params: {best_params_svm}, Score: {best_score_hpo_svm:.4f}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] SVM HPO failed: {e}\", exc_info=False) # Set exc_info=True for full traceback\n",
    "            group_best_params[model_name_svm] = None # Indicate failure\n",
    "\n",
    "        # CART HPO\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running CART HPO ---\")\n",
    "            best_params_cart, best_score_hpo_cart = cart_hpo(\n",
    "                X_train, y_train, X_val, y_val,\n",
    "                MAX_RESOURCE_CART, MIN_RESOURCE_CART, ETA_CART, RESOURCE_TYPE_CART,\n",
    "                RANDOM_SEED, DecisionTreeClassifier, randint, f1_score, hyperband_hpo, logging, model_name_cart\n",
    "            )\n",
    "            group_best_params[model_name_cart] = best_params_cart\n",
    "            logging.info(f\"[{group_name}] CART Best Params: {best_params_cart}, Score: {best_score_hpo_cart:.4f}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] CART HPO failed: {e}\", exc_info=False)\n",
    "            group_best_params[model_name_cart] = None\n",
    "\n",
    "        # Random Forest HPO\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running Random Forest HPO ---\")\n",
    "            best_params_rf, best_score_hpo_rf = rf_hpo(\n",
    "                 X_train, y_train, X_val, y_val,\n",
    "                 MAX_RESOURCE_RF, MIN_RESOURCE_RF, ETA_RF, RESOURCE_TYPE_RF,\n",
    "                 RANDOM_SEED, RandomForestClassifier, randint, f1_score, hyperband_hpo, logging, model_name_rf\n",
    "            )\n",
    "            group_best_params[model_name_rf] = best_params_rf\n",
    "            logging.info(f\"[{group_name}] RF Best Params: {best_params_rf}, Score: {best_score_hpo_rf:.4f}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] Random Forest HPO failed: {e}\", exc_info=False)\n",
    "            group_best_params[model_name_rf] = None\n",
    "\n",
    "        # XGBoost HPO\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running XGBoost HPO ---\")\n",
    "            best_params_xgb, best_score_hpo_xgb = xgb_hpo(\n",
    "                X_train, y_train, X_val, y_val,\n",
    "                MAX_RESOURCE_XGB, MIN_RESOURCE_XGB, ETA_XGB, RESOURCE_TYPE_XGB,\n",
    "                RANDOM_SEED, xgb, loguniform, randint, uniform, f1_score, hyperband_hpo, logging, model_name_xgb\n",
    "            )\n",
    "            group_best_params[model_name_xgb] = best_params_xgb\n",
    "            logging.info(f\"[{group_name}] XGB Best Params: {best_params_xgb}, Score: {best_score_hpo_xgb:.4f}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] XGBoost HPO failed: {e}\", exc_info=False)\n",
    "            group_best_params[model_name_xgb] = None\n",
    "\n",
    "        # LightGBM HPO\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running LightGBM HPO ---\")\n",
    "            best_params_lgbm, best_score_hpo_lgbm = lgbm_hpo(\n",
    "                 X_train, y_train, X_val, y_val,\n",
    "                 MAX_RESOURCE_LGBM, MIN_RESOURCE_LGBM, ETA_LGBM, RESOURCE_TYPE_LGBM,\n",
    "                 RANDOM_SEED, lgb, loguniform, randint, uniform, f1_score, hyperband_hpo, logging, model_name_lgbm\n",
    "            )\n",
    "            group_best_params[model_name_lgbm] = best_params_lgbm\n",
    "            logging.info(f\"[{group_name}] LGBM Best Params: {best_params_lgbm}, Score: {best_score_hpo_lgbm:.4f}\")\n",
    "        except Exception as e:\n",
    "             logging.error(f\"[{group_name}] LightGBM HPO failed: {e}\", exc_info=False)\n",
    "             group_best_params[model_name_lgbm] = None\n",
    "\n",
    "        # --- Save Best Params for the Group ---\n",
    "        params_filepath = os.path.join(BEST_PARAMS_DIR, f\"{group_name}_best_params.json\")\n",
    "        try:\n",
    "            # Convert numpy types to standard Python types for JSON serialization\n",
    "            serializable_params = {}\n",
    "            for model, params in group_best_params.items():\n",
    "                if params is not None:\n",
    "                     serializable_params[model] = {k: (int(v) if isinstance(v, np.integer) else\n",
    "                                                      float(v) if isinstance(v, np.floating) else\n",
    "                                                      v)\n",
    "                                                 for k, v in params.items()}\n",
    "                else:\n",
    "                    serializable_params[model] = None # Keep None for failed HPO\n",
    "\n",
    "            with open(params_filepath, 'w') as f:\n",
    "                json.dump(serializable_params, f, indent=4)\n",
    "            logging.info(f\"[{group_name}] Successfully saved best parameters to {params_filepath}\")\n",
    "            all_best_params_found[group_name] = serializable_params # Store the saved params\n",
    "        except TypeError as e:\n",
    "             logging.error(f\"[{group_name}] Failed to serialize best parameters: {e}. Params: {group_best_params}\", exc_info=True)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] Failed to save best parameters to {params_filepath}: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "        group_hpo_duration = time.time() - group_hpo_start_time\n",
    "        logging.info(f\"--- Finished HPO for Group: {group_name} in {group_hpo_duration:.2f} seconds ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[{group_name}] Unhandled exception during HPO processing for group: {e}\", exc_info=True)\n",
    "        # Optionally mark this group as failed in some way if needed later\n",
    "\n",
    "# --- End of HPO Loop ---\n",
    "hpo_loop_duration = time.time() - hpo_loop_start_time\n",
    "logging.info(f\"\\n{'='*30} Finished HPO for all groups in {hpo_loop_duration:.2f} seconds {'='*30}\")\n",
    "print(f\"\\nHPO loop finished. Best parameters saved in '{BEST_PARAMS_DIR}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90d844f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Feature Groups:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting feature set group 'group_1' with 8 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javym\\miniconda3\\envs\\phys-tfg\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "Processing Feature Groups: 100%|| 1/1 [01:06<00:00, 66.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Workflow loop finished. Results saved to 'results\\all_group_results_20250608_115202.json'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# Workflow Loop - Run Models with Best Parameters and Evaluate #\n",
    "################################################################\n",
    "\n",
    "# --- Configuration ---\n",
    "BEST_PARAMS_DIR = \"best_params\" # Directory where HPO results are saved\n",
    "RESULTS_DIR = \"results\" # Directory to save final results JSON\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "RESULTS_FILENAME = os.path.join(RESULTS_DIR, f\"all_group_results_{datetime.now():%Y%m%d_%H%M%S}.json\") # Timestamped results file\n",
    "\n",
    "# List of feature groups to process (should match HPO loop or be a subset)\n",
    "GROUP_NAMES_TO_PROCESS = list(groups.keys()) # Process all defined groups\n",
    "GROUP_NAMES_TO_PROCESS = ['group_1'] # Example: Process only specific groups\n",
    "\n",
    "# Dictionary to store results for all groups and models\n",
    "all_group_results = {}\n",
    "\n",
    "logging.info(f\"Starting main workflow loop for groups: {GROUP_NAMES_TO_PROCESS}\")\n",
    "outer_loop_start_time = time.time()\n",
    "\n",
    "# Use tqdm for the outer loop to show progress over groups\n",
    "for group_name in tqdm(GROUP_NAMES_TO_PROCESS, desc=\"Processing Feature Groups\"):\n",
    "    logging.info(f\"\\n{'='*30} Processing Feature Group: {group_name} {'='*30}\")\n",
    "    group_start_time = time.time()\n",
    "    group_results = {} # Store results for the current group\n",
    "    group_best_params = {} # To store loaded best params\n",
    "\n",
    "    # --- Load Best Params for the Group ---\n",
    "    params_filepath = os.path.join(BEST_PARAMS_DIR, f\"{group_name}_best_params.json\")\n",
    "    if os.path.exists(params_filepath):\n",
    "        try:\n",
    "            with open(params_filepath, 'r') as f:\n",
    "                group_best_params = json.load(f)\n",
    "            logging.info(f\"[{group_name}] Successfully loaded best parameters from {params_filepath}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] Failed to load best parameters from {params_filepath}: {e}. Proceeding without pre-tuned params.\", exc_info=True)\n",
    "            group_best_params = {} # Reset if loading fails\n",
    "    else:\n",
    "        logging.warning(f\"[{group_name}] Best parameters file not found: {params_filepath}. Workflows will run HPO internally.\")\n",
    "        group_best_params = {} # Ensure it's a dict\n",
    "\n",
    "    # Extract best params for each model, defaulting to None if not found/loaded\n",
    "    # The workflow functions are expected to handle None and run HPO if needed.\n",
    "    best_params_svm = group_best_params.get(model_name_svm, None)\n",
    "    best_params_cart = group_best_params.get(model_name_cart, None)\n",
    "    best_params_rf = group_best_params.get(model_name_rf, None)\n",
    "    best_params_xgb = group_best_params.get(model_name_xgb, None)\n",
    "    best_params_lgbm = group_best_params.get(model_name_lgbm, None)\n",
    "\n",
    "    try:\n",
    "        # --- 4.1 Data Preparation for the Current Group ---\n",
    "        logging.info(f\"[{group_name}] Cleaning data...\")\n",
    "        X, y, df_clean = clean_data(df, group_name, TARGET_COLUMN, logger=logging)\n",
    "\n",
    "        if X.empty or y.empty or df_clean.empty:\n",
    "            logging.warning(f\"[{group_name}] Skipping group due to insufficient data after cleaning (X: {X.shape}, y: {y.shape}).\")\n",
    "            all_group_results[group_name] = {'status': 'skipped_insufficient_data', 'results': group_results}\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"[{group_name}] Splitting data (Train/Val/Test/Cal)...\")\n",
    "        # Split into all necessary sets for training, validation (optional HPO and early stopping), calibration, and testing\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test, X_cal, y_cal = split_data(X, y)\n",
    "\n",
    "        if X_train.empty or y_train.empty or X_test.empty or y_test.empty:\n",
    "             logging.warning(f\"[{group_name}] Skipping group due to empty train or test set after splitting.\")\n",
    "             all_group_results[group_name] = {'status': 'skipped_empty_splits', 'results': group_results}\n",
    "             continue\n",
    "\n",
    "        # Check if calibration/validation sets are needed and non-empty\n",
    "        # Note: HPO might run internally if best_params were not loaded, requiring X_val/y_val\n",
    "        hpo_might_run = any(p is None for p in [best_params_svm, best_params_cart, best_params_rf, best_params_xgb, best_params_lgbm])\n",
    "        if (CALIBRATOR != 'none' and (X_cal.empty or y_cal.empty)):\n",
    "             logging.warning(f\"[{group_name}] Calibration set is empty, but CALIBRATOR is '{CALIBRATOR}'. Calibration will likely fail or be skipped.\")\n",
    "             # Decide whether to `continue` here based on strictness\n",
    "        if (hpo_might_run and (X_val.empty or y_val.empty)):\n",
    "             logging.warning(f\"[{group_name}] Validation set is empty, and HPO might run internally (missing best params). HPO will likely fail.\")\n",
    "             # Decide whether to `continue` here\n",
    "\n",
    "        logging.info(f\"[{group_name}] Applying feature scaling (for SVM)...\")\n",
    "        # Scale all necessary splits and save the scaler this time\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled, X_cal_scaled, scaler = apply_feature_scaling(\n",
    "            X_train, X_val, X_test, X_cal,\n",
    "            len(X_train) / len(X) if len(X) > 0 else 0, # Calculate actual proportions for logging inside the function\n",
    "            len(X_val) / len(X) if len(X) > 0 else 0,\n",
    "            len(X_test) / len(X) if len(X) > 0 else 0,\n",
    "            len(X_cal) / len(X) if len(X) > 0 else 0,\n",
    "            MODEL_DIR, # Pass MODEL_DIR for saving the scaler\n",
    "            save_scaler=True, # Ensure scaler is saved\n",
    "            group_name=group_name # Pass group name for potentially unique scaler filename\n",
    "        )\n",
    "\n",
    "        # --- 4.2 Run Workflows for the Current Group ---\n",
    "\n",
    "        # Workflow 1: SVM\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running SVM Workflow ---\")\n",
    "            svm_results = svm_workflow(\n",
    "                X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "                X_cal_scaled, y_cal, X_test_scaled, y_test,\n",
    "                MAX_RESOURCE_SVM, MIN_RESOURCE_SVM, ETA_SVM, RESOURCE_TYPE_SVM,\n",
    "                model_name_svm, CALIBRATOR,  RANDOM_SEED,\n",
    "                SVC=SVC, loguniform=loguniform, hyperband_hpo=hyperband_hpo, # Pass dependencies\n",
    "                f1_score=f1_score, logging=logging, # Pass dependencies\n",
    "                best_params_svm=best_params_svm, # Pass loaded/None params\n",
    "                best_score_hpo_svm=None, # HPO score not loaded, workflow calculates if needed\n",
    "                group = group_name,\n",
    "                model_dir = MODEL_DIR\n",
    "            )\n",
    "            group_results.update(svm_results if svm_results else {model_name_svm: {\"status\": \"failed\"}})\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] SVM Workflow failed: {e}\", exc_info=True)\n",
    "            group_results[model_name_svm] = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "        # Workflow 2: CART\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running CART Workflow ---\")\n",
    "            cart_results = cart_workflow(\n",
    "                X_train, y_train, X_val, y_val, X_cal, y_cal, X_test, y_test,\n",
    "                model_name_cart,\n",
    "                MAX_RESOURCE_CART, MIN_RESOURCE_CART, ETA_CART, RESOURCE_TYPE_CART,\n",
    "                RANDOM_SEED, CALIBRATOR, \n",
    "                DecisionTreeClassifier=DecisionTreeClassifier, randint=randint, # Pass dependencies\n",
    "                f1_score=f1_score, hyperband_hpo=hyperband_hpo, logging=logging, # Pass dependencies\n",
    "                best_params_cart=best_params_cart, # Pass loaded/None params\n",
    "                best_score_hpo_cart=None, # HPO score not loaded\n",
    "                group = group_name,\n",
    "                model_dir = MODEL_DIR\n",
    "            )\n",
    "            group_results.update(cart_results if cart_results else {model_name_cart: {\"status\": \"failed\"}})\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] CART Workflow failed: {e}\", exc_info=True)\n",
    "            group_results[model_name_cart] = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "        # Workflow 3: Random Forest\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running Random Forest Workflow ---\")\n",
    "            # Ensure arguments match the function definition (file_context_9)\n",
    "            rf_results = random_forest_workflow(\n",
    "                X_train, y_train, X_val, y_val, X_cal, y_cal, X_test, y_test,\n",
    "                model_name_rf, # model_name first\n",
    "                MAX_RESOURCE_RF, MIN_RESOURCE_RF, ETA_RF, RESOURCE_TYPE_RF, # HPO settings\n",
    "                CALIBRATOR,  RANDOM_SEED,\n",
    "                RandomForestClassifier=RandomForestClassifier, randint=randint, f1_score=f1_score, # Dependencies\n",
    "                hyperband_hpo=hyperband_hpo, train_calibrate_model=train_calibrate_model, # Dependencies\n",
    "                calculate_metrics=calculate_metrics, logging=logging, np=np, datetime=datetime, # Dependencies\n",
    "                best_params_rf=best_params_rf, # Pass loaded/None params\n",
    "                best_score_hpo_rf=None, # HPO score not loaded\n",
    "                group = group_name,\n",
    "                model_dir = MODEL_DIR\n",
    "            )\n",
    "            group_results.update(rf_results if rf_results else {model_name_rf: {\"status\": \"failed\"}})\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] Random Forest Workflow failed: {e}\", exc_info=True)\n",
    "            group_results[model_name_rf] = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "\n",
    "        # Workflow 4: XGBoost\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running XGBoost Workflow ---\")\n",
    "            # Ensure arguments match the function definition (file_context_4)\n",
    "            xgb_results = xgb_workflow(\n",
    "                X_train, y_train, X_val, y_val, X_cal, y_cal, X_test, y_test,\n",
    "                {}, # Pass empty dict for all_results initially, workflow should manage its own return\n",
    "                model_name_xgb,\n",
    "                MAX_RESOURCE_XGB, MIN_RESOURCE_XGB, ETA_XGB, RESOURCE_TYPE_XGB, ROUNDS,\n",
    "                CALIBRATOR,  RANDOM_SEED,\n",
    "                loguniform=loguniform, randint=randint, uniform=uniform, xgb=xgb, # Dependencies\n",
    "                hyperband_hpo=hyperband_hpo, f1_score=f1_score, EarlyStopping=EarlyStopping, # Dependencies\n",
    "                logging=logging, np=np, # Dependencies\n",
    "                best_params_xgb=best_params_xgb, # Pass loaded/None params\n",
    "                best_score_hpo_xgb=None, # HPO score not loaded\n",
    "                group = group_name,\n",
    "                model_dir = MODEL_DIR\n",
    "            )\n",
    "            group_results.update(xgb_results if xgb_results else {model_name_xgb: {\"status\": \"failed\"}})\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] XGBoost Workflow failed: {e}\", exc_info=True)\n",
    "            group_results[model_name_xgb] = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "\n",
    "        # Workflow 5: LightGBM\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running LightGBM Workflow ---\")\n",
    "            # Ensure arguments match the function definition (file_context_2)\n",
    "            lgbm_results = lgbm_workflow(\n",
    "                X_train, y_train, X_val, y_val, X_cal, y_cal, X_test, y_test,\n",
    "                model_name_lgbm,\n",
    "                MAX_RESOURCE_LGBM, MIN_RESOURCE_LGBM, ETA_LGBM, RESOURCE_TYPE_LGBM, ROUNDS,\n",
    "                CALIBRATOR,  RANDOM_SEED,\n",
    "                lgb=lgb, loguniform=loguniform, randint=randint, uniform=uniform, f1_score=f1_score, # Dependencies\n",
    "                hyperband_hpo=hyperband_hpo, early_stopping=early_stopping, train_calibrate_model=train_calibrate_model, # Dependencies\n",
    "                calculate_metrics=calculate_metrics, logging=logging, np=np, datetime=datetime, # Dependencies\n",
    "                best_params_lgbm=best_params_lgbm, # Pass loaded/None params\n",
    "                best_score_hpo_lgbm=None, # HPO score not loaded\n",
    "                group = group_name,\n",
    "                model_dir = MODEL_DIR\n",
    "            )\n",
    "            group_results.update(lgbm_results if lgbm_results else {model_name_lgbm: {\"status\": \"failed\"}})\n",
    "        except Exception as e:\n",
    "             logging.error(f\"[{group_name}] LightGBM Workflow failed: {e}\", exc_info=True)\n",
    "             group_results[model_name_lgbm] = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "\n",
    "        # --- 4.3 Store Results for the Group ---\n",
    "        all_group_results[group_name] = {'status': 'completed', 'results': group_results}\n",
    "        group_duration = time.time() - group_start_time\n",
    "        logging.info(f\"--- Finished processing Feature Group: {group_name} in {group_duration:.2f} seconds ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[{group_name}] Unhandled exception during workflow processing for group: {e}\", exc_info=True)\n",
    "        all_group_results[group_name] = {'status': 'failed_outer', 'error': str(e), 'results': group_results}\n",
    "        # Optionally `continue` or `break` depending on desired behavior\n",
    "\n",
    "# --- End of Workflow Loop ---\n",
    "outer_loop_duration = time.time() - outer_loop_start_time\n",
    "logging.info(f\"\\n{'='*30} Finished processing all groups in {outer_loop_duration:.2f} seconds {'='*30}\")\n",
    "\n",
    "# --- Serialize Final Results ---\n",
    "try:\n",
    "    # Define a helper function to convert non-standard dictionary keys recursively\n",
    "    def convert_keys_to_standard_types(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            new_dict = {}\n",
    "            for k, v in obj.items():\n",
    "                new_key = k\n",
    "                # Convert numpy integer keys to standard Python int\n",
    "                if isinstance(k, np.integer):\n",
    "                    new_key = int(k)\n",
    "                # Add conversions for other non-standard key types if needed\n",
    "                # elif isinstance(k, np.floating): new_key = float(k)\n",
    "                # elif not isinstance(k, (str, int, float, bool, type(None))): new_key = str(k)\n",
    "                new_dict[new_key] = convert_keys_to_standard_types(v) # Recurse on value\n",
    "            return new_dict\n",
    "        elif isinstance(obj, list):\n",
    "            # Recursively process items in lists\n",
    "            return [convert_keys_to_standard_types(item) for item in obj]\n",
    "        else:\n",
    "            # Return non-dict/list items as is\n",
    "            return obj\n",
    "\n",
    "    # Define a helper function to make result VALUES JSON serializable\n",
    "    def default_serializer(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            # Convert numpy integers to Python int\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            # Convert numpy floats to Python float, handle NaN/Inf\n",
    "            if np.isnan(obj): return None # Represent NaN as null\n",
    "            if np.isinf(obj): return None # Represent Inf as null\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            # Convert numpy arrays to lists\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, (datetime, pd.Timestamp)):\n",
    "             # Convert datetime/timestamp objects to ISO format string\n",
    "             return obj.isoformat()\n",
    "        elif isinstance(obj, pd.DataFrame):\n",
    "            # Serialize DataFrames (example: to dict with 'split' orientation)\n",
    "            try:\n",
    "                return obj.to_dict(orient='split')\n",
    "            except Exception:\n",
    "                return f\"DataFrame (shape {obj.shape}) - Not serialized\"\n",
    "        elif isinstance(obj, Exception): # Serialize exception objects to string\n",
    "            return f\"Error: {str(obj)}\"\n",
    "        # Fallback for other types: try converting to string\n",
    "        try:\n",
    "            return str(obj)\n",
    "        except Exception:\n",
    "            # If string conversion fails, represent as unserializable type\n",
    "            return f\"Unserializable type: {type(obj)}\"\n",
    "\n",
    "    # Convert keys in the results dictionary BEFORE dumping to JSON\n",
    "    serializable_results = convert_keys_to_standard_types(all_group_results)\n",
    "\n",
    "    # Dump the processed dictionary to JSON\n",
    "    with open(RESULTS_FILENAME, 'w') as f:\n",
    "        # Use the default_serializer for values; keys are now standard types\n",
    "        json.dump(serializable_results, f, indent=4, default=default_serializer)\n",
    "\n",
    "    logging.info(f\"Successfully saved all group results to {RESULTS_FILENAME}\")\n",
    "    print(f\"\\nWorkflow loop finished. Results saved to '{RESULTS_FILENAME}'.\")\n",
    "except Exception as e:\n",
    "    # Log the error and inform the user\n",
    "    logging.error(f\"Failed to serialize final results to {RESULTS_FILENAME}: {e}\", exc_info=True)\n",
    "    print(\"\\nWorkflow loop finished, but failed to save results to JSON.\")\n",
    "    # Ensure the in-memory variable name matches what's used later if needed\n",
    "    print(\"Results might be available in the 'all_group_results' dictionary variable in this session (keys might not be standard types).\")\n",
    "    print(\"Processed, serializable results might be available in 'serializable_results'.\")\n",
    "\n",
    "\n",
    "# The 'all_group_results' dictionary (and the saved JSON file) now holds the results.\n",
    "# The next cell can load the JSON file or use the dictionary directly to create summary tables or plots.\n",
    "# Example: Access results for SVM in group_1\n",
    "# loaded_results = {}\n",
    "# try:\n",
    "#     with open(RESULTS_FILENAME, 'r') as f:\n",
    "#         loaded_results = json.load(f)\n",
    "#     print(loaded_results.get('group_1', {}).get('results', {}).get(model_name_svm))\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading results file: {e}\")\n",
    "#     print(\"Using in-memory results:\")\n",
    "#     print(all_group_results.get('group_1', {}).get('results', {}).get(model_name_svm))\n",
    "\n",
    "# The next cell should contain the code to parse 'all_group_results' (or loaded results)\n",
    "# and display the final summary DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7047b6c",
   "metadata": {},
   "source": [
    "### 6. Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b3809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Performance Metrics Summary (All Groups) =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall_tpr</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>specificity_tnr</th>\n",
       "      <th>g_mean</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>brier_score</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>HPO F1</th>\n",
       "      <th>HPO Duration (s)</th>\n",
       "      <th>Final Estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>group_1</td>\n",
       "      <td>CART</td>\n",
       "      <td>0.9297</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7776</td>\n",
       "      <td>0.6489</td>\n",
       "      <td>0.0467</td>\n",
       "      <td>5093.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>385.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>group_1</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>group_1</td>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>group_1</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.9556</td>\n",
       "      <td>0.7336</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.6473</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.7550</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.6567</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>5012.0000</td>\n",
       "      <td>81.0000</td>\n",
       "      <td>162.0000</td>\n",
       "      <td>223.0000</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>group_1</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group          Model  accuracy  precision  recall_tpr  f1_score  \\\n",
       "0  group_1           CART    0.9297     0.0000      0.0000    0.0000   \n",
       "1  group_1       LightGBM       NaN        NaN         NaN       NaN   \n",
       "2  group_1  Random_Forest       NaN        NaN         NaN       NaN   \n",
       "3  group_1            SVM    0.9556     0.7336      0.5792    0.6473   \n",
       "4  group_1        XGBoost       NaN        NaN         NaN       NaN   \n",
       "\n",
       "   specificity_tnr  g_mean  roc_auc  pr_auc  brier_score        TN      FP  \\\n",
       "0           1.0000  0.0000   0.7776  0.6489       0.0467 5093.0000  0.0000   \n",
       "1              NaN     NaN      NaN     NaN          NaN       NaN     NaN   \n",
       "2              NaN     NaN      NaN     NaN          NaN       NaN     NaN   \n",
       "3           0.9841  0.7550   0.8929  0.6567       0.0413 5012.0000 81.0000   \n",
       "4              NaN     NaN      NaN     NaN          NaN       NaN     NaN   \n",
       "\n",
       "        FN       TP HPO F1  HPO Duration (s) Final Estimators  \n",
       "0 385.0000   0.0000   None            0.0000              N/A  \n",
       "1      NaN      NaN   None               NaN              N/A  \n",
       "2      NaN      NaN   None               NaN              N/A  \n",
       "3 162.0000 223.0000   None            0.0000              N/A  \n",
       "4      NaN      NaN   None               NaN              N/A  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shortened floats in the JSON string above for brevity, use your full precision data\n",
    "# Load the JSON file from disk, not as a raw string\n",
    "with open(r\"results\\all_group_results_20250608_115202.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_group_results = json.load(f)\n",
    "\n",
    "# --- Modified Processing Logic for all_group_results ---\n",
    "results_summary_all_groups = []\n",
    "\n",
    "# Outer loop: Iterate through each group (e.g., group_1, group_2)\n",
    "for group_name, group_data in all_group_results.items():\n",
    "    status = group_data.get(\"status\")\n",
    "    all_results_for_group = group_data.get(\"results\") # This is the dict for models in this group\n",
    "\n",
    "    if status != \"completed\":\n",
    "        logging.warning(f\"Skipping group '{group_name}' due to status: {status}\")\n",
    "        continue # Skip to the next group if this one didn't complete\n",
    "\n",
    "    if not all_results_for_group:\n",
    "        logging.warning(f\"Skipping group '{group_name}': No 'results' data found even though status is completed.\")\n",
    "        continue # Skip if results dict is missing\n",
    "\n",
    "    # Inner loop: Iterate through each model's results within the current group\n",
    "    # This part is similar to your original code, but operates on all_results_for_group\n",
    "    for model_name, results_data in all_results_for_group.items():\n",
    "        # Start summary dict, adding the Group identifier\n",
    "        summary = {'Group': group_name, 'Model': model_name}\n",
    "\n",
    "        metrics = results_data.get('metrics')\n",
    "        if metrics:\n",
    "            # Add all metrics first\n",
    "            summary.update(metrics)\n",
    "            # Pop confusion matrix and extract TN, FP, FN, TP\n",
    "            cm = summary.pop('confusion_matrix', None)\n",
    "            if cm:\n",
    "                summary['TN'] = cm.get('tn')\n",
    "                summary['FP'] = cm.get('fp')\n",
    "                summary['FN'] = cm.get('fn')\n",
    "                summary['TP'] = cm.get('tp')\n",
    "            else:\n",
    "                 # Ensure these columns exist even if CM is missing\n",
    "                 summary['TN'] = None\n",
    "                 summary['FP'] = None\n",
    "                 summary['FN'] = None\n",
    "                 summary['TP'] = None\n",
    "\n",
    "        summary['HPO F1'] = results_data.get('hpo_f1_score')\n",
    "        summary['HPO Duration (s)'] = results_data.get('hpo_duration_s')\n",
    "        summary['Final Estimators'] = results_data.get('final_n_estimators', 'N/A')\n",
    "\n",
    "        # --- Optional: Add best HPO parameters as a string column ---\n",
    "        # best_params = results_data.get('best_hpo_params')\n",
    "        # summary['Best HPO Params'] = str(best_params) if best_params else 'N/A'\n",
    "\n",
    "        results_summary_all_groups.append(summary)\n",
    "    # End inner loop (models)\n",
    "# End outer loop (groups)\n",
    "\n",
    "# --- Create the final DataFrame from the list of summaries ---\n",
    "results_df = pd.DataFrame(results_summary_all_groups)\n",
    "\n",
    "# Set display options for float formatting\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}' if isinstance(x, float) else x)\n",
    "\n",
    "print(\"\\n===== Performance Metrics Summary (All Groups) =====\")\n",
    "if not results_df.empty:\n",
    "    # Sort by Group and Model for better organization before displaying/saving\n",
    "    results_df = results_df.sort_values(by=['Group', 'Model']).reset_index(drop=True)\n",
    "\n",
    "    # Ensure all columns and all rows are displayed\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        display(results_df)\n",
    "else:\n",
    "    print(\"No results to display.\")\n",
    "\n",
    "# --- Update CSV saving ---\n",
    "# Use a filename indicating it contains results from all groups\n",
    "results_csv_path = os.path.join(\".\", f\"model_comparison_summary_all_groups_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "try:\n",
    "    # Save the sorted DataFrame\n",
    "    results_df.to_csv(results_csv_path, index=False, float_format='%.6f') # Control float precision in CSV\n",
    "    logging.info(f\"Results summary DataFrame saved to {results_csv_path}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to save results summary CSV: {e}\")\n",
    "\n",
    "# --- Optional Plotting (Needs adaptation for multi-group data) ---\n",
    "# The original plotting code for 'CP Empty %' etc. would need to be\n",
    "# significantly changed to handle multiple groups (e.g., group by model,\n",
    "# plot average across groups, or create separate plots per group).\n",
    "# Keeping it commented out as per your original code's structure.\n",
    "logging.warning(\"Plotting code for CP set types is commented out and may need adaptation for multi-group data.\")\n",
    "# if not results_df.empty and 'CP Empty Sets' in results_df.columns and 'CP Multi-Class Sets' in results_df.columns:\n",
    "#    ... (adapt plotting code here) ...\n",
    "# else:\n",
    "#    logging.warning(\"Could not plot CP set types: Results DataFrame is empty or missing required columns.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys-tfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
