{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter nbconvert --to script Model.ipynb\n",
    "\n",
    "\n",
    "#TODO: Añadir las métricas que voy a usar finales\n",
    "#TODO: Platt scaling\n",
    "#TODO: Hyperband\n",
    "#TODO: Corregir los logs\n",
    "#TODO: Conformal prediction\n",
    "\n",
    "# Optional with Shapely or sth like that?\n",
    "#TODO: Estudiar relevancia de las features para cada modelo y cada grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any existing log files\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "# Reset logger to avoid any issues with permissions\n",
    "logging.shutdown()\n",
    "# Remove loggers\n",
    "for log_file in glob.glob(\"*.log\"):\n",
    "    os.remove(log_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star-Galaxy Classification using ALHAMBRA Photometry\n",
    "\n",
    "This notebook implements and evaluates several machine learning models for classifying astronomical objects as stars or galaxies based on multi-band photometric data from the ALHAMBRA survey, using labels derived from higher-resolution COSMOS2020 data.\n",
    "\n",
    "**Target Variable:** `acs_mu_class` (from COSMOS2020)\n",
    " - Which is 1 for Galaxy and 2 for Star. We will remap this to 0 (Galaxy, majority class) and 1 (Star, minority class).\n",
    "\n",
    "**Features:** Selected columns from the ALHAMBRA survey data.\n",
    "\n",
    "**Models:**\n",
    "1. Support Vector Machine (SVM)\n",
    "2. Decision Tree (CART)\n",
    "3. Random Forest\n",
    "4. XGBoost\n",
    "5. LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "from tqdm.notebook import tqdm # Use notebook tqdm for better integration in Jupyter\n",
    "import time\n",
    "from datetime import datetime\n",
    "import joblib # For saving/loading models efficiently\n",
    "import glob\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV,  ParameterSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import loguniform # For hyperparameter distributions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support, roc_auc_score,\n",
    "    brier_score_loss, precision_recall_curve, auc\n",
    ")   \n",
    "import seaborn as sns # For confusion matrix heatmap\n",
    "\n",
    "# Boosting models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.shutdown()\n",
    "logging.basicConfig(\n",
    "    filename=f'models_{datetime.now().strftime(\"%d_%H-%M-%S\")}.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True\n",
    ")\n",
    "# Prevent logs from being printed to console\n",
    "logging.getLogger().handlers = [h for h in logging.getLogger().handlers if isinstance(h, logging.FileHandler)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Dataset & Feature Selection\n",
    "\n",
    "**Interesting Feature Combinations for Modeling:**\n",
    " \n",
    " The feature groups are defined as follows:\n",
    " - Group 1: Morphology features and their uncertainties\n",
    " - Group 2: Photometry magnitudes\n",
    " - Group 3: Photometry magnitude and errors\n",
    " - Group 4: Redshift features and their uncertainties\n",
    " - Group 5: Combination of photometry magnitude errors and morphology features (including uncertainties)\n",
    " - Group 6: Combination of photometry magnitude errors, morphology features (including uncertainties), and redshift features (including uncertainties)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the df\n",
    "df = pd.read_csv('data/match_alhambra_cosmos2020_ACS_class_0.8arcsec.csv')\n",
    "logging.info(f\"DataFrame created with shape: {df.shape}\")\n",
    "# Map ACS classification: 1 (Galaxy, Majority) -> 0, 2 (Star, minority) -> 1, 3 (Fake) -> drop\n",
    "logging.info(\"Original class counts:\")\n",
    "logging.info(df['acs_mu_class'].value_counts().to_string())\n",
    "\n",
    "# Drop fake detections (class 3)\n",
    "# Drop fake detections\n",
    "n_fakes = (df['acs_mu_class'] == 3).sum()\n",
    "logging.info(f\"Number of fake detections (class 3): {n_fakes}\")\n",
    "df = df[df['acs_mu_class'] != 3]\n",
    "\n",
    "# Map classifications\n",
    "df['acs_mu_class'] = df['acs_mu_class'].map({1: 0, 2: 1})\n",
    "\n",
    "logging.info(\"After dropping fakes and mapping classes (0: Galaxy, 1: Star):\")\n",
    "logging.info(df['acs_mu_class'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features\n",
    "\n",
    "# --- Define feature categories based on ALHAMBRA data using exact names ---\n",
    "\n",
    "# 1. ALHAMBRA Morphology Features (SExtractor-based)\n",
    "morphology_features = [\n",
    "    'area', 'fwhm', 'stell', 'ell', 'a', 'b', 'theta', 'rk', 'rf'\n",
    "]\n",
    "\n",
    "morphology_err = [\n",
    "    's2n'\n",
    "]\n",
    "\n",
    "morphology_mags_errors = morphology_features + morphology_err\n",
    "\n",
    "# 2. ALHAMBRA Photometry Magnitudes (Optical + NIR + Synthetic)\n",
    "OPTICAL_MAG_COLS = [\n",
    "    'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W',\n",
    "    'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W',\n",
    "    'F799W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W'\n",
    "]\n",
    "photometry_magnitudes = (\n",
    "    OPTICAL_MAG_COLS +\n",
    "    ['J', 'H', 'KS', 'F814W']\n",
    ")\n",
    "\n",
    "# 3. ALHAMBRA Photometry Uncertainties\n",
    "OPTICAL_ERR_COLS = [\n",
    "    'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W',\n",
    "    'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W',\n",
    "    'dF799W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W'\n",
    "]\n",
    "photometry_uncertainties = (\n",
    "    OPTICAL_ERR_COLS +\n",
    "    ['dJ', 'dH', 'dKS', 'dF814W']\n",
    ")\n",
    "\n",
    "photometry_mags_errors = photometry_magnitudes + photometry_uncertainties\n",
    "\n",
    "# 4. ALHAMBRA Photometric Redshift & Derived Features (BPZ-based)\n",
    "redshift_features = [\n",
    "    'zb_1', 'zb_Min_1', 'zb_Max_1', 'Tb_1',\n",
    "    'z_ml', 't_ml',\n",
    "    'Stell_Mass_1', 'M_Abs_1', 'MagPrior'\n",
    "]\n",
    "\n",
    "redshift_uncertainties = [\n",
    "    'Odds_1', 'Chi2'\n",
    "]\n",
    "\n",
    "\n",
    "redshift_mags_errors = redshift_features + redshift_uncertainties\n",
    "\n",
    "# 5. ALHAMBRA Quality/Auxiliary Features (per-band quality etc.)\n",
    "OPTICAL_IRMS_COLS = [\n",
    "    'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W',\n",
    "    'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W',\n",
    "    'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W',\n",
    "    'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W'\n",
    "]\n",
    "quality_aux_features = (\n",
    "    ['nfobs'] +\n",
    "    OPTICAL_IRMS_COLS +\n",
    "    ['irms_J', 'irms_H', 'irms_KS', 'irms_F814W']\n",
    ")\n",
    "\n",
    "# --- Define lists of features NOT used for modeling ---\n",
    "\n",
    "non_modeling_identifiers = ['ID_1', 'id_2'] # ALHAMBRA ID, COSMOS ID\n",
    "\n",
    "non_modeling_astrometry = [\n",
    "    'RA_1', 'Dec_1', 'x', 'y', # ALHAMBRA Astrometry\n",
    "    'ra_2', 'dec_2',          # COSMOS Astrometry\n",
    "    'Separation'              # Matching Quality\n",
    "]\n",
    "\n",
    "non_modeling_flags = [\n",
    "    'photoflag', 'xray', 'PercW', 'Satur_Flag', # ALHAMBRA Object/Photometry Flags\n",
    "    'irms_OPT_Flag', 'irms_NIR_Flag'           # ALHAMBRA Overall Quality Flags\n",
    "]\n",
    "\n",
    "alhambra_prediction = ['Stellar_Flag'] # ALHAMBRA's own classification\n",
    "\n",
    "non_modeling_aperture_mags = [ # Specific aperture mags, usually use total mags\n",
    "    'F814W_3arcs', 'dF814W_3arcs', 'F814W_3arcs_corr'\n",
    "]\n",
    "\n",
    "non_modeling_cosmos_features = [ # Measurements/flags derived from COSMOS data (HST, HSC, VISTA...)\n",
    "    'model_flag',\n",
    "    'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista',\n",
    "    'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid',\n",
    "    'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid',\n",
    "    'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid',\n",
    "    'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid',\n",
    "    'acs_f814w_mag', 'acs_f814w_magerr',\n",
    "    'acs_fwhm_world', 'acs_mu_max',\n",
    "    'solution_model' # This is categorical, but still COSMOS-derived info\n",
    "]\n",
    "\n",
    "target_variable = ['acs_mu_class'] # The COSMOS classification label to predict\n",
    "\n",
    "##########################################################################################\n",
    "#! --- Consolidate into the main dictionary for easy access ---\n",
    "##########################################################################################\n",
    "\n",
    "feature_sets = {\n",
    "        # --- Potential Input Feature Sets ---\n",
    "        'morphology_only': morphology_mags_errors,\n",
    "        'photometry_magnitudes_only': photometry_magnitudes,\n",
    "        'photometry_mags_errors': photometry_mags_errors,\n",
    "        'photometry_plus_morphology': photometry_mags_errors + morphology_mags_errors,\n",
    "        'photometry_no_redshift': photometry_mags_errors + morphology_mags_errors + quality_aux_features,\n",
    "        'redshift_only': redshift_mags_errors,\n",
    "        'full_alhambra_all': (morphology_mags_errors +\n",
    "                            photometry_mags_errors +\n",
    "                            redshift_mags_errors + \n",
    "                            quality_aux_features),\n",
    "\n",
    "        # --- Excluded Feature Sets ---\n",
    "        'non_modeling_identifiers': non_modeling_identifiers,\n",
    "        'non_modeling_astrometry': non_modeling_astrometry,\n",
    "        'non_modeling_flags': non_modeling_flags,\n",
    "        'non_modeling_aperture_mags': non_modeling_aperture_mags,\n",
    "        'non_modeling_cosmos_features': non_modeling_cosmos_features,\n",
    "        'alhambra_prediction': alhambra_prediction,\n",
    "        'target_variable': target_variable\n",
    "    }\n",
    "\n",
    "#! This is excluding the quality aux.\n",
    "# Include target_variable in each group by appending it to the feature list\n",
    "groups = {\n",
    "        'group_1': feature_sets.get('morphology_only', []) + feature_sets.get('target_variable', []),\n",
    "        'group_2': feature_sets.get('photometry_magnitudes_only', []) + feature_sets.get('target_variable', []),\n",
    "        'group_3': feature_sets.get('photometry_mags_errors', []) + feature_sets.get('target_variable', []),\n",
    "        'group_4': feature_sets.get('redshift_only', []) + feature_sets.get('target_variable', []),\n",
    "        'group_5': feature_sets.get('photometry_plus_morphology', []) + feature_sets.get('target_variable', []),\n",
    "        'group_6': (feature_sets.get('photometry_mags_errors', []) +\n",
    "                   feature_sets.get('morphology_only', []) +\n",
    "                   feature_sets.get('redshift_only', []) +\n",
    "                   feature_sets.get('target_variable', [])),\n",
    "        'group_7': feature_sets.get('full_alhambra_all', []) + feature_sets.get('target_variable', [])\n",
    "    }\n",
    "\n",
    "# --- Function to get a specific feature set (Unchanged from before) ---\n",
    "\n",
    "def get_feature_set(df, set_name, groups = groups):\n",
    "    \"\"\"\n",
    "    Selects columns from a DataFrame based on a predefined feature set name,\n",
    "    including six specific groups defined by combinations of morphology,\n",
    "    photometry magnitudes, uncertainties, and redshift features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        set_name (str): The name of the desired feature set group:\n",
    "                        'group_1' to 'group_6' as defined below.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing only the columns\n",
    "                      belonging to the specified feature set group.\n",
    "                      Returns an empty DataFrame if no columns are found.\n",
    "    \"\"\"\n",
    "\n",
    "    if set_name not in groups:\n",
    "        raise ValueError(f\"Feature set group '{set_name}' not defined. \"\n",
    "                         f\"Available groups: {list(groups.keys())}\")\n",
    "\n",
    "    required_cols_in_set = groups[set_name]\n",
    "\n",
    "    # Find which of these columns actually exist in the DataFrame\n",
    "    available_cols = [col for col in required_cols_in_set if col in df.columns]\n",
    "\n",
    "    # Warn if some columns from the set definition are missing\n",
    "    missing_cols = [col for col in required_cols_in_set if col not in available_cols]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: The following columns defined for feature set group '{set_name}'\"\n",
    "              f\" were not found in the DataFrame and will be excluded: {missing_cols}\")\n",
    "\n",
    "    if not available_cols:\n",
    "        print(f\"Warning: No columns for feature set group '{set_name}' found in the DataFrame.\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "\n",
    "    print(f\"Selecting feature set group '{set_name}' with {len(available_cols)} columns.\")\n",
    "    return df[available_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All df columns are included in feature_sets.\n",
      "\n",
      "=== group_1 ===\n",
      "Selecting feature set group 'group_1' with 11 columns.\n",
      "\n",
      "Features present (11 columns):\n",
      "['a', 'acs_mu_class', 'area', 'b', 'ell', 'fwhm', 'rf', 'rk', 's2n', 'stell', 'theta']\n",
      "\n",
      "Features missing (125 columns):\n",
      "['Chi2', 'Dec_1', 'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F814W_3arcs', 'F814W_3arcs_corr', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'ID_1', 'J', 'KS', 'M_Abs_1', 'MagPrior', 'Odds_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stell_Mass_1', 'Stellar_Flag', 'Tb_1', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF814W_3arcs', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'dec_2', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'solution_model', 't_ml', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "=== group_2 ===\n",
      "Selecting feature set group 'group_2' with 25 columns.\n",
      "\n",
      "Features present (25 columns):\n",
      "['F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'acs_mu_class']\n",
      "\n",
      "Features missing (111 columns):\n",
      "['Chi2', 'Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'M_Abs_1', 'MagPrior', 'Odds_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stell_Mass_1', 'Stellar_Flag', 'Tb_1', 'a', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF814W_3arcs', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'dec_2', 'ell', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'fwhm', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'rf', 'rk', 's2n', 'solution_model', 'stell', 't_ml', 'theta', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "=== group_3 ===\n",
      "Selecting feature set group 'group_3' with 49 columns.\n",
      "\n",
      "Features present (49 columns):\n",
      "['F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'acs_mu_class', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS']\n",
      "\n",
      "Features missing (87 columns):\n",
      "['Chi2', 'Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'M_Abs_1', 'MagPrior', 'Odds_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stell_Mass_1', 'Stellar_Flag', 'Tb_1', 'a', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'area', 'b', 'dF814W_3arcs', 'dec_2', 'ell', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'fwhm', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'rf', 'rk', 's2n', 'solution_model', 'stell', 't_ml', 'theta', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "=== group_4 ===\n",
      "Selecting feature set group 'group_4' with 12 columns.\n",
      "\n",
      "Features present (12 columns):\n",
      "['Chi2', 'M_Abs_1', 'MagPrior', 'Odds_1', 'Stell_Mass_1', 'Tb_1', 'acs_mu_class', 't_ml', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "Features missing (124 columns):\n",
      "['Dec_1', 'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F814W_3arcs', 'F814W_3arcs_corr', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'ID_1', 'J', 'KS', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stellar_Flag', 'a', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF814W_3arcs', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'dec_2', 'ell', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'fwhm', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'rf', 'rk', 's2n', 'solution_model', 'stell', 'theta', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y']\n",
      "\n",
      "=== group_5 ===\n",
      "Selecting feature set group 'group_5' with 59 columns.\n",
      "\n",
      "Features present (59 columns):\n",
      "['F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'a', 'acs_mu_class', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'ell', 'fwhm', 'rf', 'rk', 's2n', 'stell', 'theta']\n",
      "\n",
      "Features missing (77 columns):\n",
      "['Chi2', 'Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'M_Abs_1', 'MagPrior', 'Odds_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stell_Mass_1', 'Stellar_Flag', 'Tb_1', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'dF814W_3arcs', 'dec_2', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'solution_model', 't_ml', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "=== group_6 ===\n",
      "Selecting feature set group 'group_6' with 70 columns.\n",
      "\n",
      "Features present (70 columns):\n",
      "['Chi2', 'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'M_Abs_1', 'MagPrior', 'Odds_1', 'Stell_Mass_1', 'Tb_1', 'a', 'acs_mu_class', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'ell', 'fwhm', 'rf', 'rk', 's2n', 'stell', 't_ml', 'theta', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "Features missing (66 columns):\n",
      "['Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stellar_Flag', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'dF814W_3arcs', 'dec_2', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'solution_model', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y']\n",
      "\n",
      "=== group_7 ===\n",
      "Selecting feature set group 'group_7' with 95 columns.\n",
      "\n",
      "Features present (95 columns):\n",
      "['Chi2', 'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'M_Abs_1', 'MagPrior', 'Odds_1', 'Stell_Mass_1', 'Tb_1', 'a', 'acs_mu_class', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'ell', 'fwhm', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'nfobs', 'rf', 'rk', 's2n', 'stell', 't_ml', 'theta', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "Features missing (41 columns):\n",
      "['Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stellar_Flag', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'dF814W_3arcs', 'dec_2', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'photoflag', 'ra_2', 'solution_model', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y']\n"
     ]
    }
   ],
   "source": [
    "# Quality check to see which cols are excluded and contained in each group\n",
    "all_feature_cols = set()\n",
    "for cols in feature_sets.values():\n",
    "    all_feature_cols.update(cols)\n",
    "\n",
    "df_cols_set = set(df.columns)\n",
    "not_in_feature_sets = df_cols_set - all_feature_cols\n",
    "\n",
    "if not_in_feature_sets:\n",
    "    print(f\"Columns in df not included in any feature_sets: {sorted(not_in_feature_sets)}\")\n",
    "else:\n",
    "    print(\"All df columns are included in feature_sets.\")\n",
    "\n",
    "\n",
    "# Check which columns are in each feature group\n",
    "for group_name in ['group_1', 'group_2', 'group_3', 'group_4', 'group_5', 'group_6', 'group_7']:\n",
    "    print(f\"\\n=== {group_name} ===\")\n",
    "    \n",
    "    # Get the feature set definition\n",
    "    feature_set = groups[group_name]\n",
    "    \n",
    "    # Get the actual columns that exist in the data\n",
    "    group_df = get_feature_set(df, group_name)\n",
    "    \n",
    "\n",
    "    available_cols = list(group_df.columns)\n",
    "    \n",
    "    # Find columns that are defined but not in the data\n",
    "    missing_cols = [col for col in list(df.columns) if col not in feature_set]\n",
    "    \n",
    "    print(f\"\\nFeatures present ({len(available_cols)} columns):\")\n",
    "    print(list(sorted(available_cols)))\n",
    "    \n",
    "    print(f\"\\nFeatures missing ({len(missing_cols)} columns):\")\n",
    "    print(list(sorted(missing_cols)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting parameters\n",
    "TEST_SIZE = 0.20 # Test set proportion\n",
    "VAL_SIZE = 0.10 # Validation set proportion\n",
    "CAL_SIZE = 0.10 # Calibration set proportion\n",
    "# Train size will be 1 - (TEST_SIZE + VAL_SIZE + CAL_SIZE)\n",
    "\n",
    "TARGET_COLUMN = feature_sets.get('target_variable', [])[0]\n",
    "RANDOM_SEED = 33 # For reproducibility\n",
    "\n",
    "# Model saving directory\n",
    "MODEL_DIR = \"trained_models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Data splitting strategy ('stratified' or 'random')\n",
    "SPLIT_STRATEGY = 'stratified' # Recommended for imbalanced datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting feature set group 'group_7' with 95 columns.\n"
     ]
    }
   ],
   "source": [
    "# --- Data Cleaning ---\n",
    "logging.info(f\"Original dataset size: {df.shape}\")\n",
    "\n",
    "# Choose the feature group to use (e.g., 'group_1', 'group_2', etc.)\n",
    "FEATURE_GROUP = 'group_7'  # Change this to select a different group\n",
    "\n",
    "# Get the feature columns for the selected group using get_feature_set\n",
    "df_clean = get_feature_set(df, FEATURE_GROUP).dropna().copy()\n",
    "logging.info(f\"Dataset size after dropping NaNs: {df_clean.shape}\")\n",
    "\n",
    "# Ensure TARGET_COLUMN is defined correctly\n",
    "if TARGET_COLUMN not in df_clean.columns:\n",
    "    raise KeyError(f\"Target column '{TARGET_COLUMN}' not found in the cleaned DataFrame columns: {df_clean.columns.tolist()}\")\n",
    "\n",
    "# Log value counts for target\n",
    "logging.info(f\"Value counts for target:\\n1 (Star): {(df_clean[TARGET_COLUMN] == 1).sum()}\\n0 (Galaxy): {(df_clean[TARGET_COLUMN] == 0).sum()}\")\n",
    "\n",
    "# Separate features (X) and target (y) for the cleaned DataFrame\n",
    "X = df_clean.drop(columns=[TARGET_COLUMN])\n",
    "y = df_clean[TARGET_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Splitting ---\n",
    "import numpy as np # Ensure numpy is imported\n",
    "from sklearn.model_selection import train_test_split # Ensure train_test_split is imported\n",
    "\n",
    "logging.info(f\"Splitting data using '{SPLIT_STRATEGY}' strategy...\")\n",
    "\n",
    "# --- Validate Proportions ---\n",
    "if not (0 <= TEST_SIZE <= 1 and 0 <= VAL_SIZE <= 1 and 0 <= CAL_SIZE <= 1):\n",
    "     raise ValueError(\"Split proportions (TEST_SIZE, VAL_SIZE, CAL_SIZE) must be between 0 and 1.\")\n",
    "\n",
    "TRAIN_SIZE = 1.0 - TEST_SIZE - VAL_SIZE - CAL_SIZE\n",
    "if not (0 <= TRAIN_SIZE <= 1):\n",
    "     raise ValueError(f\"Calculated TRAIN_SIZE ({TRAIN_SIZE:.3f}) is invalid. Sum of TEST_SIZE, VAL_SIZE, and CAL_SIZE must be between 0 and 1.\")\n",
    "\n",
    "if not np.isclose(TRAIN_SIZE + TEST_SIZE + VAL_SIZE + CAL_SIZE, 1.0):\n",
    "    # This check might be redundant given the calculation of TRAIN_SIZE, but good for safety.\n",
    "    raise ValueError(\"Sum of split proportions must be equal to 1.\")\n",
    "\n",
    "if np.isclose(TRAIN_SIZE, 0) and (np.isclose(VAL_SIZE, 0) or np.isclose(TEST_SIZE, 0) or np.isclose(CAL_SIZE, 0)):\n",
    "     # Avoid scenarios where train is 0 but other splits are also 0, leading to ambiguity.\n",
    "     # If only train is 0, it might be valid in some rare cases, but usually requires at least one other non-zero split.\n",
    "     # Let's enforce Train > 0 for typical ML workflows.\n",
    "     # If you need zero training data, adjust this check.\n",
    "     logging.warning(\"TRAIN_SIZE is zero or near zero. Ensure this is intended.\")\n",
    "     if TRAIN_SIZE < 0: # Definitely an error\n",
    "         raise ValueError(\"TRAIN_SIZE cannot be negative.\")\n",
    "     # Allow TRAIN_SIZE = 0 only if explicitly handled later, otherwise raise error?\n",
    "     # For now, let's proceed but log a warning. If TRAIN_SIZE must be > 0, uncomment the raise below.\n",
    "     # raise ValueError(\"TRAIN_SIZE must be greater than 0 for typical model training.\")\n",
    "\n",
    "\n",
    "logging.info(f\"Target split ratios: Train={TRAIN_SIZE:.2f}, Val={VAL_SIZE:.2f}, Test={TEST_SIZE:.2f}, Cal={CAL_SIZE:.2f}\")\n",
    "\n",
    "# --- Initialize Splits ---\n",
    "# Use iloc[0:0] to create empty DataFrames/Series with the same columns/dtype\n",
    "empty_X = X.iloc[0:0]\n",
    "empty_y = y.iloc[0:0]\n",
    "X_train, y_train = empty_X.copy(), empty_y.copy()\n",
    "X_val, y_val = empty_X.copy(), empty_y.copy()\n",
    "X_test, y_test = empty_X.copy(), empty_y.copy()\n",
    "X_cal, y_cal = empty_X.copy(), empty_y.copy()\n",
    "\n",
    "# Temporary variables for sequential splitting\n",
    "X_remaining, y_remaining = X.copy(), y.copy() # Use copies to avoid modifying original X, y\n",
    "\n",
    "# --- Stratification Option ---\n",
    "# Define stratify_func only once\n",
    "def get_stratify_array(y_arr):\n",
    "    return y_arr if SPLIT_STRATEGY == 'stratified' and not y_arr.empty else None\n",
    "\n",
    "# --- First Split: Train vs. Remainder (Val + Test + Cal) ---\n",
    "val_test_cal_size = VAL_SIZE + TEST_SIZE + CAL_SIZE\n",
    "\n",
    "if np.isclose(val_test_cal_size, 0): # Only Train set needed\n",
    "    X_train, y_train = X_remaining, y_remaining\n",
    "    logging.info(\"All data assigned to Train set (Val, Test, Cal sizes are 0).\")\n",
    "    X_remaining, y_remaining = empty_X.copy(), empty_y.copy() # No remainder\n",
    "elif np.isclose(TRAIN_SIZE, 0): # No Train set needed\n",
    "    logging.info(\"Train set is empty (TRAIN_SIZE=0). Remainder passed to next splits.\")\n",
    "    # X_remaining, y_remaining already hold all data\n",
    "else: # Split Train vs Remainder\n",
    "    split_test_size = val_test_cal_size # Proportion of remainder relative to total (1.0)\n",
    "    X_train, X_remaining, y_train, y_remaining = train_test_split(\n",
    "        X_remaining, y_remaining,\n",
    "        test_size=split_test_size,\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=get_stratify_array(y_remaining)\n",
    "    )\n",
    "logging.info(f\"Train set shape: {X_train.shape}\")\n",
    "\n",
    "\n",
    "# --- Second Split: Val vs. Remainder (Test + Cal) ---\n",
    "if not X_remaining.empty:\n",
    "    test_cal_size = TEST_SIZE + CAL_SIZE\n",
    "    # Denominator for relative size calculation: size of the current remaining pool\n",
    "    current_remaining_size_frac = VAL_SIZE + test_cal_size # = val_test_cal_size\n",
    "\n",
    "    if np.isclose(VAL_SIZE, 0): # No Val set, pass remainder to next stage\n",
    "        X_temp2, y_temp2 = X_remaining, y_remaining # Remainder is Test + Cal\n",
    "        logging.info(\"Validation set is empty (VAL_SIZE=0).\")\n",
    "    elif np.isclose(test_cal_size, 0): # Only Val set left in remainder\n",
    "        X_val, y_val = X_remaining, y_remaining\n",
    "        X_temp2, y_temp2 = empty_X.copy(), empty_y.copy() # No data left for Test/Cal\n",
    "        logging.info(f\"Validation set shape: {X_val.shape}\")\n",
    "    else: # Split Val vs (Test + Cal)\n",
    "        # Proportion of (Test + Cal) relative to (Val + Test + Cal)\n",
    "        split_test_size = test_cal_size / current_remaining_size_frac\n",
    "        X_val, X_temp2, y_val, y_temp2 = train_test_split(\n",
    "            X_remaining, y_remaining,\n",
    "            test_size=split_test_size,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=get_stratify_array(y_remaining)\n",
    "        )\n",
    "        logging.info(f\"Validation set shape: {X_val.shape}\")\n",
    "else: # No data remaining after train split\n",
    "    X_temp2, y_temp2 = empty_X.copy(), empty_y.copy()\n",
    "    if not np.isclose(VAL_SIZE, 0): # Log only if Val set was expected\n",
    "       logging.info(\"Validation set is empty (no data remaining after train split).\")\n",
    "\n",
    "\n",
    "# --- Third Split: Test vs. Cal ---\n",
    "if not X_temp2.empty:\n",
    "    # Denominator for relative size calculation: size of the current remaining pool\n",
    "    current_remaining_size_frac = TEST_SIZE + CAL_SIZE # = test_cal_size\n",
    "\n",
    "    if np.isclose(CAL_SIZE, 0): # No Cal set, remainder is Test\n",
    "        X_test, y_test = X_temp2, y_temp2\n",
    "        logging.info(\"Calibration set is empty (CAL_SIZE=0).\")\n",
    "    elif np.isclose(TEST_SIZE, 0): # Only Cal set left in remainder\n",
    "        X_cal, y_cal = X_temp2, y_temp2\n",
    "        logging.info(\"Test set is empty (TEST_SIZE=0).\")\n",
    "    else: # Split Test vs Cal\n",
    "        # Proportion of Cal relative to (Test + Cal)\n",
    "        split_test_size = CAL_SIZE / current_remaining_size_frac\n",
    "        X_test, X_cal, y_test, y_cal = train_test_split(\n",
    "            X_temp2, y_temp2,\n",
    "            test_size=split_test_size,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=get_stratify_array(y_temp2)\n",
    "        )\n",
    "        # Logging shapes done after the if/else block\n",
    "else: # No data remaining for Test/Cal split\n",
    "    if not (np.isclose(TEST_SIZE, 0) and np.isclose(CAL_SIZE, 0)): # Log only if Test or Cal were expected\n",
    "        logging.info(\"Test and Calibration sets are empty (no data remaining for final split).\")\n",
    "\n",
    "# Log final shapes for Test and Cal\n",
    "logging.info(f\"Test set shape: {X_test.shape}\")\n",
    "logging.info(f\"Calibration set shape: {X_cal.shape}\")\n",
    "\n",
    "\n",
    "# --- Verification and Final Logging ---\n",
    "total_len = len(X_train) + len(X_val) + len(X_test) + len(X_cal)\n",
    "original_len = len(X)\n",
    "\n",
    "if total_len != original_len:\n",
    "     # Calculate actual proportions based on lengths\n",
    "     actual_train = len(X_train) / original_len if original_len > 0 else 0\n",
    "     actual_val = len(X_val) / original_len if original_len > 0 else 0\n",
    "     actual_test = len(X_test) / original_len if original_len > 0 else 0\n",
    "     actual_cal = len(X_cal) / original_len if original_len > 0 else 0\n",
    "     logging.warning(f\"Total split length ({total_len}) does not exactly match original length ({original_len}). \"\n",
    "                     f\"This can happen with stratification or rounding. \"\n",
    "                     f\"Target proportions: Train={TRAIN_SIZE:.3f}, Val={VAL_SIZE:.3f}, Test={TEST_SIZE:.3f}, Cal={CAL_SIZE:.3f}. \"\n",
    "                     f\"Actual proportions: Train={actual_train:.3f}, Val={actual_val:.3f}, Test={actual_test:.3f}, Cal={actual_cal:.3f}\")\n",
    "else:\n",
    "    logging.info(\"Split lengths verification successful.\")\n",
    "\n",
    "logging.info(\"Data splitting complete.\")\n",
    "\n",
    "# Log distributions, handling empty sets\n",
    "def log_distribution(name, y_set):\n",
    "    if y_set.empty:\n",
    "        logging.info(f\"{name} target distribution: Set is empty.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Use normalize=True, handle potential division by zero if counts are zero (though unlikely if not empty)\n",
    "            counts = y_set.value_counts()\n",
    "            dist = counts / counts.sum() if counts.sum() > 0 else counts\n",
    "            logging.info(f\"{name} target distribution:\\n{dist}\")\n",
    "            # Log absolute counts as well for clarity\n",
    "            logging.info(f\"{name} target counts:\\n{counts}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Could not calculate distribution for {name}: {e}\")\n",
    "            # Attempt to log raw value counts even if normalization fails\n",
    "            try:\n",
    "                logging.info(f\"{name} raw value counts:\\n{y_set.value_counts()}\")\n",
    "            except Exception as e_raw:\n",
    "                 logging.error(f\"Could not get raw value counts for {name}: {e_raw}\")\n",
    "\n",
    "\n",
    "log_distribution(\"Train\", y_train)\n",
    "log_distribution(\"Validation\", y_val)\n",
    "log_distribution(\"Test\", y_test)\n",
    "log_distribution(\"Calibration\", y_cal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization via Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler, train_test_split\n",
    "from sklearn.metrics import f1_score # Default scorer\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "\n",
    "# --- Internal Helper ---\n",
    "def _train_and_eval(model_class, params,\n",
    "                    X_train, y_train, X_val, y_val,\n",
    "                    resource, resource_type,\n",
    "                    scoring_func, random_state, fit_params):\n",
    "    \"\"\"Internal helper function to train and evaluate a single configuration.\"\"\"\n",
    "    try:\n",
    "        model = model_class(**params)\n",
    "\n",
    "        if resource_type == 'data_fraction':\n",
    "            # Create a stratified subset of the training data\n",
    "            if resource < 1.0:\n",
    "                 # Ensure resource is not too small to stratify\n",
    "                min_samples_per_class = min(y_train.value_counts()) if isinstance(y_train, pd.Series) else 0\n",
    "                # Estimate required samples, handle cases where min_samples_per_class is 0 or very small\n",
    "                if min_samples_per_class > 0:\n",
    "                    required_samples = max(2, math.ceil(resource * min_samples_per_class)) # Need >= 2 per class for stratification\n",
    "                else:\n",
    "                    required_samples = 2 # Default if counts are weird\n",
    "\n",
    "                if required_samples > min_samples_per_class and min_samples_per_class > 0: # Avoid warning if min_samples is 0\n",
    "                     logging.warning(f\"Resource fraction {resource:.2f} too small for stratification with min class size {min_samples_per_class}. Using full data.\")\n",
    "                     X_subset, y_subset = X_train, y_train\n",
    "                elif len(y_train) * resource < 2:\n",
    "                     logging.warning(f\"Resource fraction {resource:.2f} results in < 2 samples ({len(y_train)*resource:.1f}). Using full data.\")\n",
    "                     X_subset, y_subset = X_train, y_train\n",
    "                else:\n",
    "                    # Use train_test_split to get a stratified subset. We only need the 'train' part.\n",
    "                    try:\n",
    "                        X_subset, _, y_subset, _ = train_test_split(\n",
    "                            X_train, y_train,\n",
    "                            train_size=resource,\n",
    "                            stratify=y_train,\n",
    "                            random_state=random_state\n",
    "                        )\n",
    "                        if len(X_subset) < 2: # Safety check after split\n",
    "                            logging.warning(f\"Subset resulted in < 2 samples after split. Using full data.\")\n",
    "                            X_subset, y_subset = X_train, y_train\n",
    "                    except ValueError as e:\n",
    "                        # Handle cases where stratification might fail (e.g., too few samples in a class for the split)\n",
    "                        logging.warning(f\"Stratified split failed for resource {resource:.2f} (Error: {e}). Using full data.\")\n",
    "                        X_subset, y_subset = X_train, y_train\n",
    "\n",
    "            else:\n",
    "                 X_subset, y_subset = X_train, y_train # Use full data if resource is 1.0\n",
    "\n",
    "            # Train on the subset\n",
    "            start_fit = time.time()\n",
    "            model.fit(X_subset, y_subset) # No fit_params here usually\n",
    "            fit_duration = time.time() - start_fit\n",
    "\n",
    "        elif resource_type == 'iterations':\n",
    "            # Resource represents n_estimators or similar iteration parameter\n",
    "            # Assumes the parameter name is 'n_estimators' - adjust if needed for other models\n",
    "            params_iter = params.copy() # Avoid modifying original params dict\n",
    "            iter_param_name = 'n_estimators' # Common case\n",
    "            # Handle potential different names if necessary (e.g., 'max_iter' for some models)\n",
    "            # if 'n_estimators' not in model.get_params(): iter_param_name = 'SOME_OTHER_NAME'\n",
    "            params_iter[iter_param_name] = int(resource)\n",
    "            model = model_class(**params_iter) # Re-instantiate with correct n_estimators\n",
    "\n",
    "            # Prepare fit_params for early stopping if applicable\n",
    "            current_fit_params = {}\n",
    "            # Check if 'eval_set' is needed and provided in fit_params keys\n",
    "            if 'eval_set' in fit_params:\n",
    "                 # Ensure X_val, y_val are correctly formatted\n",
    "                 # XGBoost/LightGBM usually want list of tuples [(X, y)]\n",
    "                 # Make sure the passed X_val, y_val are the correct ones (scaled/unscaled)\n",
    "                 current_fit_params['eval_set'] = [(X_val, y_val)] # Use the validation set passed to HPO\n",
    "                 # Copy other relevant early stopping params\n",
    "                 for key in ['early_stopping_rounds', 'verbose', 'callbacks']: # Add other potential keys like 'callbacks' for LightGBM\n",
    "                     if key in fit_params:\n",
    "                         current_fit_params[key] = fit_params[key]\n",
    "\n",
    "            start_fit = time.time()\n",
    "            # Use try-except as fit might fail\n",
    "            try:\n",
    "                 model.fit(X_train, y_train, **current_fit_params)\n",
    "            except Exception as fit_error:\n",
    "                 logging.error(f\"Fit failed for config {params_iter} with resource {resource}: {fit_error}\")\n",
    "                 return -1.0 # Indicate failure\n",
    "            fit_duration = time.time() - start_fit\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid resource_type. Choose 'data_fraction' or 'iterations'.\")\n",
    "\n",
    "        # Evaluate on the full validation set\n",
    "        start_eval = time.time()\n",
    "        # Use try-except as predict might fail (e.g., if fit failed silently or model is unusable)\n",
    "        try:\n",
    "             y_pred_val = model.predict(X_val)\n",
    "             score = scoring_func(y_val, y_pred_val)\n",
    "        except Exception as eval_error:\n",
    "             logging.error(f\"Predict/Score failed for config {params} with resource {resource}: {eval_error}\")\n",
    "             score = -1.0 # Indicate failure\n",
    "        eval_duration = time.time() - start_eval\n",
    "\n",
    "        logging.debug(f\"Evaluated config: {params} | Resource: {resource:.2f} | Score: {score:.4f} | Fit: {fit_duration:.2f}s | Eval: {eval_duration:.2f}s\")\n",
    "        return score\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error training/evaluating config {params} with resource {resource}: {e}\", exc_info=False) # Set exc_info=True for traceback\n",
    "        return -1.0 # Return a clearly bad score\n",
    "    # --- End of _train_and_eval code ---\n",
    "\n",
    "\n",
    "def hyperband_hpo(model_class, param_space,\n",
    "                  X_train, y_train, X_val, y_val,\n",
    "                  max_resource, eta=3, resource_type='iterations',\n",
    "                  min_resource=1, # Min iterations or min data fraction\n",
    "                  scoring_func=f1_score, # Function accepting (y_true, y_pred)\n",
    "                  random_state=None,\n",
    "                  fit_params=None): # For early stopping etc. passed to .fit()\n",
    "    \"\"\"\n",
    "    Performs Hyperband Hyperparameter Optimization.\n",
    "\n",
    "    Args:\n",
    "        model_class: The model class (e.g., SVC, RandomForestClassifier).\n",
    "        param_space (dict): Dictionary defining the hyperparameter search space\n",
    "                           compatible with ParameterSampler.\n",
    "        X_train, y_train: Training data and labels.\n",
    "        X_val, y_val: Validation data and labels for evaluation.\n",
    "        max_resource (float/int): Maximum resource allocation\n",
    "                                 (e.g., max n_estimators or 1.0 for data fraction).\n",
    "        eta (int): Reduction factor for successive halving (>= 2).\n",
    "        resource_type (str): How resource is allocated:\n",
    "                             'iterations' -> resource sets n_estimators (or similar).\n",
    "                             'data_fraction' -> resource is fraction of training data used (stratified).\n",
    "        min_resource (float/int): Minimum resource for the first iteration.\n",
    "                                 Must be >= 1 for 'iterations', > 0 for 'data_fraction'.\n",
    "        scoring_func (callable): Function to evaluate performance (e.g., f1_score).\n",
    "                                Higher score is assumed better.\n",
    "        random_state (int): Seed for reproducibility of parameter sampling and data subsetting.\n",
    "        fit_params (dict, optional): Additional parameters passed to the model's .fit()\n",
    "                                      method (e.g., for early stopping: {'eval_set':..., 'early_stopping_rounds':...}).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_params, best_score)\n",
    "               best_params (dict): The hyperparameters of the best performing configuration.\n",
    "               best_score (float): The score achieved by the best configuration on the validation set\n",
    "                                  using the maximum resource.\n",
    "    \"\"\"\n",
    "    if fit_params is None:\n",
    "        fit_params = {}\n",
    "\n",
    "    log_max_r = math.log(max_resource / min_resource, eta) if max_resource > min_resource and min_resource > 0 else 0\n",
    "    s_max = int(log_max_r)\n",
    "    B = (s_max + 1) * max_resource # Approximate total resource budget\n",
    "\n",
    "    logging.info(f\"--- Starting Hyperband HPO ---\")\n",
    "    logging.info(f\"Model: {model_class.__name__}\")\n",
    "    logging.info(f\"Resource Type: {resource_type}\")\n",
    "    logging.info(f\"Resource Range: [{min_resource}, {max_resource}]\")\n",
    "    logging.info(f\"Eta: {eta}\")\n",
    "    logging.info(f\"Max Brackets (s_max): {s_max}\")\n",
    "    logging.info(f\"Approx. Budget (B): {B:.2f}\")\n",
    "    logging.info(f\"Scoring: {scoring_func.__name__}\")\n",
    "\n",
    "    best_params = None\n",
    "    best_score = -1.0\n",
    "    total_configs_evaluated = 0\n",
    "    outer_tqdm = tqdm(range(s_max, -1, -1), desc=\"Hyperband Brackets (s)\")\n",
    "\n",
    "    # Outer loop: Iterate through brackets (s values)\n",
    "    for s in outer_tqdm:\n",
    "        n_configs = int(math.ceil(int(B / max_resource / (s + 1)) * eta**s)) # Number of configs in this bracket\n",
    "        r_initial = max_resource * eta**(-s) # Initial resource for this bracket\n",
    "        # Ensure initial resource is not less than min_resource\n",
    "        r_initial = max(r_initial, min_resource)\n",
    "\n",
    "        outer_tqdm.set_description(f\"Bracket s={s} (n={n_configs}, r0={r_initial:.2f})\")\n",
    "        logging.info(f\"\\n>> Bracket s={s}: n_configs={n_configs}, r_initial={r_initial:.2f}\")\n",
    "\n",
    "        # Sample configurations for this bracket\n",
    "        param_list = list(ParameterSampler(param_space, n_iter=n_configs, random_state=random_state + s if random_state is not None else None))\n",
    "        \n",
    "        # --- Add common fixed parameters ---\n",
    "        # Calculate scale_pos_weight once if needed\n",
    "        scale_pos_weight_val = None\n",
    "        if model_class in [xgb.XGBClassifier, lgb.LGBMClassifier]:\n",
    "             neg_count = (y_train == 0).sum()\n",
    "             pos_count = (y_train == 1).sum()\n",
    "             if pos_count > 0:\n",
    "                 scale_pos_weight_val = neg_count / pos_count\n",
    "\n",
    "        for p in param_list:\n",
    "             # Add random_state if model supports it and it's not sampled\n",
    "             if 'random_state' not in p and hasattr(model_class(random_state=1), 'random_state'): # Check if attr exists\n",
    "                 p['random_state'] = random_state\n",
    "             # Add class_weight='balanced' for relevant sklearn models if not sampled\n",
    "             if model_class in [SVC, RandomForestClassifier, DecisionTreeClassifier] and 'class_weight' not in p:\n",
    "                 p['class_weight'] = 'balanced'\n",
    "             # Add scale_pos_weight for boosting models if not sampled and calculated\n",
    "             if model_class in [xgb.XGBClassifier, lgb.LGBMClassifier] and 'scale_pos_weight' not in p and scale_pos_weight_val is not None:\n",
    "                  p['scale_pos_weight'] = scale_pos_weight_val\n",
    "             # For LightGBM, also consider adding 'objective': 'binary' if not sampled\n",
    "             if model_class is lgb.LGBMClassifier and 'objective' not in p:\n",
    "                  p['objective'] = 'binary'\n",
    "        # -----------------------------------\n",
    "\n",
    "        # Inner loop: Successive halving rounds\n",
    "        inner_tqdm = tqdm(range(s + 1), desc=f\"SH Round (s={s})\", leave=False)\n",
    "        for i in inner_tqdm:\n",
    "            current_resource = r_initial * eta**i\n",
    "            # Ensure resource doesn't exceed max_resource due to floating point/rounding\n",
    "            current_resource = min(current_resource, max_resource)\n",
    "\n",
    "            n_configs_in_round = len(param_list)\n",
    "            inner_tqdm.set_description(f\"SH Round i={i} (n={n_configs_in_round}, r={current_resource:.2f})\")\n",
    "            logging.info(f\"  -- Round i={i}: Evaluating {n_configs_in_round} configs with resource={current_resource:.2f} --\")\n",
    "\n",
    "            round_scores = []\n",
    "            # Use tqdm for the configurations within the round\n",
    "            eval_tqdm = tqdm(param_list, desc=f\"Evaluating Configs (i={i})\", leave=False)\n",
    "            for params in eval_tqdm:\n",
    "                score = _train_and_eval(model_class, params, X_train, y_train, X_val, y_val,\n",
    "                                        current_resource, resource_type, scoring_func,\n",
    "                                        random_state, fit_params)\n",
    "                round_scores.append((score, params))\n",
    "                total_configs_evaluated += 1 # Count unique evaluations\n",
    "\n",
    "            # Sort by score (descending, higher is better)\n",
    "            round_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            # Track the best overall score and params seen so far *at max resource*\n",
    "            # Only update if we are actually at max resource in this round\n",
    "            if abs(current_resource - max_resource) < 1e-6: # Check if we are at max resource\n",
    "                 if round_scores and round_scores[0][0] > best_score:\n",
    "                      best_score = round_scores[0][0]\n",
    "                      best_params = round_scores[0][1]\n",
    "                      logging.info(f\"  ** New Best Found (Score: {best_score:.4f}) at max resource ** Params: {best_params}\")\n",
    "                      # Update outer tqdm description with best score found so far\n",
    "                      outer_tqdm.set_postfix_str(f\"Best F1: {best_score:.4f}\", refresh=True)\n",
    "\n",
    "\n",
    "            # --- Halving Step ---\n",
    "            n_keep = int(n_configs_in_round / eta)\n",
    "            logging.info(f\"  -- Round i={i}: Completed {len(round_scores)} evaluations. Keeping top {n_keep} configs. --\")\n",
    "\n",
    "            if n_keep < 1 or i == s: # Keep at least one, or if it's the last round\n",
    "                # If it's the last round, ensure the best score from *this bracket* at *max resource* is considered\n",
    "                if abs(current_resource - max_resource) < 1e-6 and round_scores:\n",
    "                     bracket_best_score = round_scores[0][0]\n",
    "                     bracket_best_params = round_scores[0][1]\n",
    "                     logging.info(f\"  Bracket s={s} final best score: {bracket_best_score:.4f}\")\n",
    "                     # No need to update global best here, already done above\n",
    "                break # Exit inner loop\n",
    "\n",
    "            # Prepare parameter list for the next round\n",
    "            param_list = [params for score, params in round_scores[:n_keep]]\n",
    "            if not param_list: # Safety break if list becomes empty unexpectedly\n",
    "                 logging.warning(f\"  Param list empty after halving round i={i}. Stopping bracket.\")\n",
    "                 break\n",
    "\n",
    "    logging.info(f\"\\n--- Hyperband HPO Finished ---\")\n",
    "    logging.info(f\"Total configurations evaluated (approx): {total_configs_evaluated}\") # Might overcount if errors happened\n",
    "    if best_params:\n",
    "        logging.info(f\"Best Overall Score ({scoring_func.__name__}): {best_score:.4f}\")\n",
    "        logging.info(f\"Best Params: {best_params}\")\n",
    "    else:\n",
    "        logging.warning(\"No best parameters found. Check logs for errors or increase resources/configs.\")\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platt Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_platt_scaler(base_estimator_class, best_params, X_train, y_train,\n",
    "                       score_method='decision_function', # Nuevo parámetro\n",
    "                       n_splits=5, random_state=None):\n",
    "    \"\"\"\n",
    "    Trains a base estimator and calibrates its outputs using Platt scaling\n",
    "    with k-fold cross-validation to obtain out-of-fold scores.\n",
    "\n",
    "    Args:\n",
    "        base_estimator_class: The class of the base estimator (e.g., SVC, RandomForestClassifier).\n",
    "        best_params (dict): Dictionary of best hyperparameters for the base estimator.\n",
    "        X_train (pd.DataFrame or np.ndarray): Training features.\n",
    "        y_train (pd.Series or np.ndarray): Training labels.\n",
    "        score_method (str): Method to get scores from the base estimator during CV.\n",
    "                            Options: 'decision_function', 'predict_proba',\n",
    "                                     'raw_margin_xgb', 'raw_score_lgbm'.\n",
    "        n_splits (int): Number of folds for cross-validation.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (fitted_base_estimator, fitted_platt_scaler)\n",
    "               Returns (None, None) if an error occurs.\n",
    "    \"\"\"\n",
    "    logging.info(f\"--- Starting Platt Scaling Training ({score_method}) ---\")\n",
    "    try:\n",
    "        # 1. Train the final base model on the entire training set\n",
    "        logging.info(\"Training final base model on full training data...\")\n",
    "        final_base_estimator = base_estimator_class(**best_params)\n",
    "        # Make sure y_train is a numpy array for fitting if needed\n",
    "        y_train_np = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "        final_base_estimator.fit(X_train, y_train_np)\n",
    "        logging.info(\"Final base model trained.\")\n",
    "\n",
    "        # 2. Get out-of-fold scores using k-fold CV\n",
    "        logging.info(f\"Performing {n_splits}-fold CV to get out-of-fold scores ({score_method})...\")\n",
    "        # Use StratifiedKFold for classification\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "        # Determine if X_train is DataFrame or ndarray for proper indexing\n",
    "        is_pandas_X = isinstance(X_train, pd.DataFrame)\n",
    "        is_pandas_y = isinstance(y_train, pd.Series)\n",
    "\n",
    "        oof_scores = np.zeros(len(y_train), dtype=float)\n",
    "        oof_true_labels = np.zeros(len(y_train), dtype=int)\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train_np)):\n",
    "            logging.info(f\"Processing Fold {fold+1}/{n_splits}...\")\n",
    "\n",
    "            # Select data based on index type\n",
    "            if is_pandas_X:\n",
    "                X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            else: # Assume numpy array\n",
    "                X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "\n",
    "            if is_pandas_y:\n",
    "                y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "            else: # Assume numpy array\n",
    "                y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "            # Ensure y_train_fold is numpy for fitting fold estimator\n",
    "            y_train_fold_np = y_train_fold.values if isinstance(y_train_fold, pd.Series) else y_train_fold\n",
    "\n",
    "\n",
    "            # Clone and train estimator on the fold's training data\n",
    "            # Use clone to ensure fresh state and proper param handling\n",
    "            estimator_fold = clone(final_base_estimator) # Clone the already instantiated final estimator\n",
    "            estimator_fold.fit(X_train_fold, y_train_fold_np)\n",
    "\n",
    "            # Get scores based on the specified method\n",
    "            scores_fold = None\n",
    "            if score_method == 'decision_function':\n",
    "                 if hasattr(estimator_fold, 'decision_function'):\n",
    "                     scores_fold = estimator_fold.decision_function(X_val_fold)\n",
    "                 else:\n",
    "                     raise AttributeError(f\"{base_estimator_class.__name__} does not have 'decision_function' method.\")\n",
    "            elif score_method == 'predict_proba':\n",
    "                 if hasattr(estimator_fold, 'predict_proba'):\n",
    "                     # Use probability of the positive class (class 1)\n",
    "                     scores_fold = estimator_fold.predict_proba(X_val_fold)[:, 1]\n",
    "                 else:\n",
    "                      raise AttributeError(f\"{base_estimator_class.__name__} does not have 'predict_proba' method.\")\n",
    "            elif score_method == 'raw_margin_xgb':\n",
    "                 # Assumes XGBoost model\n",
    "                 scores_fold = estimator_fold.predict(X_val_fold, output_margin=True)\n",
    "            elif score_method == 'raw_score_lgbm':\n",
    "                 # Assumes LightGBM model\n",
    "                 scores_fold = estimator_fold.predict(X_val_fold, raw_score=True)\n",
    "            else:\n",
    "                 raise ValueError(f\"Unsupported score_method: {score_method}\")\n",
    "\n",
    "            # Store results\n",
    "            oof_scores[val_idx] = scores_fold\n",
    "            # Ensure y_val_fold is numpy for assignment\n",
    "            y_val_fold_np = y_val_fold.values if isinstance(y_val_fold, pd.Series) else y_val_fold\n",
    "            oof_true_labels[val_idx] = y_val_fold_np\n",
    "\n",
    "        logging.info(\"Out-of-fold scores collected.\")\n",
    "\n",
    "        # Reshape scores for Logistic Regression input\n",
    "        oof_scores_reshaped = oof_scores.reshape(-1, 1)\n",
    "\n",
    "        # 3. Train the Logistic Regression scaler\n",
    "        logging.info(\"Training Logistic Regression (Platt) scaler...\")\n",
    "        # Use high C to approximate original Platt scaling (low regularization)\n",
    "        platt_scaler = LogisticRegression(C=1e10, solver='liblinear', random_state=random_state)\n",
    "        platt_scaler.fit(oof_scores_reshaped, oof_true_labels)\n",
    "        logging.info(\"Platt scaler trained.\")\n",
    "\n",
    "        # Verify shapes one last time\n",
    "        if oof_scores_reshaped.shape[0] != len(oof_true_labels):\n",
    "            raise ValueError(f\"Shape mismatch after collecting OOF scores: scores {oof_scores_reshaped.shape[0]}, labels {len(oof_true_labels)}\")\n",
    "\n",
    "        logging.info(f\"--- Platt Scaling Training ({score_method}) Complete ---\")\n",
    "        return final_base_estimator, platt_scaler\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during Platt scaling ({score_method}): {e}\", exc_info=True)\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ncm_scores(calibrated_probs, true_labels):\n",
    "    \"\"\"Calculates non-conformity scores (1 - probability of true class).\"\"\"\n",
    "    if not isinstance(calibrated_probs, np.ndarray) or not isinstance(true_labels, np.ndarray):\n",
    "         # Ensure inputs are numpy arrays for proper indexing\n",
    "        calibrated_probs = np.asarray(calibrated_probs)\n",
    "        true_labels = np.asarray(true_labels)\n",
    "        \n",
    "    if calibrated_probs.shape[0] != true_labels.shape[0]:\n",
    "         raise ValueError(\"Probs and labels must have the same number of samples.\")\n",
    "    if calibrated_probs.shape[1] < np.max(true_labels) + 1:\n",
    "        raise ValueError(\"Probs array has fewer columns than needed for max label index.\")\n",
    "\n",
    "    # Get probability of the true class for each sample\n",
    "    # Using np.take_along_axis for efficient indexing\n",
    "    true_class_probs = np.take_along_axis(calibrated_probs, true_labels[:, np.newaxis], axis=1).squeeze()\n",
    "\n",
    "    return 1.0 - true_class_probs\n",
    "\n",
    "\n",
    "def calibrate_conformal_threshold(ncm_scores, alpha):\n",
    "    \"\"\"Calculates the quantile threshold q for ICP.\"\"\"\n",
    "    n = len(ncm_scores)\n",
    "    q_level = np.ceil((n + 1) * (1 - alpha)) / n\n",
    "    q_threshold = np.quantile(ncm_scores, q_level, method='higher') # Use 'higher' to ensure coverage\n",
    "    logging.info(f\"Calibrated CP: n={n}, alpha={alpha}, q_level={q_level:.4f}, q_threshold={q_threshold:.6f}\")\n",
    "    return q_threshold\n",
    "\n",
    "def predict_conformal_sets(calibrated_probs, q_threshold):\n",
    "    \"\"\"Generates prediction sets based on calibrated probabilities and threshold.\"\"\"\n",
    "    prediction_sets = []\n",
    "    ncm_scores_per_class = 1.0 - calibrated_probs # NCM score for *each* potential class\n",
    "\n",
    "    for scores in ncm_scores_per_class:\n",
    "        set_for_sample = [i for i, score in enumerate(scores) if score <= q_threshold]\n",
    "        # Handle empty sets - should be rare with correct quantile calculation\n",
    "        # but as a fallback, could include the most likely class\n",
    "        if not set_for_sample:\n",
    "             set_for_sample = [np.argmin(scores)] # Index of lowest NCM score == highest prob\n",
    "             logging.warning(f\"Empty prediction set generated, falling back to most likely class: {set_for_sample}\")\n",
    "        prediction_sets.append(set(set_for_sample)) # Store as sets\n",
    "    return prediction_sets\n",
    "\n",
    "def evaluate_conformal_prediction(y_true, prediction_sets, alpha, model_name=\"Model\"):\n",
    "    \"\"\"Evaluates the performance of conformal prediction sets.\"\"\"\n",
    "    if not isinstance(y_true, (pd.Series, np.ndarray)):\n",
    "        y_true = np.asarray(y_true) # Ensure y_true is indexable\n",
    "\n",
    "    n_samples = len(y_true)\n",
    "    if n_samples == 0:\n",
    "        logging.warning(f\"[{model_name} CP Eval] Empty y_true provided.\")\n",
    "        return None, None\n",
    "\n",
    "    # Empirical Coverage\n",
    "    coverage_count = sum(y_true.iloc[i] in prediction_sets[i] for i in range(n_samples))\n",
    "    empirical_coverage = coverage_count / n_samples\n",
    "\n",
    "    # Average Set Size\n",
    "    average_set_size = np.mean([len(s) for s in prediction_sets])\n",
    "\n",
    "    logging.info(f\"--- {model_name} Conformal Prediction Evaluation (alpha={alpha}) ---\")\n",
    "    logging.info(f\"Target Coverage: {1 - alpha:.2f}\")\n",
    "    logging.info(f\"Empirical Coverage: {empirical_coverage:.4f} ({coverage_count}/{n_samples})\")\n",
    "    logging.info(f\"Average Prediction Set Size: {average_set_size:.4f}\")\n",
    "\n",
    "    return empirical_coverage, average_set_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Comprehensive Metrics ---\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_proba, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Calculates a comprehensive set of classification metrics.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Ground truth labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        y_proba (array-like): Predicted probabilities for the positive class (class 1).\n",
    "        model_name (str): Name of the model for logging.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing calculated metrics.\n",
    "              Returns None if input arrays are empty or invalid.\n",
    "    \"\"\"\n",
    "    if len(y_true) == 0 or len(y_pred) == 0 or len(y_proba) == 0:\n",
    "        logging.error(f\"[{model_name}] Empty input arrays provided for metric calculation.\")\n",
    "        return None\n",
    "    if len(y_true) != len(y_pred) or len(y_true) != len(y_proba):\n",
    "        logging.error(f\"[{model_name}] Mismatched lengths in input arrays for metric calculation.\")\n",
    "        return None\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # --- Threshold-based Metrics (using y_pred) ---\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['precision'] = precision\n",
    "    metrics['recall_tpr'] = recall # True Positive Rate (Sensitivity)\n",
    "    metrics['f1_score'] = f1\n",
    "\n",
    "    # Specificity (True Negative Rate)\n",
    "    metrics['specificity_tnr'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "    # Geometric Mean\n",
    "    metrics['g_mean'] = np.sqrt(metrics['recall_tpr'] * metrics['specificity_tnr'])\n",
    "\n",
    "    # Confusion Matrix\n",
    "    metrics['confusion_matrix'] = {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp}\n",
    "\n",
    "    # --- Ranking/Probabilistic Metrics (using y_proba) ---\n",
    "    try:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_true, y_proba)\n",
    "    except ValueError as e:\n",
    "        logging.warning(f\"[{model_name}] Could not calculate ROC AUC: {e}. Setting to 0.0.\")\n",
    "        metrics['roc_auc'] = 0.0 # Handle cases with only one class present\n",
    "\n",
    "    # PR AUC\n",
    "    pr_curve_precision, pr_curve_recall, _ = precision_recall_curve(y_true, y_proba)\n",
    "    metrics['pr_auc'] = auc(pr_curve_recall, pr_curve_precision) # Note order: recall is x, precision is y\n",
    "\n",
    "    # Brier Score\n",
    "    metrics['brier_score'] = brier_score_loss(y_true, y_proba)\n",
    "\n",
    "    logging.info(f\"--- {model_name} Metrics ---\")\n",
    "    logging.info(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    logging.info(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    logging.info(f\"Recall (TPR): {metrics['recall_tpr']:.4f}\")\n",
    "    logging.info(f\"Specificity (TNR): {metrics['specificity_tnr']:.4f}\")\n",
    "    logging.info(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    logging.info(f\"G-Mean: {metrics['g_mean']:.4f}\")\n",
    "    logging.info(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    logging.info(f\"PR AUC: {metrics['pr_auc']:.4f}\")\n",
    "    logging.info(f\"Brier Score: {metrics['brier_score']:.4f}\")\n",
    "    logging.info(f\"Confusion Matrix (TN, FP, FN, TP): ({tn}, {fp}, {fn}, {tp})\")\n",
    "\n",
    "    # Optional: Plot Confusion Matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap([[tn, fp], [fn, tp]], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted Galaxy (0)', 'Predicted Star (1)'],\n",
    "                yticklabels=['Actual Galaxy (0)', 'Actual Star (1)'])\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    cm_filename = os.path.join(MODEL_DIR, f\"{model_name}_confusion_matrix_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\")\n",
    "    plt.savefig(cm_filename)\n",
    "    plt.close()\n",
    "    logging.info(f\"Confusion matrix plot saved to {cm_filename}\")\n",
    "\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Scaling ---\n",
    "# Important for SVM, not used for the other models.\n",
    "# Fit scaler ONLY on training data, then transform all sets.\n",
    "\n",
    "# Check if training set and other datasets are non-empty before scaling\n",
    "if len(X_train) > 0 and TRAIN_SIZE > 0:\n",
    "    logging.info(\"Applying StandardScaler to features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "else:\n",
    "    logging.info(\"Empty training set, NOT able to apply StandardScaler!\")\n",
    "    X_train_scaled = X_train\n",
    "\n",
    "if len(X_val) > 0 and VAL_SIZE > 0:\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "else:\n",
    "    X_val_scaled = X_val\n",
    "\n",
    "if len(X_test) > 0 and TEST_SIZE > 0:\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "else:\n",
    "    X_test_scaled = X_test\n",
    "\n",
    "if len(X_cal) > 0 and CAL_SIZE > 0:\n",
    "    X_cal_scaled = scaler.transform(X_cal)\n",
    "else:\n",
    "    X_cal_scaled = X_cal\n",
    "\n",
    "# Save the scaler if it was fitted\n",
    "if 'scaler' in locals():\n",
    "    scaler_filename = os.path.join(MODEL_DIR, f\"scaler_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib\")\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    logging.info(f\"Scaler saved to {scaler_filename}\")\n",
    "\n",
    "# Use scaled data for models sensitive to scale (like SVM)\n",
    "logging.info(\"Feature scaling complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {} # Dictionary to store metrics for each model\n",
    "\n",
    "ALPHA = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for SVM (using data fraction)\n",
    "MAX_RESOURCE_SVM = 1.0  # Max data fraction\n",
    "MIN_RESOURCE_SVM = 0.1  # Min data fraction (adjust based on minority class size)\n",
    "ETA_SVM = 3\n",
    "RESOURCE_TYPE_SVM = 'data_fraction'\n",
    "model_name_svm = \"SVM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¡MODIFICADO PARA PRUEBA RÁPIDA!\n",
    "MAX_RESOURCE_SVM = 1.0  # Se mantiene en 1.0 para usar todos los datos al final\n",
    "MIN_RESOURCE_SVM = 0.5  # Aumentado para reducir s_max (menos brackets/configs)\n",
    "ETA_SVM = 4             # Aumentado para eliminar configuraciones más rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "logging.info(f\"\\n\\n===== Starting Workflow for {model_name_svm} =====\")\n",
    "timestamp_svm = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "hpo_start_time_svm = time.time()\n",
    "\n",
    "# --- 1.1 SVM: Define Search Space and HPO Params ---\n",
    "param_space_svm = {\n",
    "    'C': loguniform(1e-2, 1e3),\n",
    "    'gamma': loguniform(1e-4, 1e1),\n",
    "    'kernel': ['rbf'], # Example: Fixed RBF kernel\n",
    "    # 'kernel': ['rbf', 'linear'], # Example: If you want to search kernels\n",
    "    # class_weight is added automatically inside hyperband_hpo\n",
    "    # random_state is added automatically inside hyperband_hpo\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# --- 1.2 SVM: Run Hyperband HPO ---\n",
    "logging.info(f\"--- [{model_name_svm}] Running Hyperband HPO ---\")\n",
    "best_params_svm, best_score_hpo_svm = hyperband_hpo(\n",
    "    model_class=SVC,\n",
    "    param_space=param_space_svm,\n",
    "    X_train=X_train_scaled, # USE SCALED DATA\n",
    "    y_train=y_train,\n",
    "    X_val=X_val_scaled,     # USE SCALED DATA\n",
    "    y_val=y_val,\n",
    "    max_resource=MAX_RESOURCE_SVM,\n",
    "    eta=ETA_SVM,\n",
    "    resource_type=RESOURCE_TYPE_SVM,\n",
    "    min_resource=MIN_RESOURCE_SVM,\n",
    "    scoring_func=f1_score,\n",
    "    random_state=RANDOM_SEED,\n",
    "    fit_params={} # No specific fit_params for SVC\n",
    ")\n",
    "hpo_duration_svm = time.time() - hpo_start_time_svm\n",
    "logging.info(f\"--- [{model_name_svm}] HPO finished in {hpo_duration_svm:.2f} seconds ---\")\n",
    "\n",
    "# --- 1.3 SVM: Train Final Model & Platt Scaler (using Full Training Set) ---\n",
    "fitted_svm_base = None\n",
    "platt_scaler_svm = None\n",
    "if best_params_svm:\n",
    "    logging.info(f\"--- [{model_name_svm}] Training final model and Platt scaler ---\")\n",
    "    platt_start_time_svm = time.time()\n",
    "    # Ensure necessary fixed parameters are present for the final fit\n",
    "    best_params_svm['random_state'] = RANDOM_SEED\n",
    "    if 'class_weight' not in best_params_svm: best_params_svm['class_weight'] = 'balanced'\n",
    "    if 'probability' in best_params_svm: del best_params_svm['probability'] # Use decision_function\n",
    "\n",
    "    fitted_svm_base, platt_scaler_svm = train_platt_scaler(\n",
    "        base_estimator_class=SVC, # Pass the class\n",
    "        best_params=best_params_svm,\n",
    "        X_train=X_train_scaled, # Use scaled training data\n",
    "        y_train=y_train,        # Use original y_train for CV indexing\n",
    "        n_splits=5,             # Folds for Platt CV\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    platt_duration_svm = time.time() - platt_start_time_svm\n",
    "    if fitted_svm_base and platt_scaler_svm:\n",
    "        logging.info(f\"--- [{model_name_svm}] Platt scaling finished in {platt_duration_svm:.2f} seconds ---\")\n",
    "        # Optional: Save models\n",
    "        # joblib.dump(...)\n",
    "    else:\n",
    "        logging.error(f\"[{model_name_svm}] Failed to train base model or Platt scaler.\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_svm}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "\n",
    "# --- 1.4 SVM: Calibrate ICP Threshold (using Calibration Set) ---\n",
    "q_threshold_svm = None\n",
    "if fitted_svm_base and platt_scaler_svm:\n",
    "    logging.info(f\"--- [{model_name_svm}] Calibrating Conformal Prediction (alpha={ALPHA}) ---\")\n",
    "    icp_cal_start_time_svm = time.time()\n",
    "    decision_scores_cal_svm = fitted_svm_base.decision_function(X_cal_scaled) # Use scaled cal data\n",
    "    calibrated_probs_cal_svm = platt_scaler_svm.predict_proba(decision_scores_cal_svm.reshape(-1, 1))\n",
    "    ncm_scores_cal_svm = calculate_ncm_scores(calibrated_probs_cal_svm, y_cal.values) # Use .values\n",
    "    q_threshold_svm = calibrate_conformal_threshold(ncm_scores_cal_svm, ALPHA)\n",
    "    icp_cal_duration_svm = time.time() - icp_cal_start_time_svm\n",
    "    logging.info(f\"--- [{model_name_svm}] ICP calibration finished in {icp_cal_duration_svm:.2f} seconds. Threshold={q_threshold_svm:.6f} ---\")\n",
    "    # Optional: Save threshold\n",
    "    # joblib.dump(...)\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_svm}] Skipping ICP calibration.\")\n",
    "\n",
    "\n",
    "# --- 1.5 SVM: Final Evaluation (using Test Set) ---\n",
    "if fitted_svm_base and platt_scaler_svm:\n",
    "    logging.info(f\"--- [{model_name_svm}] Final Evaluation on Test Set ---\")\n",
    "    eval_start_time_svm = time.time()\n",
    "    decision_scores_test_svm = fitted_svm_base.decision_function(X_test_scaled) # Use scaled test data\n",
    "    calibrated_probs_test_svm = platt_scaler_svm.predict_proba(decision_scores_test_svm.reshape(-1, 1))\n",
    "    y_proba_test_svm = calibrated_probs_test_svm[:, 1]\n",
    "    y_pred_test_svm = (y_proba_test_svm >= 0.5).astype(int)\n",
    "\n",
    "    metrics_svm = calculate_metrics(y_test, y_pred_test_svm, y_proba_test_svm, model_name=model_name_svm)\n",
    "\n",
    "    cp_coverage_svm, cp_avg_set_size_svm = None, None\n",
    "    if q_threshold_svm is not None:\n",
    "        prediction_sets_test_svm = predict_conformal_sets(calibrated_probs_test_svm, q_threshold_svm)\n",
    "        cp_coverage_svm, cp_avg_set_size_svm = evaluate_conformal_prediction(\n",
    "            y_test, prediction_sets_test_svm, ALPHA, model_name=model_name_svm\n",
    "        )\n",
    "    else:\n",
    "         logging.warning(f\"[{model_name_svm}] No CP threshold, skipping CP evaluation.\")\n",
    "\n",
    "    eval_duration_svm = time.time() - eval_start_time_svm\n",
    "    logging.info(f\"--- [{model_name_svm}] Evaluation finished in {eval_duration_svm:.2f} seconds ---\")\n",
    "\n",
    "    # Store results\n",
    "    all_results[model_name_svm] = {\n",
    "        'metrics': metrics_svm,\n",
    "        'cp_coverage': cp_coverage_svm,\n",
    "        'cp_avg_set_size': cp_avg_set_size_svm,\n",
    "        'best_hpo_params': best_params_svm,\n",
    "        'hpo_f1_score': best_score_hpo_svm,\n",
    "        'hpo_duration_s': hpo_duration_svm,\n",
    "        'q_threshold': q_threshold_svm\n",
    "    }\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_svm}] Skipping final evaluation.\")\n",
    "\n",
    "logging.info(f\"===== Finished Workflow for {model_name_svm} =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for CART (using data fraction)\n",
    "MAX_RESOURCE_CART = 1.0\n",
    "MIN_RESOURCE_CART = 0.1 # Can start with smaller fraction for trees\n",
    "ETA_CART = 3\n",
    "RESOURCE_TYPE_CART = 'data_fraction'\n",
    "model_name_cart = \"CART\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¡MODIFICADO PARA PRUEBA RÁPIDA!\n",
    "MAX_RESOURCE_CART = 1.0\n",
    "MIN_RESOURCE_CART = 0.5 # Aumentado para reducir s_max\n",
    "ETA_CART = 4            # Aumentado para eliminar configuraciones más rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "\n",
    "logging.info(f\"\\n\\n===== Starting Workflow for {model_name_cart} =====\")\n",
    "timestamp_cart = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "hpo_start_time_cart = time.time()\n",
    "\n",
    "# --- 2.1 CART: Define Search Space and HPO Params ---\n",
    "param_space_cart = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': randint(3, 50),\n",
    "    'min_samples_split': randint(2, 100),\n",
    "    'min_samples_leaf': randint(1, 50),\n",
    "    # class_weight added automatically\n",
    "    # random_state added automatically\n",
    "}\n",
    "\n",
    "# --- 2.2 CART: Run Hyperband HPO ---\n",
    "logging.info(f\"--- [{model_name_cart}] Running Hyperband HPO ---\")\n",
    "best_params_cart, best_score_hpo_cart = hyperband_hpo(\n",
    "    model_class=DecisionTreeClassifier,\n",
    "    param_space=param_space_cart,\n",
    "    X_train=X_train, # USE UNSCALED DATA\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,     # USE UNSCALED DATA\n",
    "    y_val=y_val,\n",
    "    max_resource=MAX_RESOURCE_CART,\n",
    "    eta=ETA_CART,\n",
    "    resource_type=RESOURCE_TYPE_CART,\n",
    "    min_resource=MIN_RESOURCE_CART,\n",
    "    scoring_func=f1_score,\n",
    "    random_state=RANDOM_SEED,\n",
    "    fit_params={}\n",
    ")\n",
    "hpo_duration_cart = time.time() - hpo_start_time_cart\n",
    "logging.info(f\"--- [{model_name_cart}] HPO finished in {hpo_duration_cart:.2f} seconds ---\")\n",
    "\n",
    "# --- 2.3 CART: Train Final Model & Platt Scaler ---\n",
    "fitted_cart_base = None\n",
    "platt_scaler_cart = None\n",
    "if best_params_cart:\n",
    "    logging.info(f\"--- [{model_name_cart}] Training final model and Platt scaler ---\")\n",
    "    platt_start_time_cart = time.time()\n",
    "    # Ensure necessary fixed parameters are present for the final fit\n",
    "    best_params_cart['random_state'] = RANDOM_SEED\n",
    "    if 'class_weight' not in best_params_cart: best_params_cart['class_weight'] = 'balanced'\n",
    "\n",
    "    # Use the modified train_platt_scaler with predict_proba\n",
    "    fitted_cart_base, platt_scaler_cart = train_platt_scaler(\n",
    "        base_estimator_class=DecisionTreeClassifier,\n",
    "        best_params=best_params_cart,\n",
    "        X_train=X_train, # Use UNSCALED training data\n",
    "        y_train=y_train,\n",
    "        score_method='predict_proba', # <<< Specify score method for CART\n",
    "        n_splits=5,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    platt_duration_cart = time.time() - platt_start_time_cart\n",
    "    if fitted_cart_base and platt_scaler_cart:\n",
    "        logging.info(f\"--- [{model_name_cart}] Platt scaling finished in {platt_duration_cart:.2f} seconds ---\")\n",
    "        # Optional: Save models\n",
    "        # joblib.dump(...)\n",
    "    else:\n",
    "        logging.error(f\"[{model_name_cart}] Failed to train base model or Platt scaler.\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_cart}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "\n",
    "# --- 2.4 CART: Calibrate ICP Threshold (using Calibration Set) ---\n",
    "q_threshold_cart = None\n",
    "if fitted_cart_base and platt_scaler_cart:\n",
    "    logging.info(f\"--- [{model_name_cart}] Calibrating Conformal Prediction (alpha={ALPHA}) ---\")\n",
    "    icp_cal_start_time_cart = time.time()\n",
    "    # Get base model probabilities (class 1) for calibration set\n",
    "    base_probs_cal_cart = fitted_cart_base.predict_proba(X_cal)[:, 1].reshape(-1, 1) # UNSCALED cal data\n",
    "    # Get calibrated probabilities from Platt scaler\n",
    "    calibrated_probs_cal_cart = platt_scaler_cart.predict_proba(base_probs_cal_cart)\n",
    "    ncm_scores_cal_cart = calculate_ncm_scores(calibrated_probs_cal_cart, y_cal.values) # Use .values\n",
    "    q_threshold_cart = calibrate_conformal_threshold(ncm_scores_cal_cart, ALPHA)\n",
    "    icp_cal_duration_cart = time.time() - icp_cal_start_time_cart\n",
    "    logging.info(f\"--- [{model_name_cart}] ICP calibration finished in {icp_cal_duration_cart:.2f} seconds. Threshold={q_threshold_cart:.6f} ---\")\n",
    "    # Optional: Save threshold\n",
    "    # joblib.dump(...)\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_cart}] Skipping ICP calibration.\")\n",
    "\n",
    "\n",
    "# --- 2.5 CART: Final Evaluation (using Test Set) ---\n",
    "if fitted_cart_base and platt_scaler_cart:\n",
    "    logging.info(f\"--- [{model_name_cart}] Final Evaluation on Test Set ---\")\n",
    "    eval_start_time_cart = time.time()\n",
    "    # Get base model probabilities (class 1) for test set\n",
    "    base_probs_test_cart = fitted_cart_base.predict_proba(X_test)[:, 1].reshape(-1, 1) # UNSCALED test data\n",
    "    # Get calibrated probabilities from Platt scaler\n",
    "    calibrated_probs_test_cart = platt_scaler_cart.predict_proba(base_probs_test_cart)\n",
    "    y_proba_test_cart = calibrated_probs_test_cart[:, 1] # Probability of positive class\n",
    "    y_pred_test_cart = (y_proba_test_cart >= 0.5).astype(int) # Threshold calibrated probabilities\n",
    "\n",
    "    metrics_cart = calculate_metrics(y_test, y_pred_test_cart, y_proba_test_cart, model_name=model_name_cart)\n",
    "\n",
    "    cp_coverage_cart, cp_avg_set_size_cart = None, None\n",
    "    if q_threshold_cart is not None:\n",
    "        # Use calibrated probabilities for prediction set generation\n",
    "        prediction_sets_test_cart = predict_conformal_sets(calibrated_probs_test_cart, q_threshold_cart)\n",
    "        cp_coverage_cart, cp_avg_set_size_cart = evaluate_conformal_prediction(\n",
    "            y_test, prediction_sets_test_cart, ALPHA, model_name=model_name_cart\n",
    "        )\n",
    "    else:\n",
    "         logging.warning(f\"[{model_name_cart}] No CP threshold, skipping CP evaluation.\")\n",
    "\n",
    "    eval_duration_cart = time.time() - eval_start_time_cart\n",
    "    logging.info(f\"--- [{model_name_cart}] Evaluation finished in {eval_duration_cart:.2f} seconds ---\")\n",
    "\n",
    "    # Store results\n",
    "    all_results[model_name_cart] = {\n",
    "        'metrics': metrics_cart,\n",
    "        'cp_coverage': cp_coverage_cart,\n",
    "        'cp_avg_set_size': cp_avg_set_size_cart,\n",
    "        'best_hpo_params': best_params_cart,\n",
    "        'hpo_f1_score': best_score_hpo_cart,\n",
    "        'hpo_duration_s': hpo_duration_cart,\n",
    "        'q_threshold': q_threshold_cart # Store the threshold\n",
    "    }\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_cart}] Skipping final evaluation.\")\n",
    "\n",
    "logging.info(f\"===== Finished Workflow for {model_name_cart} =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for RF (using iterations)\n",
    "MAX_RESOURCE_RF = 300  # Max n_estimators\n",
    "MIN_RESOURCE_RF = 20   # Min n_estimators\n",
    "ETA_RF = 3\n",
    "RESOURCE_TYPE_RF = 'iterations'\n",
    "model_name_rf = \"Random_Forest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¡MODIFICADO PARA PRUEBA RÁPIDA!\n",
    "MAX_RESOURCE_RF = 20   # ¡Reducido drásticamente! (Antes 300)\n",
    "MIN_RESOURCE_RF = 5    # Mínimo bajo pero cercano a max para pocos brackets\n",
    "ETA_RF = 4             # Aumentado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint\n",
    "\n",
    "\n",
    "logging.info(f\"\\n\\n===== Starting Workflow for {model_name_rf} =====\")\n",
    "timestamp_rf = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "hpo_start_time_rf = time.time()\n",
    "\n",
    "# --- 3.1 RF: Define Search Space and HPO Params ---\n",
    "param_space_rf = {\n",
    "    # n_estimators is controlled by resource_type='iterations'\n",
    "    'max_depth': randint(5, 50),\n",
    "    'min_samples_split': randint(2, 50),\n",
    "    'min_samples_leaf': randint(1, 25),\n",
    "    'max_features': ['sqrt', 'log2', None], # None means max_features=n_features\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    # class_weight added automatically\n",
    "    # random_state added automatically\n",
    "}\n",
    "\n",
    "# --- 3.2 RF: Run Hyperband HPO ---\n",
    "logging.info(f\"--- [{model_name_rf}] Running Hyperband HPO ---\")\n",
    "best_params_rf, best_score_hpo_rf = hyperband_hpo(\n",
    "    model_class=RandomForestClassifier,\n",
    "    param_space=param_space_rf,\n",
    "    X_train=X_train, # USE UNSCALED DATA\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,     # USE UNSCALED DATA\n",
    "    y_val=y_val,\n",
    "    max_resource=MAX_RESOURCE_RF,\n",
    "    eta=ETA_RF,\n",
    "    resource_type=RESOURCE_TYPE_RF,\n",
    "    min_resource=MIN_RESOURCE_RF,\n",
    "    scoring_func=f1_score,\n",
    "    random_state=RANDOM_SEED,\n",
    "    fit_params={} # No special fit_params needed\n",
    ")\n",
    "hpo_duration_rf = time.time() - hpo_start_time_rf\n",
    "logging.info(f\"--- [{model_name_rf}] HPO finished in {hpo_duration_rf:.2f} seconds ---\")\n",
    "\n",
    "\n",
    "# --- 3.3 RF: Train Final Model & Platt Scaler ---\n",
    "fitted_rf_base = None\n",
    "platt_scaler_rf = None\n",
    "if best_params_rf:\n",
    "    logging.info(f\"--- [{model_name_rf}] Training final model and Platt scaler ---\")\n",
    "    platt_start_time_rf = time.time()\n",
    "    # Ensure necessary fixed parameters are present\n",
    "    best_params_rf['random_state'] = RANDOM_SEED\n",
    "    if 'class_weight' not in best_params_rf: best_params_rf['class_weight'] = 'balanced'\n",
    "    best_params_rf['n_jobs'] = -1 # Use all cores\n",
    "\n",
    "    # Use the modified train_platt_scaler with predict_proba\n",
    "    fitted_rf_base, platt_scaler_rf = train_platt_scaler(\n",
    "        base_estimator_class=RandomForestClassifier,\n",
    "        best_params=best_params_rf,\n",
    "        X_train=X_train, # Use UNSCALED training data\n",
    "        y_train=y_train,\n",
    "        score_method='predict_proba', # <<< Specify score method for RF\n",
    "        n_splits=5,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    platt_duration_rf = time.time() - platt_start_time_rf\n",
    "    if fitted_rf_base and platt_scaler_rf:\n",
    "        logging.info(f\"--- [{model_name_rf}] Platt scaling finished in {platt_duration_rf:.2f} seconds ---\")\n",
    "        # Optional: Save models\n",
    "        # joblib.dump(...)\n",
    "    else:\n",
    "        logging.error(f\"[{model_name_rf}] Failed to train base model or Platt scaler.\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_rf}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "\n",
    "# --- 3.4 RF: Calibrate ICP Threshold ---\n",
    "# (Keep this section as it is, but verify input to platt_scaler_rf.predict_proba)\n",
    "q_threshold_rf = None\n",
    "if fitted_rf_base and platt_scaler_rf:\n",
    "    logging.info(f\"--- [{model_name_rf}] Calibrating Conformal Prediction (alpha={ALPHA}) ---\")\n",
    "    icp_cal_start_time_rf = time.time()\n",
    "    # *** Verify this line ***\n",
    "    # Get base model probabilities (class 1) for calibration set\n",
    "    base_probs_cal_rf = fitted_rf_base.predict_proba(X_cal)[:, 1].reshape(-1, 1) # UNSCALED cal data, prob class 1\n",
    "    calibrated_probs_cal_rf = platt_scaler_rf.predict_proba(base_probs_cal_rf) # Get calibrated probs for BOTH classes\n",
    "    ncm_scores_cal_rf = calculate_ncm_scores(calibrated_probs_cal_rf, y_cal.values)\n",
    "    q_threshold_rf = calibrate_conformal_threshold(ncm_scores_cal_rf, ALPHA)\n",
    "    icp_cal_duration_rf = time.time() - icp_cal_start_time_rf\n",
    "    logging.info(f\"--- [{model_name_rf}] ICP calibration finished in {icp_cal_duration_rf:.2f} seconds. Threshold={q_threshold_rf:.6f} ---\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_rf}] Skipping ICP calibration.\")\n",
    "\n",
    "# --- 3.5 RF: Final Evaluation ---\n",
    "# (Keep this section as it is, but verify input to platt_scaler_rf.predict_proba)\n",
    "if fitted_rf_base and platt_scaler_rf:\n",
    "    logging.info(f\"--- [{model_name_rf}] Final Evaluation on Test Set ---\")\n",
    "    eval_start_time_rf = time.time()\n",
    "    # *** Verify this line ***\n",
    "    # Get base model probabilities (class 1) for test set\n",
    "    base_probs_test_rf = fitted_rf_base.predict_proba(X_test)[:, 1].reshape(-1, 1) # UNSCALED test data, prob class 1\n",
    "    calibrated_probs_test_rf = platt_scaler_rf.predict_proba(base_probs_test_rf)\n",
    "    y_proba_test_rf = calibrated_probs_test_rf[:, 1]\n",
    "    y_pred_test_rf = (y_proba_test_rf >= 0.5).astype(int)\n",
    "\n",
    "    metrics_rf = calculate_metrics(y_test, y_pred_test_rf, y_proba_test_rf, model_name=model_name_rf)\n",
    "\n",
    "    cp_coverage_rf, cp_avg_set_size_rf = None, None\n",
    "    if q_threshold_rf is not None:\n",
    "        # Use calibrated probabilities\n",
    "        prediction_sets_test_rf = predict_conformal_sets(calibrated_probs_test_rf, q_threshold_rf)\n",
    "        cp_coverage_rf, cp_avg_set_size_rf = evaluate_conformal_prediction(\n",
    "            y_test, prediction_sets_test_rf, ALPHA, model_name=model_name_rf\n",
    "        )\n",
    "    else:\n",
    "         logging.warning(f\"[{model_name_rf}] No CP threshold, skipping CP evaluation.\")\n",
    "\n",
    "    eval_duration_rf = time.time() - eval_start_time_rf\n",
    "    logging.info(f\"--- [{model_name_rf}] Evaluation finished in {eval_duration_rf:.2f} seconds ---\")\n",
    "\n",
    "    # Store results\n",
    "    all_results[model_name_rf] = {\n",
    "        'metrics': metrics_rf,\n",
    "        'cp_coverage': cp_coverage_rf,\n",
    "        'cp_avg_set_size': cp_avg_set_size_rf,\n",
    "        'best_hpo_params': best_params_rf,\n",
    "        'hpo_f1_score': best_score_hpo_rf,\n",
    "        'hpo_duration_s': hpo_duration_rf,\n",
    "        'q_threshold': q_threshold_rf\n",
    "    }\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_rf}] Skipping final evaluation.\")\n",
    "\n",
    "logging.info(f\"===== Finished Workflow for {model_name_rf} =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for XGB (using iterations)\n",
    "MAX_RESOURCE_XGB = 500 # Max n_estimators\n",
    "MIN_RESOURCE_XGB = 30  # Min n_estimators\n",
    "ETA_XGB = 3\n",
    "RESOURCE_TYPE_XGB = 'iterations'\n",
    "model_name_xgb = \"XGBoost\"\n",
    "\n",
    "# Fit params for early stopping within HPO\n",
    "fit_params_xgb_hpo = {\n",
    "    'early_stopping_rounds': 5, #!15,\n",
    "    # 'eval_set': Will be set inside _train_and_eval using X_val, y_val\n",
    "    'verbose': False # Suppress verbose output during HPO fitting\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¡MODIFICADO PARA PRUEBA RÁPIDA!\n",
    "MAX_RESOURCE_XGB = 30  # ¡Reducido drásticamente! (Antes 500)\n",
    "MIN_RESOURCE_XGB = 10  # Mínimo bajo pero cercano a max\n",
    "ETA_XGB = 4            # Aumentado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "\n",
    "logging.info(f\"\\n\\n===== Starting Workflow for {model_name_xgb} =====\")\n",
    "timestamp_xgb = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "hpo_start_time_xgb = time.time()\n",
    "\n",
    "# --- 4.1 XGB: Define Search Space and HPO Params ---\n",
    "param_space_xgb = {\n",
    "    # n_estimators controlled by resource\n",
    "    'learning_rate': loguniform(0.01, 0.3),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'subsample': uniform(0.6, 0.4), # range [0.6, 1.0)\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'gamma': loguniform(1e-2, 1.0), # Min loss reduction\n",
    "    'reg_alpha': loguniform(1e-3, 1.0), # L1 reg\n",
    "    'reg_lambda': loguniform(1e-3, 1.0), # L2 reg\n",
    "    # scale_pos_weight added automatically\n",
    "    # random_state added automatically\n",
    "    'objective': ['binary:logistic'], # Fixed objective\n",
    "    'eval_metric': ['logloss'],        # Fixed eval metric for early stopping\n",
    "    'use_label_encoder': [False]       # Deprecated, set to False\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# --- 4.2 XGB: Run Hyperband HPO ---\n",
    "logging.info(f\"--- [{model_name_xgb}] Running Hyperband HPO ---\")\n",
    "best_params_xgb, best_score_hpo_xgb = hyperband_hpo(\n",
    "    model_class=xgb.XGBClassifier,\n",
    "    param_space=param_space_xgb,\n",
    "    X_train=X_train, # USE UNSCALED DATA\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,     # USE UNSCALED DATA\n",
    "    y_val=y_val,\n",
    "    max_resource=MAX_RESOURCE_XGB,\n",
    "    eta=ETA_XGB,\n",
    "    resource_type=RESOURCE_TYPE_XGB,\n",
    "    min_resource=MIN_RESOURCE_XGB,\n",
    "    scoring_func=f1_score,\n",
    "    random_state=RANDOM_SEED,\n",
    "    fit_params=fit_params_xgb_hpo\n",
    ")\n",
    "hpo_duration_xgb = time.time() - hpo_start_time_xgb\n",
    "logging.info(f\"--- [{model_name_xgb}] HPO finished in {hpo_duration_xgb:.2f} seconds ---\")\n",
    "\n",
    "\n",
    "# --- 4.3 XGB: Train Final Model & Platt Scaler ---\n",
    "fitted_xgb_base = None\n",
    "platt_scaler_xgb = None\n",
    "final_best_params_xgb = None # Initialize\n",
    "\n",
    "if best_params_xgb:\n",
    "    logging.info(f\"--- [{model_name_xgb}] Determining best iteration and training Platt scaler ---\")\n",
    "    platt_start_time_xgb = time.time()\n",
    "\n",
    "    # 1. Determine best iteration using early stopping on validation set\n",
    "    temp_best_params_xgb = best_params_xgb.copy() # Work with a copy\n",
    "    temp_best_params_xgb['random_state'] = RANDOM_SEED\n",
    "    if 'objective' not in temp_best_params_xgb: temp_best_params_xgb['objective'] = 'binary:logistic'\n",
    "    if 'eval_metric' not in temp_best_params_xgb: temp_best_params_xgb['eval_metric'] = 'logloss'\n",
    "    if 'use_label_encoder' not in temp_best_params_xgb: temp_best_params_xgb['use_label_encoder'] = False\n",
    "    if 'n_jobs' not in temp_best_params_xgb: temp_best_params_xgb['n_jobs'] = -1\n",
    "    if 'scale_pos_weight' not in temp_best_params_xgb:\n",
    "         neg_count = (y_train == 0).sum(); pos_count = (y_train == 1).sum()\n",
    "         if pos_count > 0: temp_best_params_xgb['scale_pos_weight'] = neg_count / pos_count\n",
    "\n",
    "    logging.info(\"Training temporary XGBoost with early stopping to find best iteration...\")\n",
    "    temp_xgb_model = xgb.XGBClassifier(**temp_best_params_xgb)\n",
    "    # Ensure X_val, y_val are appropriate (unscaled)\n",
    "    eval_set_final = [(X_val, y_val)]\n",
    "    temp_xgb_model.fit(X_train, y_train,\n",
    "                       early_stopping_rounds=20,\n",
    "                       eval_set=eval_set_final,\n",
    "                       verbose=False)\n",
    "    best_iteration = temp_xgb_model.best_iteration\n",
    "    logging.info(f\"Best iteration found: {best_iteration}\")\n",
    "\n",
    "    # Update best_params with the optimal number of estimators found\n",
    "    final_best_params_xgb = temp_best_params_xgb.copy()\n",
    "    final_best_params_xgb['n_estimators'] = best_iteration if best_iteration is not None and best_iteration > 0 else MAX_RESOURCE_XGB\n",
    "\n",
    "    # 2. Train final model and Platt scaler using train_platt_scaler\n",
    "    logging.info(f\"--- [{model_name_xgb}] Training final model ({final_best_params_xgb['n_estimators']} est.) and Platt scaler ---\")\n",
    "    fitted_xgb_base, platt_scaler_xgb = train_platt_scaler(\n",
    "        base_estimator_class=xgb.XGBClassifier,\n",
    "        best_params=final_best_params_xgb, # Use params with best_iteration\n",
    "        X_train=X_train, # Use UNSCALED training data\n",
    "        y_train=y_train,\n",
    "        score_method='raw_margin_xgb', # <<< Specify score method for XGB\n",
    "        n_splits=5,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    platt_duration_xgb = time.time() - platt_start_time_xgb\n",
    "    if fitted_xgb_base and platt_scaler_xgb:\n",
    "        logging.info(f\"--- [{model_name_xgb}] Platt scaling finished in {platt_duration_xgb:.2f} seconds ---\")\n",
    "        # Optional: Save models\n",
    "        # joblib.dump(...)\n",
    "    else:\n",
    "        logging.error(f\"[{model_name_xgb}] Failed to train base model or Platt scaler.\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_xgb}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "\n",
    "\n",
    "# --- 4.4 XGB: Calibrate ICP Threshold ---\n",
    "# (Keep this section as it is, but verify input to platt_scaler_xgb.predict_proba)\n",
    "q_threshold_xgb = None\n",
    "if fitted_xgb_base and platt_scaler_xgb:\n",
    "    logging.info(f\"--- [{model_name_xgb}] Calibrating Conformal Prediction (alpha={ALPHA}) ---\")\n",
    "    icp_cal_start_time_xgb = time.time()\n",
    "    # *** Verify this line ***\n",
    "    # Get base model raw margins for calibration set\n",
    "    base_raw_cal_xgb = fitted_xgb_base.predict(X_cal, output_margin=True).reshape(-1, 1) # UNSCALED cal data\n",
    "    calibrated_probs_cal_xgb = platt_scaler_xgb.predict_proba(base_raw_cal_xgb)\n",
    "    ncm_scores_cal_xgb = calculate_ncm_scores(calibrated_probs_cal_xgb, y_cal.values)\n",
    "    q_threshold_xgb = calibrate_conformal_threshold(ncm_scores_cal_xgb, ALPHA)\n",
    "    icp_cal_duration_xgb = time.time() - icp_cal_start_time_xgb\n",
    "    logging.info(f\"--- [{model_name_xgb}] ICP calibration finished in {icp_cal_duration_xgb:.2f} seconds. Threshold={q_threshold_xgb:.6f} ---\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_xgb}] Skipping ICP calibration.\")\n",
    "\n",
    "\n",
    "# --- 4.5 XGB: Final Evaluation ---\n",
    "# (Keep this section as it is, but verify input to platt_scaler_xgb.predict_proba)\n",
    "if fitted_xgb_base and platt_scaler_xgb:\n",
    "    logging.info(f\"--- [{model_name_xgb}] Final Evaluation on Test Set ---\")\n",
    "    eval_start_time_xgb = time.time()\n",
    "    # *** Verify this line ***\n",
    "    # Get base model raw margins for test set\n",
    "    base_raw_test_xgb = fitted_xgb_base.predict(X_test, output_margin=True).reshape(-1, 1) # UNSCALED test data\n",
    "    calibrated_probs_test_xgb = platt_scaler_xgb.predict_proba(base_raw_test_xgb)\n",
    "    y_proba_test_xgb = calibrated_probs_test_xgb[:, 1]\n",
    "    y_pred_test_xgb = (y_proba_test_xgb >= 0.5).astype(int)\n",
    "\n",
    "    metrics_xgb = calculate_metrics(y_test, y_pred_test_xgb, y_proba_test_xgb, model_name=model_name_xgb)\n",
    "\n",
    "    cp_coverage_xgb, cp_avg_set_size_xgb = None, None\n",
    "    if q_threshold_xgb is not None:\n",
    "        # Use calibrated probabilities\n",
    "        prediction_sets_test_xgb = predict_conformal_sets(calibrated_probs_test_xgb, q_threshold_xgb)\n",
    "        cp_coverage_xgb, cp_avg_set_size_xgb = evaluate_conformal_prediction(\n",
    "            y_test, prediction_sets_test_xgb, ALPHA, model_name=model_name_xgb\n",
    "        )\n",
    "    else:\n",
    "         logging.warning(f\"[{model_name_xgb}] No CP threshold, skipping CP evaluation.\")\n",
    "\n",
    "    eval_duration_xgb = time.time() - eval_start_time_xgb\n",
    "    logging.info(f\"--- [{model_name_xgb}] Evaluation finished in {eval_duration_xgb:.2f} seconds ---\")\n",
    "\n",
    "    # Store results\n",
    "    all_results[model_name_xgb] = {\n",
    "        'metrics': metrics_xgb,\n",
    "        'cp_coverage': cp_coverage_xgb,\n",
    "        'cp_avg_set_size': cp_avg_set_size_xgb,\n",
    "        'best_hpo_params': best_params_xgb, # Original HPO params\n",
    "        # Store actual used estimators if available\n",
    "        'final_n_estimators': final_best_params_xgb.get('n_estimators', None) if final_best_params_xgb else None,\n",
    "        'hpo_f1_score': best_score_hpo_xgb,\n",
    "        'hpo_duration_s': hpo_duration_xgb,\n",
    "        'q_threshold': q_threshold_xgb\n",
    "    }\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_xgb}] Skipping final evaluation.\")\n",
    "\n",
    "logging.info(f\"===== Finished Workflow for {model_name_xgb} =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for LGBM (using iterations)\n",
    "MAX_RESOURCE_LGBM = 500 # Max n_estimators\n",
    "MIN_RESOURCE_LGBM = 30  # Min n_estimators\n",
    "ETA_LGBM = 3\n",
    "RESOURCE_TYPE_LGBM = 'iterations'\n",
    "model_name_lgbm = \"LightGBM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¡MODIFICADO PARA PRUEBA RÁPIDA!\n",
    "MAX_RESOURCE_LGBM = 30  # ¡Reducido drásticamente! (Antes 500)\n",
    "MIN_RESOURCE_LGBM = 10  # Mínimo bajo pero cercano a max\n",
    "ETA_LGBM = 4            # Aumentado\n",
    "RESOURCE_TYPE_LGBM = 'iterations'\n",
    "\n",
    "# Fit params for early stopping within HPO\n",
    "# Newer LightGBM uses callbacks\n",
    "fit_params_lgbm_hpo = {\n",
    "    # 'eval_set': Will be set inside _train_and_eval\n",
    "    # Use callbacks for early stopping\n",
    "    'callbacks': [early_stopping(stopping_rounds=5, #!15, \n",
    "                                 verbose=False\n",
    "                                 )]\n",
    "    # 'verbose': -1 # Suppress verbose output during HPO fitting (or False) - use callback instead\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "# Note: LightGBM early stopping might need callbacks depending on version\n",
    "# We'll try passing via fit_params first, but might need adjustment\n",
    "from lightgbm import early_stopping # For newer versions\n",
    "\n",
    "\n",
    "logging.info(f\"\\n\\n===== Starting Workflow for {model_name_lgbm} =====\")\n",
    "timestamp_lgbm = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "hpo_start_time_lgbm = time.time()\n",
    "\n",
    "# --- 5.1 LGBM: Define Search Space and HPO Params ---\n",
    "param_space_lgbm = {\n",
    "    # n_estimators controlled by resource\n",
    "    'learning_rate': loguniform(0.01, 0.3),\n",
    "    'num_leaves': randint(20, 100),\n",
    "    'max_depth': randint(3, 15), # Often kept lower than XGB depth\n",
    "    'subsample': uniform(0.6, 0.4), # Aliased as bagging_fraction\n",
    "    'colsample_bytree': uniform(0.6, 0.4), # Aliased as feature_fraction\n",
    "    'reg_alpha': loguniform(1e-3, 1.0), # L1\n",
    "    'reg_lambda': loguniform(1e-3, 1.0), # L2\n",
    "    # scale_pos_weight or is_unbalance=True added automatically\n",
    "    # random_state added automatically\n",
    "    'objective': ['binary'], # Fixed objective\n",
    "    'metric': ['logloss'],   # Fixed metric for early stopping\n",
    "}\n",
    "\n",
    "\n",
    "# --- 5.2 LGBM: Run Hyperband HPO ---\n",
    "logging.info(f\"--- [{model_name_lgbm}] Running Hyperband HPO ---\")\n",
    "best_params_lgbm, best_score_hpo_lgbm = hyperband_hpo(\n",
    "    model_class=lgb.LGBMClassifier,\n",
    "    param_space=param_space_lgbm,\n",
    "    X_train=X_train, # USE UNSCALED DATA\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,     # USE UNSCALED DATA\n",
    "    y_val=y_val,\n",
    "    max_resource=MAX_RESOURCE_LGBM,\n",
    "    eta=ETA_LGBM,\n",
    "    resource_type=RESOURCE_TYPE_LGBM,\n",
    "    min_resource=MIN_RESOURCE_LGBM,\n",
    "    scoring_func=f1_score,\n",
    "    random_state=RANDOM_SEED,\n",
    "    fit_params=fit_params_lgbm_hpo\n",
    ")\n",
    "hpo_duration_lgbm = time.time() - hpo_start_time_lgbm\n",
    "logging.info(f\"--- [{model_name_lgbm}] HPO finished in {hpo_duration_lgbm:.2f} seconds ---\")\n",
    "\n",
    "\n",
    "# --- 5.3 LGBM: Train Final Model & Platt Scaler ---\n",
    "fitted_lgbm_base = None\n",
    "platt_scaler_lgbm = None\n",
    "final_best_params_lgbm = None # Initialize\n",
    "\n",
    "if best_params_lgbm:\n",
    "    logging.info(f\"--- [{model_name_lgbm}] Determining best iteration and training Platt scaler ---\")\n",
    "    platt_start_time_lgbm = time.time()\n",
    "\n",
    "    # 1. Determine best iteration using early stopping on validation set\n",
    "    temp_best_params_lgbm = best_params_lgbm.copy() # Work with a copy\n",
    "    temp_best_params_lgbm['random_state'] = RANDOM_SEED\n",
    "    if 'objective' not in temp_best_params_lgbm: temp_best_params_lgbm['objective'] = 'binary'\n",
    "    if 'metric' not in temp_best_params_lgbm: temp_best_params_lgbm['metric'] = 'logloss'\n",
    "    if 'n_jobs' not in temp_best_params_lgbm: temp_best_params_lgbm['n_jobs'] = -1\n",
    "    if 'scale_pos_weight' not in temp_best_params_lgbm:\n",
    "        neg_count = (y_train == 0).sum(); pos_count = (y_train == 1).sum()\n",
    "        if pos_count > 0:\n",
    "            temp_best_params_lgbm['scale_pos_weight'] = neg_count / pos_count\n",
    "            if 'is_unbalance' in temp_best_params_lgbm: del temp_best_params_lgbm['is_unbalance']\n",
    "        elif 'is_unbalance' not in temp_best_params_lgbm:\n",
    "            temp_best_params_lgbm['is_unbalance'] = True\n",
    "\n",
    "    logging.info(\"Training temporary LightGBM with early stopping to find best iteration...\")\n",
    "    temp_lgbm_model = lgb.LGBMClassifier(**temp_best_params_lgbm)\n",
    "    eval_set_final_lgbm = [(X_val, y_val)]\n",
    "    callbacks_final = [early_stopping(stopping_rounds=20, verbose=False)]\n",
    "\n",
    "    temp_lgbm_model.fit(X_train, y_train,\n",
    "                         eval_set=eval_set_final_lgbm,\n",
    "                         callbacks=callbacks_final)\n",
    "\n",
    "    best_iteration_lgbm = temp_lgbm_model.best_iteration_\n",
    "    logging.info(f\"Best iteration found: {best_iteration_lgbm}\")\n",
    "\n",
    "    # Update best_params with the optimal number of estimators found\n",
    "    final_best_params_lgbm = temp_best_params_lgbm.copy()\n",
    "    final_best_params_lgbm['n_estimators'] = best_iteration_lgbm if best_iteration_lgbm is not None and best_iteration_lgbm > 0 else MAX_RESOURCE_LGBM\n",
    "\n",
    "    # 2. Train final model and Platt scaler using train_platt_scaler\n",
    "    logging.info(f\"--- [{model_name_lgbm}] Training final model ({final_best_params_lgbm['n_estimators']} est.) and Platt scaler ---\")\n",
    "    fitted_lgbm_base, platt_scaler_lgbm = train_platt_scaler(\n",
    "        base_estimator_class=lgb.LGBMClassifier,\n",
    "        best_params=final_best_params_lgbm, # Use params with best_iteration\n",
    "        X_train=X_train, # Use UNSCALED training data\n",
    "        y_train=y_train,\n",
    "        score_method='raw_score_lgbm', # <<< Specify score method for LGBM\n",
    "        n_splits=5,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    platt_duration_lgbm = time.time() - platt_start_time_lgbm\n",
    "    if fitted_lgbm_base and platt_scaler_lgbm:\n",
    "        logging.info(f\"--- [{model_name_lgbm}] Platt scaling finished in {platt_duration_lgbm:.2f} seconds ---\")\n",
    "        # Optional: Save models\n",
    "        # joblib.dump(...)\n",
    "    else:\n",
    "        logging.error(f\"[{model_name_lgbm}] Failed to train base model or Platt scaler.\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_lgbm}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "\n",
    "\n",
    "# --- 5.4 LGBM: Calibrate ICP Threshold ---\n",
    "# (Keep this section as it is, but verify input to platt_scaler_lgbm.predict_proba)\n",
    "q_threshold_lgbm = None\n",
    "if fitted_lgbm_base and platt_scaler_lgbm:\n",
    "    logging.info(f\"--- [{model_name_lgbm}] Calibrating Conformal Prediction (alpha={ALPHA}) ---\")\n",
    "    icp_cal_start_time_lgbm = time.time()\n",
    "    # *** Verify this line ***\n",
    "    # Get base model raw scores for calibration set\n",
    "    base_raw_cal_lgbm = fitted_lgbm_base.predict(X_cal, raw_score=True).reshape(-1, 1) # UNSCALED cal data\n",
    "    calibrated_probs_cal_lgbm = platt_scaler_lgbm.predict_proba(base_raw_cal_lgbm)\n",
    "    ncm_scores_cal_lgbm = calculate_ncm_scores(calibrated_probs_cal_lgbm, y_cal.values)\n",
    "    q_threshold_lgbm = calibrate_conformal_threshold(ncm_scores_cal_lgbm, ALPHA)\n",
    "    icp_cal_duration_lgbm = time.time() - icp_cal_start_time_lgbm\n",
    "    logging.info(f\"--- [{model_name_lgbm}] ICP calibration finished in {icp_cal_duration_lgbm:.2f} seconds. Threshold={q_threshold_lgbm:.6f} ---\")\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_lgbm}] Skipping ICP calibration.\")\n",
    "\n",
    "\n",
    "# --- 5.5 LGBM: Final Evaluation ---\n",
    "# (Keep this section as it is, but verify input to platt_scaler_lgbm.predict_proba)\n",
    "if fitted_lgbm_base and platt_scaler_lgbm:\n",
    "    logging.info(f\"--- [{model_name_lgbm}] Final Evaluation on Test Set ---\")\n",
    "    eval_start_time_lgbm = time.time()\n",
    "    # *** Verify this line ***\n",
    "    # Get base model raw scores for test set\n",
    "    base_raw_test_lgbm = fitted_lgbm_base.predict(X_test, raw_score=True).reshape(-1, 1) # UNSCALED test data\n",
    "    calibrated_probs_test_lgbm = platt_scaler_lgbm.predict_proba(base_raw_test_lgbm)\n",
    "    y_proba_test_lgbm = calibrated_probs_test_lgbm[:, 1]\n",
    "    y_pred_test_lgbm = (y_proba_test_lgbm >= 0.5).astype(int)\n",
    "\n",
    "    metrics_lgbm = calculate_metrics(y_test, y_pred_test_lgbm, y_proba_test_lgbm, model_name=model_name_lgbm)\n",
    "\n",
    "    cp_coverage_lgbm, cp_avg_set_size_lgbm = None, None\n",
    "    if q_threshold_lgbm is not None:\n",
    "        # Use calibrated probabilities\n",
    "        prediction_sets_test_lgbm = predict_conformal_sets(calibrated_probs_test_lgbm, q_threshold_lgbm)\n",
    "        cp_coverage_lgbm, cp_avg_set_size_lgbm = evaluate_conformal_prediction(\n",
    "            y_test, prediction_sets_test_lgbm, ALPHA, model_name=model_name_lgbm\n",
    "        )\n",
    "    else:\n",
    "         logging.warning(f\"[{model_name_lgbm}] No CP threshold, skipping CP evaluation.\")\n",
    "\n",
    "    eval_duration_lgbm = time.time() - eval_start_time_lgbm\n",
    "    logging.info(f\"--- [{model_name_lgbm}] Evaluation finished in {eval_duration_lgbm:.2f} seconds ---\")\n",
    "\n",
    "    # Store results\n",
    "    all_results[model_name_lgbm] = {\n",
    "        'metrics': metrics_lgbm,\n",
    "        'cp_coverage': cp_coverage_lgbm,\n",
    "        'cp_avg_set_size': cp_avg_set_size_lgbm,\n",
    "        'best_hpo_params': best_params_lgbm, # Original HPO params\n",
    "        # Store actual used estimators if available\n",
    "        'final_n_estimators': final_best_params_lgbm.get('n_estimators', None) if final_best_params_lgbm else None,\n",
    "        'hpo_f1_score': best_score_hpo_lgbm,\n",
    "        'hpo_duration_s': hpo_duration_lgbm,\n",
    "        'q_threshold': q_threshold_lgbm\n",
    "    }\n",
    "else:\n",
    "    logging.warning(f\"[{model_name_lgbm}] Skipping final evaluation.\")\n",
    "\n",
    "logging.info(f\"===== Finished Workflow for {model_name_lgbm} =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_summary = []\n",
    "for model_name, results_data in all_results.items():\n",
    "    summary = {'Model': model_name}\n",
    "    if results_data['metrics']:\n",
    "        summary.update(results_data['metrics'])\n",
    "        # Remove nested confusion matrix dict for simple display\n",
    "        if 'confusion_matrix' in summary:\n",
    "             cm = summary.pop('confusion_matrix')\n",
    "             summary['TN'] = cm['tn']\n",
    "             summary['FP'] = cm['fp']\n",
    "             summary['FN'] = cm['fn']\n",
    "             summary['TP'] = cm['tp']\n",
    "    summary['CP Coverage'] = results_data.get('cp_coverage', None)\n",
    "    summary['CP Avg Set Size'] = results_data.get('cp_avg_set_size', None)\n",
    "    summary['HPO F1'] = results_data.get('hpo_f1_score', None)\n",
    "    summary['HPO Duration (s)'] = results_data.get('hpo_duration_s', None)\n",
    "    summary['Final Estimators'] = results_data.get('final_n_estimators', 'N/A') # For XGB/LGBM\n",
    "    results_summary.append(summary)\n",
    "\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "\n",
    "# Set display options for float formatting\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}' if isinstance(x, float) else x)\n",
    "\n",
    "print(\"\\n===== Performance Metrics Summary =====\")\n",
    "display(results_df)\n",
    "\n",
    "# You might want to save results_df to CSV\n",
    "results_df.to_csv(os.path.join(MODEL_DIR, f\"model_comparison_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"), index=False)\n",
    "logging.info(\"Results summary DataFrame saved.\")\n",
    "\n",
    "# You can also access detailed results for a specific model:\n",
    "# print(\"\\nDetailed SVM Results:\")\n",
    "# print(all_results.get('SVM_Hyperband'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
