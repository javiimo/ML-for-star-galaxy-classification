{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter nbconvert --to script Model.ipynb\n",
    "\n",
    "#TODO: from sklearn.pipeline import Pipeline to make them all together and cleaner. \n",
    "# Include the SVM with the standard scaler in the same pipeline.\n",
    "# Maybe also the CVAP or the MICP?\n",
    "#TODO: feature names\n",
    "#c:\\Users\\javym\\miniconda3\\envs\\Lab\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
    "#  warnings.warn(\n",
    "#c:\\Users\\javym\\miniconda3\\envs\\Lab\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
    "#TODO: Thresholding\n",
    "# CART is outputting allways the 0 class due to the 0.5 threshold, the max prob from cvap in CART is 0.06. \n",
    "#TODO: Saving models and best parameters for each group\n",
    "#TODO: Tabla\n",
    "# Quiero la tabla del latex con los resultados y ejecutar por grupos\n",
    "#TODO: Implement loop for all the groups\n",
    "# Maybe wrapping each model into a function of the dataset??\n",
    "#TODO: Estudiar relevancia de las features para cada modelo y cada grupo.\n",
    "# Shapely is too computationally expensive\n",
    "# Use conformasight\n",
    "#TODO: Check MICP is correctly implemented\n",
    "# It doesn't seem clear whether it is using the true labels in the calibration split for anything\n",
    "# Maybe try different bins (using the predicted label rather than the true? Changes the condition in the probability guarantees)\n",
    "# Maybe implement ICP (non mondrian) option to see the differences\n",
    "# Maybe use better ncm with the interval probs of the Venn Abers prediction. How wide it is and how far from the true label?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any existing log files\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "# Reset logger to avoid any issues with permissions\n",
    "logging.shutdown()\n",
    "# Remove loggers\n",
    "for log_file in glob.glob(\"*.log\"):\n",
    "    os.remove(log_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star-Galaxy Classification using ALHAMBRA Photometry\n",
    "\n",
    "This notebook implements and evaluates several machine learning models for classifying astronomical objects as stars or galaxies based on multi-band photometric data from the ALHAMBRA survey, using labels derived from higher-resolution COSMOS2020 data.\n",
    "\n",
    "**Target Variable:** `acs_mu_class` (from COSMOS2020)\n",
    " - Which is 1 for Galaxy and 2 for Star. We will remap this to 0 (Galaxy, majority class) and 1 (Star, minority class).\n",
    "\n",
    "**Features:** Selected columns from the ALHAMBRA survey data.\n",
    "\n",
    "**Models:**\n",
    "1. Support Vector Machine (SVM)\n",
    "2. Decision Tree (CART)\n",
    "3. Random Forest\n",
    "4. XGBoost\n",
    "5. LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # For confusion matrix heatmap\n",
    "from scipy.stats import randint, uniform, loguniform # Ensure loguniform is imported if used\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "import logging\n",
    "from tqdm import tqdm  \n",
    "#from tqdm.notebook import tqdm # Needs pip install ipywidgets\n",
    "#from tqdm.auto import tqdm\n",
    "import joblib # For saving/loading models efficiently\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, ParameterSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from scipy.stats import loguniform # For hyperparameter distributions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone, BaseEstimator, ClassifierMixin\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support, roc_auc_score,\n",
    "    brier_score_loss, precision_recall_curve, auc, f1_score\n",
    ")   \n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "# Boosting models\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "\n",
    "# Mondrian ICP\n",
    "from crepes import ConformalClassifier\n",
    "\n",
    "# Configure logging\n",
    "logging.shutdown()\n",
    "logging.basicConfig(\n",
    "    filename=f'models_{datetime.now().strftime(\"%d_%H-%M-%S\")}.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True\n",
    ")\n",
    "# Prevent logs from being printed to console\n",
    "logging.getLogger().handlers = [h for h in logging.getLogger().handlers if isinstance(h, logging.FileHandler)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Dataset & Feature Selection\n",
    "\n",
    "**Interesting Feature Combinations for Modeling:**\n",
    " \n",
    " The feature groups are defined as follows:\n",
    " - Group 1: Morphology features and their uncertainties\n",
    " - Group 2: Photometry magnitudes\n",
    " - Group 3: Photometry magnitude and errors\n",
    " - Group 4: Redshift features and their uncertainties\n",
    " - Group 5: Combination of photometry magnitude errors and morphology features (including uncertainties)\n",
    " - Group 6: Combination of photometry magnitude errors, morphology features (including uncertainties), and redshift features (including uncertainties)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the df\n",
    "df = pd.read_csv('data/match_alhambra_cosmos2020_ACS_class_0.8arcsec.csv')\n",
    "logging.info(f\"DataFrame created with shape: {df.shape}\")\n",
    "# Map ACS classification: 1 (Galaxy, Majority) -> 0, 2 (Star, minority) -> 1, 3 (Fake) -> drop\n",
    "logging.info(\"Original class counts:\")\n",
    "logging.info(df['acs_mu_class'].value_counts().to_string())\n",
    "\n",
    "# Drop fake detections (class 3)\n",
    "# Drop fake detections\n",
    "n_fakes = (df['acs_mu_class'] == 3).sum()\n",
    "logging.info(f\"Number of fake detections (class 3): {n_fakes}\")\n",
    "df = df[df['acs_mu_class'] != 3]\n",
    "\n",
    "# Map classifications\n",
    "df['acs_mu_class'] = df['acs_mu_class'].map({1: 0, 2: 1})\n",
    "\n",
    "logging.info(\"After dropping fakes and mapping classes (0: Galaxy, 1: Star):\")\n",
    "logging.info(df['acs_mu_class'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features\n",
    "\n",
    "# --- Define feature categories based on ALHAMBRA data using exact names ---\n",
    "\n",
    "# 1. ALHAMBRA Morphology Features (SExtractor-based)\n",
    "morphology_features = [\n",
    "    'area', 'fwhm', 'stell', 'ell', 'a', 'b', 'theta', 'rk', 'rf'\n",
    "]\n",
    "\n",
    "morphology_err = [\n",
    "    's2n'\n",
    "]\n",
    "\n",
    "morphology_mags_errors = morphology_features + morphology_err\n",
    "\n",
    "# 2. ALHAMBRA Photometry Magnitudes (Optical + NIR + Synthetic)\n",
    "OPTICAL_MAG_COLS = [\n",
    "    'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W',\n",
    "    'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W',\n",
    "    'F799W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W'\n",
    "]\n",
    "photometry_magnitudes = (\n",
    "    OPTICAL_MAG_COLS +\n",
    "    ['J', 'H', 'KS', 'F814W']\n",
    ")\n",
    "\n",
    "# 3. ALHAMBRA Photometry Uncertainties\n",
    "OPTICAL_ERR_COLS = [\n",
    "    'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W',\n",
    "    'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W',\n",
    "    'dF799W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W'\n",
    "]\n",
    "photometry_uncertainties = (\n",
    "    OPTICAL_ERR_COLS +\n",
    "    ['dJ', 'dH', 'dKS', 'dF814W']\n",
    ")\n",
    "\n",
    "photometry_mags_errors = photometry_magnitudes + photometry_uncertainties\n",
    "\n",
    "# 4. ALHAMBRA Photometric Redshift & Derived Features (BPZ-based)\n",
    "redshift_features = [\n",
    "    'zb_1', 'zb_Min_1', 'zb_Max_1', 'Tb_1',\n",
    "    'z_ml', 't_ml',\n",
    "    'Stell_Mass_1', 'M_Abs_1', 'MagPrior'\n",
    "]\n",
    "\n",
    "redshift_uncertainties = [\n",
    "    'Odds_1', 'Chi2'\n",
    "]\n",
    "\n",
    "\n",
    "redshift_mags_errors = redshift_features + redshift_uncertainties\n",
    "\n",
    "# 5. ALHAMBRA Quality/Auxiliary Features (per-band quality etc.)\n",
    "OPTICAL_IRMS_COLS = [\n",
    "    'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W',\n",
    "    'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W',\n",
    "    'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W',\n",
    "    'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W'\n",
    "]\n",
    "quality_aux_features = (\n",
    "    ['nfobs'] +\n",
    "    OPTICAL_IRMS_COLS +\n",
    "    ['irms_J', 'irms_H', 'irms_KS', 'irms_F814W']\n",
    ")\n",
    "\n",
    "# --- Define lists of features NOT used for modeling ---\n",
    "\n",
    "non_modeling_identifiers = ['ID_1', 'id_2'] # ALHAMBRA ID, COSMOS ID\n",
    "\n",
    "non_modeling_astrometry = [\n",
    "    'RA_1', 'Dec_1', 'x', 'y', # ALHAMBRA Astrometry\n",
    "    'ra_2', 'dec_2',          # COSMOS Astrometry\n",
    "    'Separation'              # Matching Quality\n",
    "]\n",
    "\n",
    "non_modeling_flags = [\n",
    "    'photoflag', 'xray', 'PercW', 'Satur_Flag', # ALHAMBRA Object/Photometry Flags\n",
    "    'irms_OPT_Flag', 'irms_NIR_Flag'           # ALHAMBRA Overall Quality Flags\n",
    "]\n",
    "\n",
    "alhambra_prediction = ['Stellar_Flag'] # ALHAMBRA's own classification\n",
    "\n",
    "non_modeling_aperture_mags = [ # Specific aperture mags, usually use total mags\n",
    "    'F814W_3arcs', 'dF814W_3arcs', 'F814W_3arcs_corr'\n",
    "]\n",
    "\n",
    "non_modeling_cosmos_features = [ # Measurements/flags derived from COSMOS data (HST, HSC, VISTA...)\n",
    "    'model_flag',\n",
    "    'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista',\n",
    "    'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid',\n",
    "    'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid',\n",
    "    'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid',\n",
    "    'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid',\n",
    "    'acs_f814w_mag', 'acs_f814w_magerr',\n",
    "    'acs_fwhm_world', 'acs_mu_max',\n",
    "    'solution_model' # This is categorical, but still COSMOS-derived info\n",
    "]\n",
    "\n",
    "target_variable = ['acs_mu_class'] # The COSMOS classification label to predict\n",
    "\n",
    "##########################################################################################\n",
    "#! --- Consolidate into the main dictionary for easy access ---\n",
    "##########################################################################################\n",
    "\n",
    "feature_sets = {\n",
    "        # --- Potential Input Feature Sets ---\n",
    "        'morphology_only': morphology_mags_errors,\n",
    "        'photometry_magnitudes_only': photometry_magnitudes,\n",
    "        'photometry_mags_errors': photometry_mags_errors,\n",
    "        'photometry_plus_morphology': photometry_mags_errors + morphology_mags_errors,\n",
    "        'photometry_no_redshift': photometry_mags_errors + morphology_mags_errors + quality_aux_features,\n",
    "        'redshift_only': redshift_mags_errors,\n",
    "        'full_alhambra_all': (morphology_mags_errors +\n",
    "                            photometry_mags_errors +\n",
    "                            redshift_mags_errors + \n",
    "                            quality_aux_features),\n",
    "\n",
    "        # --- Excluded Feature Sets ---\n",
    "        'non_modeling_identifiers': non_modeling_identifiers,\n",
    "        'non_modeling_astrometry': non_modeling_astrometry,\n",
    "        'non_modeling_flags': non_modeling_flags,\n",
    "        'non_modeling_aperture_mags': non_modeling_aperture_mags,\n",
    "        'non_modeling_cosmos_features': non_modeling_cosmos_features,\n",
    "        'alhambra_prediction': alhambra_prediction,\n",
    "        'target_variable': target_variable\n",
    "    }\n",
    "\n",
    "#! This is excluding the quality aux.\n",
    "# Include target_variable in each group by appending it to the feature list\n",
    "groups = {\n",
    "        'group_1': feature_sets.get('morphology_only', []) + feature_sets.get('target_variable', []),\n",
    "        'group_2': feature_sets.get('photometry_magnitudes_only', []) + feature_sets.get('target_variable', []),\n",
    "        'group_3': feature_sets.get('photometry_mags_errors', []) + feature_sets.get('target_variable', []),\n",
    "        'group_4': feature_sets.get('redshift_only', []) + feature_sets.get('target_variable', []),\n",
    "        'group_5': feature_sets.get('photometry_plus_morphology', []) + feature_sets.get('target_variable', []),\n",
    "        'group_6': (feature_sets.get('photometry_mags_errors', []) +\n",
    "                   feature_sets.get('morphology_only', []) +\n",
    "                   feature_sets.get('redshift_only', []) +\n",
    "                   feature_sets.get('target_variable', [])),\n",
    "        'group_7': feature_sets.get('full_alhambra_all', []) + feature_sets.get('target_variable', [])\n",
    "    }\n",
    "\n",
    "# --- Function to get a specific feature set (Unchanged from before) ---\n",
    "\n",
    "def get_feature_set(df, set_name, groups = groups):\n",
    "    \"\"\"\n",
    "    Selects columns from a DataFrame based on a predefined feature set name,\n",
    "    including six specific groups defined by combinations of morphology,\n",
    "    photometry magnitudes, uncertainties, and redshift features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        set_name (str): The name of the desired feature set group:\n",
    "                        'group_1' to 'group_6' as defined below.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing only the columns\n",
    "                      belonging to the specified feature set group.\n",
    "                      Returns an empty DataFrame if no columns are found.\n",
    "    \"\"\"\n",
    "\n",
    "    if set_name not in groups:\n",
    "        raise ValueError(f\"Feature set group '{set_name}' not defined. \"\n",
    "                         f\"Available groups: {list(groups.keys())}\")\n",
    "\n",
    "    required_cols_in_set = groups[set_name]\n",
    "\n",
    "    # Find which of these columns actually exist in the DataFrame\n",
    "    available_cols = [col for col in required_cols_in_set if col in df.columns]\n",
    "\n",
    "    # Warn if some columns from the set definition are missing\n",
    "    missing_cols = [col for col in required_cols_in_set if col not in available_cols]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: The following columns defined for feature set group '{set_name}'\"\n",
    "              f\" were not found in the DataFrame and will be excluded: {missing_cols}\")\n",
    "\n",
    "    if not available_cols:\n",
    "        print(f\"Warning: No columns for feature set group '{set_name}' found in the DataFrame.\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "\n",
    "    print(f\"Selecting feature set group '{set_name}' with {len(available_cols)} columns.\")\n",
    "    return df[available_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All df columns are included in feature_sets.\n",
      "\n",
      "=== group_1 ===\n",
      "Selecting feature set group 'group_1' with 11 columns.\n",
      "\n",
      "Features present (11 columns):\n",
      "['a', 'acs_mu_class', 'area', 'b', 'ell', 'fwhm', 'rf', 'rk', 's2n', 'stell', 'theta']\n",
      "\n",
      "Features missing (125 columns):\n",
      "['Chi2', 'Dec_1', 'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F814W_3arcs', 'F814W_3arcs_corr', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'ID_1', 'J', 'KS', 'M_Abs_1', 'MagPrior', 'Odds_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stell_Mass_1', 'Stellar_Flag', 'Tb_1', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF814W_3arcs', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'dec_2', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'solution_model', 't_ml', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "=== group_2 ===\n",
      "Selecting feature set group 'group_2' with 25 columns.\n",
      "\n",
      "Features present (25 columns):\n",
      "['F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'acs_mu_class']\n",
      "\n",
      "Features missing (111 columns):\n",
      "['Chi2', 'Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'M_Abs_1', 'MagPrior', 'Odds_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stell_Mass_1', 'Stellar_Flag', 'Tb_1', 'a', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF814W_3arcs', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'dec_2', 'ell', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'fwhm', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'rf', 'rk', 's2n', 'solution_model', 'stell', 't_ml', 'theta', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "=== group_3 ===\n",
      "Selecting feature set group 'group_3' with 49 columns.\n",
      "\n",
      "Features present (49 columns):\n",
      "['F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'acs_mu_class', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS']\n",
      "\n",
      "Features missing (87 columns):\n",
      "['Chi2', 'Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'M_Abs_1', 'MagPrior', 'Odds_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stell_Mass_1', 'Stellar_Flag', 'Tb_1', 'a', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'area', 'b', 'dF814W_3arcs', 'dec_2', 'ell', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'fwhm', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'rf', 'rk', 's2n', 'solution_model', 'stell', 't_ml', 'theta', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "=== group_4 ===\n",
      "Selecting feature set group 'group_4' with 12 columns.\n",
      "\n",
      "Features present (12 columns):\n",
      "['Chi2', 'M_Abs_1', 'MagPrior', 'Odds_1', 'Stell_Mass_1', 'Tb_1', 'acs_mu_class', 't_ml', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "Features missing (124 columns):\n",
      "['Dec_1', 'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F814W_3arcs', 'F814W_3arcs_corr', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'ID_1', 'J', 'KS', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stellar_Flag', 'a', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF814W_3arcs', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'dec_2', 'ell', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'fwhm', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'rf', 'rk', 's2n', 'solution_model', 'stell', 'theta', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y']\n",
      "\n",
      "=== group_5 ===\n",
      "Selecting feature set group 'group_5' with 59 columns.\n",
      "\n",
      "Features present (59 columns):\n",
      "['F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'a', 'acs_mu_class', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'ell', 'fwhm', 'rf', 'rk', 's2n', 'stell', 'theta']\n",
      "\n",
      "Features missing (77 columns):\n",
      "['Chi2', 'Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'M_Abs_1', 'MagPrior', 'Odds_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stell_Mass_1', 'Stellar_Flag', 'Tb_1', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'dF814W_3arcs', 'dec_2', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'solution_model', 't_ml', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "=== group_6 ===\n",
      "Selecting feature set group 'group_6' with 70 columns.\n",
      "\n",
      "Features present (70 columns):\n",
      "['Chi2', 'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'M_Abs_1', 'MagPrior', 'Odds_1', 'Stell_Mass_1', 'Tb_1', 'a', 'acs_mu_class', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'ell', 'fwhm', 'rf', 'rk', 's2n', 'stell', 't_ml', 'theta', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "Features missing (66 columns):\n",
      "['Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stellar_Flag', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'dF814W_3arcs', 'dec_2', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'nfobs', 'photoflag', 'ra_2', 'solution_model', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y']\n",
      "\n",
      "=== group_7 ===\n",
      "Selecting feature set group 'group_7' with 95 columns.\n",
      "\n",
      "Features present (95 columns):\n",
      "['Chi2', 'F365W', 'F396W', 'F427W', 'F458W', 'F489W', 'F520W', 'F551W', 'F582W', 'F613W', 'F644W', 'F675W', 'F706W', 'F737W', 'F768W', 'F799W', 'F814W', 'F830W', 'F861W', 'F892W', 'F923W', 'F954W', 'H', 'J', 'KS', 'M_Abs_1', 'MagPrior', 'Odds_1', 'Stell_Mass_1', 'Tb_1', 'a', 'acs_mu_class', 'area', 'b', 'dF365W', 'dF396W', 'dF427W', 'dF458W', 'dF489W', 'dF520W', 'dF551W', 'dF582W', 'dF613W', 'dF644W', 'dF675W', 'dF706W', 'dF737W', 'dF768W', 'dF799W', 'dF814W', 'dF830W', 'dF861W', 'dF892W', 'dF923W', 'dF954W', 'dH', 'dJ', 'dKS', 'ell', 'fwhm', 'irms_F365W', 'irms_F396W', 'irms_F427W', 'irms_F458W', 'irms_F489W', 'irms_F520W', 'irms_F551W', 'irms_F582W', 'irms_F613W', 'irms_F644W', 'irms_F675W', 'irms_F706W', 'irms_F737W', 'irms_F768W', 'irms_F799W', 'irms_F814W', 'irms_F830W', 'irms_F861W', 'irms_F892W', 'irms_F923W', 'irms_F954W', 'irms_H', 'irms_J', 'irms_KS', 'nfobs', 'rf', 'rk', 's2n', 'stell', 't_ml', 'theta', 'z_ml', 'zb_1', 'zb_Max_1', 'zb_Min_1']\n",
      "\n",
      "Features missing (41 columns):\n",
      "['Dec_1', 'F814W_3arcs', 'F814W_3arcs_corr', 'ID_1', 'PercW', 'RA_1', 'Satur_Flag', 'Separation', 'Stellar_Flag', 'acs_f814w_mag', 'acs_f814w_magerr', 'acs_fwhm_world', 'acs_mu_max', 'dF814W_3arcs', 'dec_2', 'flag_hsc', 'flag_supcam', 'flag_udeep', 'flag_uvista', 'hsc_i_mag', 'hsc_i_magerr', 'hsc_i_valid', 'hsc_r_mag', 'hsc_r_magerr', 'hsc_r_valid', 'id_2', 'irms_NIR_Flag', 'irms_OPT_Flag', 'model_flag', 'photoflag', 'ra_2', 'solution_model', 'uvista_j_mag', 'uvista_j_magerr', 'uvista_j_valid', 'uvista_ks_mag', 'uvista_ks_magerr', 'uvista_ks_valid', 'x', 'xray', 'y']\n"
     ]
    }
   ],
   "source": [
    "# Quality check to see which cols are excluded and contained in each group\n",
    "all_feature_cols = set()\n",
    "for cols in feature_sets.values():\n",
    "    all_feature_cols.update(cols)\n",
    "\n",
    "df_cols_set = set(df.columns)\n",
    "not_in_feature_sets = df_cols_set - all_feature_cols\n",
    "\n",
    "if not_in_feature_sets:\n",
    "    print(f\"Columns in df not included in any feature_sets: {sorted(not_in_feature_sets)}\")\n",
    "else:\n",
    "    print(\"All df columns are included in feature_sets.\")\n",
    "\n",
    "\n",
    "# Check which columns are in each feature group\n",
    "for group_name in ['group_1', 'group_2', 'group_3', 'group_4', 'group_5', 'group_6', 'group_7']:\n",
    "    print(f\"\\n=== {group_name} ===\")\n",
    "    \n",
    "    # Get the feature set definition\n",
    "    feature_set = groups[group_name]\n",
    "    \n",
    "    # Get the actual columns that exist in the data\n",
    "    group_df = get_feature_set(df, group_name)\n",
    "    \n",
    "\n",
    "    available_cols = list(group_df.columns)\n",
    "    \n",
    "    # Find columns that are defined but not in the data\n",
    "    missing_cols = [col for col in list(df.columns) if col not in feature_set]\n",
    "    \n",
    "    print(f\"\\nFeatures present ({len(available_cols)} columns):\")\n",
    "    print(list(sorted(available_cols)))\n",
    "    \n",
    "    print(f\"\\nFeatures missing ({len(missing_cols)} columns):\")\n",
    "    print(list(sorted(missing_cols)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting parameters\n",
    "TEST_SIZE = 0.20 # Test set proportion\n",
    "VAL_SIZE = 0.10 # Validation set proportion\n",
    "CAL_SIZE = 0.10 # Calibration set proportion\n",
    "# Train size will be 1 - (TEST_SIZE + VAL_SIZE + CAL_SIZE)\n",
    "\n",
    "TARGET_COLUMN = feature_sets.get('target_variable', [])[0]\n",
    "RANDOM_SEED = 33 # For reproducibility\n",
    "\n",
    "# Model saving directory\n",
    "MODEL_DIR = \"trained_models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Data splitting strategy ('stratified' or 'random')\n",
    "SPLIT_STRATEGY = 'stratified' # Recommended for imbalanced datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Cleaning ---\n",
    "def clean_data(df, feature_group, target_column, logger=logging):\n",
    "    \"\"\"\n",
    "    Cleans the input DataFrame by selecting features for the given group,\n",
    "    dropping NaNs, and separating features and target.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        feature_group (str): The feature group to use (e.g., 'group_1', 'group_2', etc.).\n",
    "        target_column (str): The name of the target column.\n",
    "        logger (logging.Logger): Logger for info and error messages.\n",
    "\n",
    "    Returns:\n",
    "        X (pd.DataFrame): Cleaned feature DataFrame.\n",
    "        y (pd.Series): Target variable.\n",
    "        df_clean (pd.DataFrame): The cleaned DataFrame (features + target).\n",
    "    \"\"\"\n",
    "    logger.info(f\"Original dataset size: {df.shape}\")\n",
    "\n",
    "    # Get the feature columns for the selected group using get_feature_set\n",
    "    df_clean = get_feature_set(df, feature_group).dropna().copy()\n",
    "    logger.info(f\"Dataset size after dropping NaNs: {df_clean.shape}\")\n",
    "\n",
    "    # Ensure target_column is defined correctly\n",
    "    if target_column not in df_clean.columns:\n",
    "        raise KeyError(f\"Target column '{target_column}' not found in the cleaned DataFrame columns: {df_clean.columns.tolist()}\")\n",
    "\n",
    "    # Log value counts for target\n",
    "    logger.info(f\"Value counts for target:\\n1 (Star): {(df_clean[target_column] == 1).sum()}\\n0 (Galaxy): {(df_clean[target_column] == 0).sum()}\")\n",
    "\n",
    "    # Separate features (X) and target (y) for the cleaned DataFrame\n",
    "    X = df_clean.drop(columns=[target_column])\n",
    "    y = df_clean[target_column]\n",
    "    return X, y, df_clean\n",
    "\n",
    "# Example usage:\n",
    "# X, y, df_clean = clean_data(df, feature_group='group_7', target_column=TARGET_COLUMN, logger=logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Splitting ---\n",
    "def split_data(X, y):\n",
    "    \"\"\"\n",
    "    Splits the data into train, validation, test, and calibration sets according to the global\n",
    "    split proportions and strategy. Uses global variables:\n",
    "        - TEST_SIZE, VAL_SIZE, CAL_SIZE, SPLIT_STRATEGY, RANDOM_SEED\n",
    "\n",
    "    The logic and split order is identical to the original code.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature DataFrame.\n",
    "        y (pd.Series): Target variable.\n",
    "\n",
    "    Returns:\n",
    "        (X_train, y_train, X_val, y_val, X_test, y_test, X_cal, y_cal): tuple of splits.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Splitting data using '{SPLIT_STRATEGY}' strategy...\")\n",
    "\n",
    "    # --- Validate Proportions ---\n",
    "    if not (0 <= TEST_SIZE <= 1 and 0 <= VAL_SIZE <= 1 and 0 <= CAL_SIZE <= 1):\n",
    "        raise ValueError(\"Split proportions (TEST_SIZE, VAL_SIZE, CAL_SIZE) must be between 0 and 1.\")\n",
    "\n",
    "    TRAIN_SIZE = 1.0 - TEST_SIZE - VAL_SIZE - CAL_SIZE\n",
    "    if not (0 <= TRAIN_SIZE <= 1):\n",
    "        raise ValueError(f\"Calculated TRAIN_SIZE ({TRAIN_SIZE:.3f}) is invalid. Sum of TEST_SIZE, VAL_SIZE, and CAL_SIZE must be between 0 and 1.\")\n",
    "\n",
    "    if not np.isclose(TRAIN_SIZE + TEST_SIZE + VAL_SIZE + CAL_SIZE, 1.0):\n",
    "        # This check might be redundant given the calculation of TRAIN_SIZE, but good for safety.\n",
    "        raise ValueError(\"Sum of split proportions must be equal to 1.\")\n",
    "\n",
    "    if np.isclose(TRAIN_SIZE, 0) and (np.isclose(VAL_SIZE, 0) or np.isclose(TEST_SIZE, 0) or np.isclose(CAL_SIZE, 0)):\n",
    "        # Avoid scenarios where train is 0 but other splits are also 0, leading to ambiguity.\n",
    "        # If only train is 0, it might be valid in some rare cases, but usually requires at least one other non-zero split.\n",
    "        # Let's enforce Train > 0 for typical ML workflows.\n",
    "        # If you need zero training data, adjust this check.\n",
    "        logging.warning(\"TRAIN_SIZE is zero or near zero. Ensure this is intended.\")\n",
    "        if TRAIN_SIZE < 0: # Definitely an error\n",
    "            raise ValueError(\"TRAIN_SIZE cannot be negative.\")\n",
    "        # Allow TRAIN_SIZE = 0 only if explicitly handled later, otherwise raise error?\n",
    "        # For now, let's proceed but log a warning. If TRAIN_SIZE must be > 0, uncomment the raise below.\n",
    "        # raise ValueError(\"TRAIN_SIZE must be greater than 0 for typical model training.\")\n",
    "\n",
    "    logging.info(f\"Target split ratios: Train={TRAIN_SIZE:.2f}, Val={VAL_SIZE:.2f}, Test={TEST_SIZE:.2f}, Cal={CAL_SIZE:.2f}\")\n",
    "\n",
    "    # --- Initialize Splits ---\n",
    "    # Use iloc[0:0] to create empty DataFrames/Series with the same columns/dtype\n",
    "    empty_X = X.iloc[0:0]\n",
    "    empty_y = y.iloc[0:0]\n",
    "    X_train, y_train = empty_X.copy(), empty_y.copy()\n",
    "    X_val, y_val = empty_X.copy(), empty_y.copy()\n",
    "    X_test, y_test = empty_X.copy(), empty_y.copy()\n",
    "    X_cal, y_cal = empty_X.copy(), empty_y.copy()\n",
    "\n",
    "    # Temporary variables for sequential splitting\n",
    "    X_remaining, y_remaining = X.copy(), y.copy() # Use copies to avoid modifying original X, y\n",
    "\n",
    "    # --- Stratification Option ---\n",
    "    # Define stratify_func only once\n",
    "    def get_stratify_array(y_arr):\n",
    "        return y_arr if SPLIT_STRATEGY == 'stratified' and not y_arr.empty else None\n",
    "\n",
    "    # --- First Split: Train vs. Remainder (Val + Test + Cal) ---\n",
    "    val_test_cal_size = VAL_SIZE + TEST_SIZE + CAL_SIZE\n",
    "\n",
    "    if np.isclose(val_test_cal_size, 0): # Only Train set needed\n",
    "        X_train, y_train = X_remaining, y_remaining\n",
    "        logging.info(\"All data assigned to Train set (Val, Test, Cal sizes are 0).\")\n",
    "        X_remaining, y_remaining = empty_X.copy(), empty_y.copy() # No remainder\n",
    "    elif np.isclose(TRAIN_SIZE, 0): # No Train set needed\n",
    "        logging.info(\"Train set is empty (TRAIN_SIZE=0). Remainder passed to next splits.\")\n",
    "        # X_remaining, y_remaining already hold all data\n",
    "    else: # Split Train vs Remainder\n",
    "        split_test_size = val_test_cal_size # Proportion of remainder relative to total (1.0)\n",
    "        X_train, X_remaining, y_train, y_remaining = train_test_split(\n",
    "            X_remaining, y_remaining,\n",
    "            test_size=split_test_size,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=get_stratify_array(y_remaining)\n",
    "        )\n",
    "    logging.info(f\"Train set shape: {X_train.shape}\")\n",
    "\n",
    "    # --- Second Split: Val vs. Remainder (Test + Cal) ---\n",
    "    if not X_remaining.empty:\n",
    "        test_cal_size = TEST_SIZE + CAL_SIZE\n",
    "        # Denominator for relative size calculation: size of the current remaining pool\n",
    "        current_remaining_size_frac = VAL_SIZE + test_cal_size # = val_test_cal_size\n",
    "\n",
    "        if np.isclose(VAL_SIZE, 0): # No Val set, pass remainder to next stage\n",
    "            X_temp2, y_temp2 = X_remaining, y_remaining # Remainder is Test + Cal\n",
    "            logging.info(\"Validation set is empty (VAL_SIZE=0).\")\n",
    "        elif np.isclose(test_cal_size, 0): # Only Val set left in remainder\n",
    "            X_val, y_val = X_remaining, y_remaining\n",
    "            X_temp2, y_temp2 = empty_X.copy(), empty_y.copy() # No data left for Test/Cal\n",
    "            logging.info(f\"Validation set shape: {X_val.shape}\")\n",
    "        else: # Split Val vs (Test + Cal)\n",
    "            # Proportion of (Test + Cal) relative to (Val + Test + Cal)\n",
    "            split_test_size = test_cal_size / current_remaining_size_frac\n",
    "            X_val, X_temp2, y_val, y_temp2 = train_test_split(\n",
    "                X_remaining, y_remaining,\n",
    "                test_size=split_test_size,\n",
    "                random_state=RANDOM_SEED,\n",
    "                stratify=get_stratify_array(y_remaining)\n",
    "            )\n",
    "            logging.info(f\"Validation set shape: {X_val.shape}\")\n",
    "    else: # No data remaining after train split\n",
    "        X_temp2, y_temp2 = empty_X.copy(), empty_y.copy()\n",
    "        if not np.isclose(VAL_SIZE, 0): # Log only if Val set was expected\n",
    "            logging.info(\"Validation set is empty (no data remaining after train split).\")\n",
    "\n",
    "    # --- Third Split: Test vs. Cal ---\n",
    "    if not X_temp2.empty:\n",
    "        # Denominator for relative size calculation: size of the current remaining pool\n",
    "        current_remaining_size_frac = TEST_SIZE + CAL_SIZE # = test_cal_size\n",
    "\n",
    "        if np.isclose(CAL_SIZE, 0): # No Cal set, remainder is Test\n",
    "            X_test, y_test = X_temp2, y_temp2\n",
    "            logging.info(\"Calibration set is empty (CAL_SIZE=0).\")\n",
    "        elif np.isclose(TEST_SIZE, 0): # Only Cal set left in remainder\n",
    "            X_cal, y_cal = X_temp2, y_temp2\n",
    "            logging.info(\"Test set is empty (TEST_SIZE=0).\")\n",
    "        else: # Split Test vs Cal\n",
    "            # Proportion of Cal relative to (Test + Cal)\n",
    "            split_test_size = CAL_SIZE / current_remaining_size_frac\n",
    "            X_test, X_cal, y_test, y_cal = train_test_split(\n",
    "                X_temp2, y_temp2,\n",
    "                test_size=split_test_size,\n",
    "                random_state=RANDOM_SEED,\n",
    "                stratify=get_stratify_array(y_temp2)\n",
    "            )\n",
    "            # Logging shapes done after the if/else block\n",
    "    else: # No data remaining for Test/Cal split\n",
    "        if not (np.isclose(TEST_SIZE, 0) and np.isclose(CAL_SIZE, 0)): # Log only if Test or Cal were expected\n",
    "            logging.info(\"Test and Calibration sets are empty (no data remaining for final split).\")\n",
    "\n",
    "    # Log final shapes for Test and Cal\n",
    "    logging.info(f\"Test set shape: {X_test.shape}\")\n",
    "    logging.info(f\"Calibration set shape: {X_cal.shape}\")\n",
    "\n",
    "    # --- Verification and Final Logging ---\n",
    "    total_len = len(X_train) + len(X_val) + len(X_test) + len(X_cal)\n",
    "    original_len = len(X)\n",
    "\n",
    "    if total_len != original_len:\n",
    "        # Calculate actual proportions based on lengths\n",
    "        actual_train = len(X_train) / original_len if original_len > 0 else 0\n",
    "        actual_val = len(X_val) / original_len if original_len > 0 else 0\n",
    "        actual_test = len(X_test) / original_len if original_len > 0 else 0\n",
    "        actual_cal = len(X_cal) / original_len if original_len > 0 else 0\n",
    "        logging.warning(f\"Total split length ({total_len}) does not exactly match original length ({original_len}). \"\n",
    "                        f\"This can happen with stratification or rounding. \"\n",
    "                        f\"Target proportions: Train={TRAIN_SIZE:.3f}, Val={VAL_SIZE:.3f}, Test={TEST_SIZE:.3f}, Cal={CAL_SIZE:.3f}. \"\n",
    "                        f\"Actual proportions: Train={actual_train:.3f}, Val={actual_val:.3f}, Test={actual_test:.3f}, Cal={actual_cal:.3f}\")\n",
    "    else:\n",
    "        logging.info(\"Split lengths verification successful.\")\n",
    "\n",
    "    logging.info(\"Data splitting complete.\")\n",
    "\n",
    "    # Log distributions, handling empty sets\n",
    "    def log_distribution(name, y_set):\n",
    "        if y_set.empty:\n",
    "            logging.info(f\"{name} target distribution: Set is empty.\")\n",
    "        else:\n",
    "            try:\n",
    "                # Use normalize=True, handle potential division by zero if counts are zero (though unlikely if not empty)\n",
    "                counts = y_set.value_counts()\n",
    "                dist = counts / counts.sum() if counts.sum() > 0 else counts\n",
    "                logging.info(f\"{name} target distribution:\\n{dist}\")\n",
    "                # Log absolute counts as well for clarity\n",
    "                logging.info(f\"{name} target counts:\\n{counts}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Could not calculate distribution for {name}: {e}\")\n",
    "                # Attempt to log raw value counts even if normalization fails\n",
    "                try:\n",
    "                    logging.info(f\"{name} raw value counts:\\n{y_set.value_counts()}\")\n",
    "                except Exception as e_raw:\n",
    "                    logging.error(f\"Could not get raw value counts for {name}: {e_raw}\")\n",
    "\n",
    "    log_distribution(\"Train\", y_train)\n",
    "    log_distribution(\"Validation\", y_val)\n",
    "    log_distribution(\"Test\", y_test)\n",
    "    log_distribution(\"Calibration\", y_cal)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_cal, y_cal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization via Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Internal Helper ---\n",
    "def _train_and_eval(model_class, params,\n",
    "                    X_train, y_train, X_val, y_val,\n",
    "                    resource, resource_type,\n",
    "                    scoring_func, random_state):\n",
    "    \"\"\"Internal helper function to train and evaluate a single configuration.\"\"\"\n",
    "    try:\n",
    "        # Instantiate the base model without iteration-specific params first\n",
    "        # Iteration param (e.g., n_estimators) will be handled later if needed\n",
    "        model = model_class(**params)\n",
    "\n",
    "        fit_duration = 0.0\n",
    "        eval_duration = 0.0\n",
    "        start_fit = time.time() # Start timing fit process\n",
    "\n",
    "        if resource_type == 'data_fraction':\n",
    "            # --- FIX 1: Implement data subsetting ---\n",
    "            if resource < 1.0:\n",
    "                # Use train_test_split to get a stratified fraction\n",
    "                # We only need the 'train' part of this split for the subset\n",
    "                try:\n",
    "                    X_subset, _, y_subset, _ = train_test_split(\n",
    "                        X_train, y_train,\n",
    "                        train_size=resource,\n",
    "                        random_state=random_state, # Use provided random state\n",
    "                        stratify=y_train # Stratify based on original train labels\n",
    "                    )\n",
    "                except ValueError as e:\n",
    "                    # Handle cases where stratification is not possible (e.g., too few samples)\n",
    "                    logging.warning(f\"Stratification failed for resource {resource:.2f}: {e}. Falling back to non-stratified split.\")\n",
    "                    X_subset, _, y_subset, _ = train_test_split(\n",
    "                        X_train, y_train,\n",
    "                        train_size=resource,\n",
    "                        random_state=random_state\n",
    "                    )\n",
    "            else:\n",
    "                # Use the full training data if resource is 1.0\n",
    "                X_subset, y_subset = X_train, y_train\n",
    "\n",
    "            # Ensure y_subset is numpy for fitting if needed by model\n",
    "            y_subset_np = y_subset.values if isinstance(y_subset, pd.Series) else y_subset\n",
    "\n",
    "            # Fit the model \n",
    "            model.fit(X_subset, y_subset_np)\n",
    "            fit_duration = time.time() - start_fit\n",
    "            # -----------------------------------------\n",
    "\n",
    "        elif resource_type == 'iterations':\n",
    "            # Resource represents n_estimators or similar iteration parameter\n",
    "            params_iter = params.copy() # Avoid modifying original params dict\n",
    "            iter_param_name = 'n_estimators' # Common case for RF, XGB, LGBM\n",
    "\n",
    "            # Ensure resource is an integer for iterations\n",
    "            params_iter[iter_param_name] = int(max(1, resource)) # Ensure at least 1 iteration\n",
    "            model = model_class(**params_iter) # Re-instantiate with correct n_estimators\n",
    "\n",
    "            # --- FIX 2 & 3: Conditional Fit Parameters ---\n",
    "            current_fit_args = {} # Dictionary for specific fit arguments\n",
    "            eval_set_for_fit = [(X_val, y_val)] # Common eval set\n",
    "\n",
    "            if model_class is xgb.XGBClassifier:\n",
    "                current_fit_args['eval_set'] = eval_set_for_fit\n",
    "                current_fit_args['verbose'] = False\n",
    "\n",
    "            elif model_class is lgb.LGBMClassifier:\n",
    "                current_fit_args['eval_set'] = eval_set_for_fit\n",
    "                if 'metric' in params_iter: # Get metric from HPO params\n",
    "                     current_fit_args['eval_metric'] = params_iter['metric']\n",
    "                elif isinstance(model.metric, str): # Get metric from model instance if set\n",
    "                     current_fit_args['eval_metric'] = model.metric\n",
    "                else: # Default if not found (might cause issues if early stopping expects it)\n",
    "                     logging.warning(f\"LGBM eval_metric not found in HPO params or model instance for config {params_iter}. Early stopping might fail.\")\n",
    "                     # You might need to add a default like 'logloss' or raise an error\n",
    "                     # current_fit_args['eval_metric'] = 'logloss' # Example default\n",
    "\n",
    "            # For models like RandomForest or DecisionTree, current_fit_args remains empty {}\n",
    "            # as they don't use eval_set or callbacks in their standard fit method\n",
    "\n",
    "            # Fit the model with appropriate arguments\n",
    "            # Ensure y_train is numpy if needed\n",
    "            y_train_np = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "            try:\n",
    "                model.fit(X_train, y_train_np, **current_fit_args)\n",
    "            except Exception as fit_error:\n",
    "                 logging.error(f\"Fit failed for config {params_iter} with resource {resource}: {fit_error}\")\n",
    "                 # logging.exception(\"Fit Traceback:\") # Uncomment for full traceback\n",
    "                 return -1.0 # Indicate failure\n",
    "            fit_duration = time.time() - start_fit\n",
    "            # -----------------------------------------\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid resource_type. Choose 'data_fraction' or 'iterations'.\")\n",
    "\n",
    "        # Evaluate on the full validation set (common part)\n",
    "        start_eval = time.time()\n",
    "        try:\n",
    "             y_pred_val = model.predict(X_val)\n",
    "             # Ensure y_val is numpy if needed by scoring_func\n",
    "             y_val_np = y_val.values if isinstance(y_val, pd.Series) else y_val\n",
    "             score = scoring_func(y_val_np, y_pred_val)\n",
    "        except Exception as eval_error:\n",
    "             logging.error(f\"Predict/Score failed for config {params} with resource {resource}: {eval_error}\")\n",
    "             score = -1.0 # Indicate failure\n",
    "        eval_duration = time.time() - start_eval\n",
    "\n",
    "        logging.debug(f\"Evaluated config: {params} | Resource: {resource:.2f} | Score: {score:.4f} | Fit: {fit_duration:.2f}s | Eval: {eval_duration:.2f}s\")\n",
    "        return score\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the configuration that caused the error\n",
    "        logging.error(f\"Error training/evaluating config {params} with resource {resource}: {e}\", exc_info=False) # Set exc_info=True for traceback if needed\n",
    "        return -1.0 # Return a clearly bad score\n",
    "\n",
    "\n",
    "def hyperband_hpo(model_class, param_space,\n",
    "                  X_train, y_train, X_val, y_val,\n",
    "                  max_resource, eta=3, resource_type='iterations',\n",
    "                  min_resource=1, # Min iterations or min data fraction\n",
    "                  scoring_func=f1_score, # Function accepting (y_true, y_pred)\n",
    "                  random_state=None): # For early stopping etc. passed to .fit()\n",
    "    \"\"\"\n",
    "    Performs Hyperband Hyperparameter Optimization.\n",
    "\n",
    "    Args:\n",
    "        model_class: The model class (e.g., SVC, RandomForestClassifier).\n",
    "        param_space (dict): Dictionary defining the hyperparameter search space\n",
    "                           compatible with ParameterSampler.\n",
    "        X_train, y_train: Training data and labels.\n",
    "        X_val, y_val: Validation data and labels for evaluation.\n",
    "        max_resource (float/int): Maximum resource allocation\n",
    "                                 (e.g., max n_estimators or 1.0 for data fraction).\n",
    "        eta (int): Reduction factor for successive halving (>= 2).\n",
    "        resource_type (str): How resource is allocated:\n",
    "                             'iterations' -> resource sets n_estimators (or similar).\n",
    "                             'data_fraction' -> resource is fraction of training data used (stratified).\n",
    "        min_resource (float/int): Minimum resource for the first iteration.\n",
    "                                 Must be >= 1 for 'iterations', > 0 for 'data_fraction'.\n",
    "        scoring_func (callable): Function to evaluate performance (e.g., f1_score).\n",
    "                                Higher score is assumed better.\n",
    "        random_state (int): Seed for reproducibility of parameter sampling and data subsetting.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_params, best_score)\n",
    "               best_params (dict): The hyperparameters of the best performing configuration.\n",
    "               best_score (float): The score achieved by the best configuration on the validation set\n",
    "                                  using the maximum resource.\n",
    "    \"\"\"\n",
    "\n",
    "    log_max_r = math.log(max_resource / min_resource, eta) if max_resource > min_resource and min_resource > 0 else 0\n",
    "    s_max = int(log_max_r)\n",
    "    B = (s_max + 1) * max_resource # Approximate total resource budget\n",
    "\n",
    "    logging.info(f\"--- Starting Hyperband HPO ---\")\n",
    "    logging.info(f\"Model: {model_class.__name__}\")\n",
    "    logging.info(f\"Resource Type: {resource_type}\")\n",
    "    logging.info(f\"Resource Range: [{min_resource}, {max_resource}]\")\n",
    "    logging.info(f\"Eta: {eta}\")\n",
    "    logging.info(f\"Max Brackets (s_max): {s_max}\")\n",
    "    logging.info(f\"Approx. Budget (B): {B:.2f}\")\n",
    "    logging.info(f\"Scoring: {scoring_func.__name__}\")\n",
    "\n",
    "    best_params = None\n",
    "    best_score = -1.0\n",
    "    total_configs_evaluated = 0\n",
    "    outer_tqdm = tqdm(range(s_max, -1, -1), desc=\"Hyperband Brackets (s)\")\n",
    "\n",
    "    # Outer loop: Iterate through brackets (s values)\n",
    "    for s in outer_tqdm:\n",
    "        n_configs = int(math.ceil(int(B / max_resource / (s + 1)) * eta**s)) # Number of configs in this bracket\n",
    "        r_initial = max_resource * eta**(-s) # Initial resource for this bracket\n",
    "        # Ensure initial resource is not less than min_resource\n",
    "        r_initial = max(r_initial, min_resource)\n",
    "\n",
    "        outer_tqdm.set_description(f\"Bracket s={s} (n={n_configs}, r0={r_initial:.2f})\")\n",
    "        logging.info(f\"\\n>> Bracket s={s}: n_configs={n_configs}, r_initial={r_initial:.2f}\")\n",
    "\n",
    "        # Sample configurations for this bracket\n",
    "        param_list = list(ParameterSampler(param_space, n_iter=n_configs, random_state=random_state + s if random_state is not None else None))\n",
    "        \n",
    "        # --- Add common fixed parameters ---\n",
    "        # Calculate scale_pos_weight once if needed\n",
    "        scale_pos_weight_val = None\n",
    "        if model_class in [xgb.XGBClassifier, lgb.LGBMClassifier]:\n",
    "             neg_count = (y_train == 0).sum()\n",
    "             pos_count = (y_train == 1).sum()\n",
    "             if pos_count > 0:\n",
    "                 scale_pos_weight_val = neg_count / pos_count\n",
    "\n",
    "        for p in param_list:\n",
    "             # Add random_state if model supports it and it's not sampled\n",
    "             if 'random_state' not in p and hasattr(model_class(random_state=1), 'random_state'): # Check if attr exists\n",
    "                 p['random_state'] = random_state\n",
    "             # Add class_weight='balanced' for relevant sklearn models if not sampled\n",
    "             if model_class in [SVC, RandomForestClassifier, DecisionTreeClassifier] and 'class_weight' not in p:\n",
    "                 p['class_weight'] = 'balanced'\n",
    "             # Add scale_pos_weight for boosting models if not sampled and calculated\n",
    "             if model_class in [xgb.XGBClassifier, lgb.LGBMClassifier] and 'scale_pos_weight' not in p and scale_pos_weight_val is not None:\n",
    "                  p['scale_pos_weight'] = scale_pos_weight_val\n",
    "             # For LightGBM, also consider adding 'objective': 'binary' if not sampled\n",
    "             if model_class is lgb.LGBMClassifier and 'objective' not in p:\n",
    "                  p['objective'] = 'binary'\n",
    "        # -----------------------------------\n",
    "\n",
    "        # Inner loop: Successive halving rounds\n",
    "        inner_tqdm = tqdm(range(s + 1), desc=f\"SH Round (s={s})\", leave=False)\n",
    "        for i in inner_tqdm:\n",
    "            current_resource = r_initial * eta**i\n",
    "            # Ensure resource doesn't exceed max_resource due to floating point/rounding\n",
    "            current_resource = min(current_resource, max_resource)\n",
    "\n",
    "            n_configs_in_round = len(param_list)\n",
    "            inner_tqdm.set_description(f\"SH Round i={i} (n={n_configs_in_round}, r={current_resource:.2f})\")\n",
    "            logging.info(f\"  -- Round i={i}: Evaluating {n_configs_in_round} configs with resource={current_resource:.2f} --\")\n",
    "\n",
    "            round_scores = []\n",
    "            # Use tqdm for the configurations within the round\n",
    "            eval_tqdm = tqdm(param_list, desc=f\"Evaluating Configs (i={i})\", leave=False)\n",
    "            for params in eval_tqdm:\n",
    "                score = _train_and_eval(model_class, params, X_train, y_train, X_val, y_val,\n",
    "                                        current_resource, resource_type, scoring_func,\n",
    "                                        random_state)\n",
    "                round_scores.append((score, params))\n",
    "                total_configs_evaluated += 1 # Count unique evaluations\n",
    "\n",
    "            # Sort by score (descending, higher is better)\n",
    "            round_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            # Track the best overall score and params seen so far *at max resource*\n",
    "            # Only update if we are actually at max resource in this round\n",
    "            if abs(current_resource - max_resource) < 1e-6: # Check if we are at max resource\n",
    "                 if round_scores and round_scores[0][0] > best_score:\n",
    "                      best_score = round_scores[0][0]\n",
    "                      best_params = round_scores[0][1]\n",
    "                      logging.info(f\"  ** New Best Found (Score: {best_score:.4f}) at max resource ** Params: {best_params}\")\n",
    "                      # Update outer tqdm description with best score found so far\n",
    "                      outer_tqdm.set_postfix_str(f\"Best F1: {best_score:.4f}\", refresh=True)\n",
    "\n",
    "\n",
    "            # --- Halving Step ---\n",
    "            n_keep = int(n_configs_in_round / eta)\n",
    "            logging.info(f\"  -- Round i={i}: Completed {len(round_scores)} evaluations. Keeping top {n_keep} configs. --\")\n",
    "\n",
    "            if n_keep < 1 or i == s: # Keep at least one, or if it's the last round\n",
    "                # If it's the last round, ensure the best score from *this bracket* at *max resource* is considered\n",
    "                if abs(current_resource - max_resource) < 1e-6 and round_scores:\n",
    "                     bracket_best_score = round_scores[0][0]\n",
    "                     bracket_best_params = round_scores[0][1]\n",
    "                     logging.info(f\"  Bracket s={s} final best score: {bracket_best_score:.4f}\")\n",
    "                     # No need to update global best here, already done above\n",
    "                break # Exit inner loop\n",
    "\n",
    "            # Prepare parameter list for the next round\n",
    "            param_list = [params for score, params in round_scores[:n_keep]]\n",
    "            if not param_list: # Safety break if list becomes empty unexpectedly\n",
    "                 logging.warning(f\"  Param list empty after halving round i={i}. Stopping bracket.\")\n",
    "                 break\n",
    "\n",
    "    logging.info(f\"\\n--- Hyperband HPO Finished ---\")\n",
    "    logging.info(f\"Total configurations evaluated (approx): {total_configs_evaluated}\") # Might overcount if errors happened\n",
    "    if best_params:\n",
    "        logging.info(f\"Best Overall Score ({scoring_func.__name__}): {best_score:.4f}\")\n",
    "        logging.info(f\"Best Params: {best_params}\")\n",
    "    else:\n",
    "        logging.warning(\"No best parameters found. Check logs for errors or increase resources/configs.\")\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Calibration (Platt Scaling/Isotonic Regression/Cross Venn Abers Predictors)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding CVAP Calibration: Raw Scores vs. Probabilities & Implementation Approaches\n",
    "\n",
    "Cross Venn-Abers Prediction (CVAP) is a method for producing well-calibrated probability predictions from machine learning models by applying the Inductive Venn-Abers Predictor (IVAP) logic within a cross-validation framework. A key implementation detail arises depending on whether the underlying base model produces probabilities \\[0, 1] or **raw scores** (e.g., decision function values, log-odds, margins). The reference code is the [venn-abers python library](https://github.com/ip200/venn-abers/blob/main/src/venn_abers.py), in particular its source file `venn_abers.py`.\n",
    "\n",
    "**CVAP Core Idea:**\n",
    "\n",
    "1.  Split the training data into *K* folds.\n",
    "2.  For each fold *k*:\n",
    "    *   Train a base model on the *K-1* other folds.\n",
    "    *   Use this model to get **scores** $S_{cal}^k$ for the held-out fold *k*, creating the Out-Of-Fold (OOF) calibration set $(S_{cal}^k, y_{cal}^k)$.\n",
    "3.  Train a final base model on *all* training data.\n",
    "4.  For a new test instance $x_{test}$:\n",
    "    *   Get its score $s_{test}$ using the final base model.\n",
    "    *   For *each* fold *k*, calculate the IVAP lower/upper probability bounds $p_0^k(s_{test})$ and $p_1^k(s_{test})$ using the fixed calibration set $(S_{cal}^k, y_{cal}^k)$ and the test score $s_{test}$.\n",
    "5.  Aggregate the $K$ pairs of $(p_0^k, p_1^k)$ across all folds to produce the final CVAP probability prediction for $x_{test}$.\n",
    "\n",
    "The crucial part is **Step 4**. The method used to calculate $p_0^k$ and $p_1^k$ depends on how the IVAP step is implemented, especially concerning the nature of the scores.\n",
    "\n",
    "##### Approach 1: Efficient Pre-Calculation / Lookup (Standard IVAP Algorithm)\n",
    "\n",
    "This is the computationally efficient algorithm presented in Vovk et al. (2015, Algorithms 1-6) for implementing the IVAP calibration step. ([Large-scale probabilistic predictors...](https://proceedings.neurips.cc/paper_files/paper/2015/file/a9a1d5317a33ae8cef33961c34144f84-Paper.pdf), Proposition 2).\n",
    "\n",
    "**Algorithm Explanation:**\n",
    "\n",
    "1.  **Calibration Pre-computation (per fold k):** Using *only* the OOF calibration set $(S_{cal}^k, y_{cal}^k)$:\n",
    "    *   Take the calibration scores $S_{cal}^k$. **Crucially, these only need to be totally ordered real numbers; they are *not* required by the theory to be probabilities in \\[0, 1].**\n",
    "    *   Sort the unique calibration scores to get $C^k = \\{c_1^k, ..., c_m^k\\}$.\n",
    "    *   Compute the cumulative sum diagram (CSD) and use PAVA (or equivalent GCM/LCM construction, see Algorithms 1-4) to derive two structures (`p0_structure^k`, `p1_structure^k`). These structures implicitly store the upper ($p_0$) and lower ($p_1$) probability bounds corresponding to the intervals defined by $C^k$.\n",
    "2.  **Prediction (per fold k):** For *all* test scores $S_{test}$:\n",
    "    *   For each $s_{test}$ (which is on the same raw score scale as $S_{cal}^k$):\n",
    "        *   Find its position relative to the sorted unique *raw* scores $C^k$ using binary search (Algorithm 6).\n",
    "        *   Use this position to **lookup** the appropriate probability values from the pre-calculated `p0_structure^k` and `p1_structure^k`.\n",
    "\n",
    "**Why Practical Implementations Often Assume Probabilities (The Source of Confusion):**\n",
    "\n",
    "While the *algorithm* itself works perfectly with raw scores, popular *implementations* (like the cited `venn_abers.py`) often introduce constraints for convenience or specific use cases:\n",
    "*   They might expect input arrays (`p_cal`, `p_test`) to have two columns (probabilities for class 0 and 1).\n",
    "*   They might explicitly select the column corresponding to the positive class probability (e.g., `[:, 1]`).\n",
    "*   They might include checks or internal logic assuming values are within \\[0, 1].\n",
    "\n",
    "Feeding raw scores (e.g., `[-10, 0, 50]`) directly into **such specific library functions** causes errors or warnings (like `Input probabilities p_test are outside [0, 1]`). This is an **implementation artifact**, not a flaw in the theoretical Approach 1 algorithm itself. The algorithm remains mathematically sound for raw scores.\n",
    "\n",
    "*   *(Note: One could adapt the library to handle 1D raw score arrays directly, bypassing the probability-specific checks and indexing, to leverage this efficient O(log k) lookup.)*\n",
    "\n",
    "##### Approach 2: Direct IVAP Definition via Re-fitting Isotonic Regression\n",
    "\n",
    "This approach bypasses the efficient lookup algorithm and instead directly implements the *fundamental definition* of IVAP for each test point, correctly handling raw scores using standard isotonic regression tools.\n",
    "\n",
    "**Algorithm Explanation:**\n",
    "\n",
    "This happens *inside* the loop over the *K* folds of CVAP. For fold *k* with OOF data $(S_{cal}^k, y_{cal}^k)$ and raw test scores $S_{test}$:\n",
    "\n",
    "1.  **Iterate through each test score $s_{test, j}$ in $S_{test}$:**\n",
    "    *   **Calculate $p_0^k(s_{test, j})$:** Augment the *raw score* calibration data with $(s_{test, j}, 0)$, fit Isotonic Regression $IR_0$ to this augmented set, and get $p_0^k(s_{test, j}) = IR_0(s_{test, j})$.\n",
    "    *   **Calculate $p_1^k(s_{test, j})$:** Augment with $(s_{test, j}, 1)$, fit $IR_1$, and get $p_1^k(s_{test, j}) = IR_1(s_{test, j})$.\n",
    "2.  **Aggregate:** Collect all $(p_0^k, p_1^k)$ pairs and aggregate across folds.\n",
    "\n",
    "**Justification & Equivalence:**\n",
    "\n",
    "*   **Follows Definition:** This exactly mirrors the definition of IVAP given in Vovk et al. (2015, lines L11-L13):\n",
    "    > \"When a new test object x arrives, compute its score s. Fit isotonic regression to (s1, y1),..., (sk, yk), (s, 0) obtaining a function f0. Fit isotonic regression to (s1, y1), ..., (sk, yk), (s, 1) obtaining a function f1. The multiprobability prediction ... is the pair (p0, p1) := (f0(s), f1(s))\"\n",
    "*   **Mathematical Equivalence:** Importantly, **Approach 2 yields the exact same $(p_0, p_1)$ values as the correctly implemented Approach 1**. The efficient algorithms (Approach 1) were proven to be equivalent ways of computing the result of the definition (Approach 2). Approach 2 is essentially a brute-force evaluation of the functions defined by the GCM/LCM construction when the efficient lookup code isn't suitable.\n",
    "*   **Handles Raw Scores:** Standard isotonic regression libraries (`sklearn.isotonic.IsotonicRegression`) naturally handle arbitrary real-valued inputs (raw scores).\n",
    "\n",
    "**Why Use Approach 2?**\n",
    "\n",
    "Given that Approach 1 is theoretically valid and much faster (O(log k) vs O(k) isotonic fits per test point), why use Approach 2?\n",
    "1.  **Implementation Convenience:** It avoids modifying existing probability-based library code (like `venn_abers.py`). One can use standard IR tools directly.\n",
    "2.  **Direct Definition:** It serves as a clear, direct implementation of the foundational IVAP definition.\n",
    "\n",
    "The trade-off is **computational cost during prediction**, which becomes O(K \\* n_test \\* k_fold), where k_fold is the cost of the IR fit (roughly linear in the fold size). This can be prohibitive for very large test sets or calibration fold sizes.\n",
    "\n",
    "**Why Approach 2 is Not \"Online Training\":**\n",
    "\n",
    "This approach remains **inductive**:\n",
    "1.  The base model generating scores is fixed during prediction.\n",
    "2.  The OOF calibration sets $(S_{cal}^k, y_{cal}^k)$ are fixed for each fold.\n",
    "3.  The repeated IR fitting computes the output of a *fixed* (though complex) calibration rule defined by the OOF set for that fold; it does not update the rule itself based on test data (beyond the single test point required by the definition).\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "While the efficient Venn-Abers/IVAP algorithm (Approach 1) is theoretically sound for raw scores, practical implementations often assume probability inputs. If using such implementations or needing a direct application of the definition, Approach 2 (re-fitting IR twice per test point within each CVAP fold) is the correct method for handling raw scores. It is mathematically equivalent to Approach 1 but computationally more expensive at prediction time. This necessity arises from implementation details, not theoretical limitations of the efficient algorithm itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPROACH 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raw_cvap import RawVennAbers, CVAPPredictorRaw\n",
    "\n",
    "# --- Helper Function to Get Scores ---\n",
    "def get_scores(estimator, X, score_method):\n",
    "    \"\"\"Gets scores from an estimator based on the specified method.\"\"\"\n",
    "    if score_method == 'decision_function':\n",
    "        if hasattr(estimator, 'decision_function'):\n",
    "            scores = estimator.decision_function(X)\n",
    "            # Ensure scores are 1D for binary classification\n",
    "            if scores.ndim == 2 and scores.shape[1] == 1:\n",
    "                 scores = scores.flatten()\n",
    "            elif scores.ndim > 1:\n",
    "                 # For binary, decision_function should be 1D. If not, maybe multiclass? Raise error.\n",
    "                 raise ValueError(f\"decision_function returned shape {scores.shape}, expected 1D for binary classification.\")\n",
    "            return scores\n",
    "        else:\n",
    "            raise AttributeError(f\"{estimator.__class__.__name__} does not have 'decision_function' method.\")\n",
    "    elif score_method == 'predict_proba':\n",
    "        if hasattr(estimator, 'predict_proba'):\n",
    "            # Return probability of the positive class (class 1)\n",
    "            proba = estimator.predict_proba(X)\n",
    "            if proba.shape[1] != 2:\n",
    "                 raise ValueError(f\"predict_proba returned shape {proba.shape}, expected (n_samples, 2)\")\n",
    "            return proba[:, 1]\n",
    "        else:\n",
    "            raise AttributeError(f\"{estimator.__class__.__name__} does not have 'predict_proba' method.\")\n",
    "    elif score_method == 'raw_margin_xgb':\n",
    "        # Check if it looks like an XGBoost model (basic check)\n",
    "        if hasattr(estimator, 'predict') and 'output_margin' in estimator.predict.__code__.co_varnames:\n",
    "             try:\n",
    "                 # XGBoost convention: predict with output_margin=True gives raw scores\n",
    "                 # For binary classification, this is usually a single value per instance\n",
    "                 scores = estimator.predict(X, output_margin=True)\n",
    "                 return scores.flatten() # Ensure 1D\n",
    "             except TypeError as e:\n",
    "                 raise TypeError(f\"Error calling predict with output_margin=True on {estimator.__class__.__name__}. Is it an XGBoost model? Original error: {e}\")\n",
    "        else:\n",
    "             raise AttributeError(f\"{estimator.__class__.__name__} might not be an XGBoost model supporting 'output_margin'.\")\n",
    "    elif score_method == 'raw_score_lgbm':\n",
    "         # Check if it looks like a LightGBM model (basic check)\n",
    "        if hasattr(estimator, 'predict') and 'raw_score' in estimator.predict.__code__.co_varnames:\n",
    "             try:\n",
    "                 # LightGBM convention: predict with raw_score=True gives raw scores\n",
    "                 # For binary classification, output shape might depend on objective.\n",
    "                 # Often (n_samples,) or (n_samples, 1) for binary logloss/cross_entropy\n",
    "                 scores = estimator.predict(X, raw_score=True)\n",
    "                 # Handle potential (n_samples, 1) output for binary\n",
    "                 if scores.ndim == 2 and scores.shape[1] == 1:\n",
    "                     scores = scores.flatten()\n",
    "                 elif scores.ndim != 1:\n",
    "                      # If multiclass raw_score=True might return (n_samples, n_classes)\n",
    "                      raise ValueError(f\"LightGBM raw_score returned shape {scores.shape}. Expected 1D for binary.\")\n",
    "                 return scores\n",
    "             except TypeError as e:\n",
    "                 raise TypeError(f\"Error calling predict with raw_score=True on {estimator.__class__.__name__}. Is it a LightGBM model? Original error: {e}\")\n",
    "        else:\n",
    "             raise AttributeError(f\"{estimator.__class__.__name__} might not be a LightGBM model supporting 'raw_score'.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported score_method: {score_method}. Choose 'decision_function', 'predict_proba', 'raw_margin_xgb', or 'raw_score_lgbm'.\")\n",
    "\n",
    "# --- Main Unified Function ---\n",
    "def train_calibrate_model(base_estimator_class, best_params, X_train, y_train,\n",
    "                          calibration_method='platt', # 'platt', 'isotonic', 'cvap'\n",
    "                          n_splits=5, random_state=None,\n",
    "                          # CVAP specific params\n",
    "                          score_method='decision_function', # 'decision_function', 'predict_proba', 'raw_margin_xgb', 'raw_score_lgbm'\n",
    "                          cvap_loss='log', # 'log' or 'brier' for aggregation\n",
    "                          cvap_precision=None, # Precision for rounding scores in VA fit\n",
    "                          # Platt/Isotonic specific params (CalibratedClassifierCV handles score method)\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Trains a base estimator and calibrates its outputs using the specified method.\n",
    "\n",
    "    For Platt/Isotonic, uses sklearn's CalibratedClassifierCV.\n",
    "    For CVAP, uses the provided VennAbers implementation with k-fold CV,\n",
    "    operating on raw scores specified by `score_method`.\n",
    "\n",
    "    Args:\n",
    "        base_estimator_class: Class of the base estimator (e.g., SVC, RandomForestClassifier).\n",
    "        best_params (dict): Dictionary of best hyperparameters for the base estimator.\n",
    "        X_train (pd.DataFrame or np.ndarray): Training features.\n",
    "        y_train (pd.Series or np.ndarray): Training labels (binary 0/1).\n",
    "        calibration_method (str): 'platt', 'isotonic', or 'cvap'.\n",
    "        n_splits (int): Number of folds for cross-validation (used by all methods).\n",
    "        random_state (int): Random state for reproducibility.\n",
    "        score_method (str): Method to get scores for CVAP calibration.\n",
    "                            Options: 'decision_function', 'predict_proba',\n",
    "                                     'raw_margin_xgb', 'raw_score_lgbm'.\n",
    "                            Ignored for 'platt' and 'isotonic' methods.\n",
    "        cvap_loss (str): Aggregation loss for CVAP ('log' or 'brier').\n",
    "        cvap_precision (int, optional): Precision for rounding scores in CVAP's VennAbers fit.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (fitted_base_estimator, fitted_calibrator_object)\n",
    "               - fitted_base_estimator: The base estimator trained on the full training data.\n",
    "               - fitted_calibrator_object: An object with a `predict_proba` method\n",
    "                 that returns calibrated probabilities.\n",
    "                 For Platt/Isotonic, this is a CalibratedClassifierCV instance.\n",
    "                 For CVAP, this is the custom _CVAPPredictor instance.\n",
    "               Returns (None, None) if an error occurs.\n",
    "    \"\"\"\n",
    "    logging.info(f\"--- Starting Model Training & Calibration ({calibration_method}) ---\")\n",
    "\n",
    "    # Input Type Handling\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        X_train_np = X_train.values\n",
    "    else:\n",
    "        X_train_np = np.asarray(X_train)\n",
    "    if isinstance(y_train, pd.Series):\n",
    "        y_train_np = y_train.values\n",
    "    else:\n",
    "        y_train_np = np.asarray(y_train)\n",
    "    if len(np.unique(y_train_np)) != 2:\n",
    "        raise ValueError(f\"This function currently supports only binary classification. Found labels: {np.unique(y_train_np)}\")\n",
    "\n",
    "    # Instantiate the base estimator\n",
    "    try:\n",
    "        # Special handling for SVC probability if needed by Platt/Isotonic *internal* logic\n",
    "        # CalibratedClassifierCV might internally require probability=True for some base estimators\n",
    "        # even if we don't explicitly use predict_proba. Let's ensure it's set if method needs it.\n",
    "        current_params = best_params.copy()\n",
    "        is_svc = issubclass(base_estimator_class, SVC)\n",
    "\n",
    "        # Check if the chosen calibration method *might* rely on predict_proba internally\n",
    "        needs_proba = False\n",
    "        if calibration_method in ['platt', 'isotonic']:\n",
    "             # CalibratedClassifierCV's default ('auto') tries decision_function first,\n",
    "             # but might fall back to predict_proba. Safest to enable for SVC.\n",
    "             if is_svc: needs_proba = True\n",
    "        elif calibration_method == 'cvap' and score_method == 'predict_proba':\n",
    "             needs_proba = True\n",
    "\n",
    "        if needs_proba and is_svc and not current_params.get('probability', False):\n",
    "             logging.warning(f\"Setting probability=True for SVC as required by calibration method '{calibration_method}' or score_method '{score_method}'.\")\n",
    "             current_params['probability'] = True\n",
    "\n",
    "        base_estimator = base_estimator_class(**current_params)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error instantiating base estimator {base_estimator_class.__name__} with params {current_params}: {e}\", exc_info=True)\n",
    "        return None, None\n",
    "\n",
    "\n",
    "    # --- Calibration Method Logic ---\n",
    "    try:\n",
    "        if calibration_method in ['platt', 'isotonic']:\n",
    "            logging.info(f\"Using CalibratedClassifierCV with method='{'sigmoid' if calibration_method == 'platt' else 'isotonic'}'\")\n",
    "            logging.info(f\"(Ignoring 'score_method' parameter '{score_method}' for CalibratedClassifierCV)\")\n",
    "\n",
    "            cv_strategy = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "            # Pass the potentially modified estimator (e.g., SVC with probability=True)\n",
    "            calibrator = CalibratedClassifierCV(\n",
    "                base_estimator, # Use the instance created above\n",
    "                method='sigmoid' if calibration_method == 'platt' else 'isotonic',\n",
    "                cv=cv_strategy,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            logging.info(\"Fitting CalibratedClassifierCV...\")\n",
    "            calibrator.fit(X_train_np, y_train_np)\n",
    "            logging.info(\"CalibratedClassifierCV fitting complete.\")\n",
    "\n",
    "            # Extract final estimator (same logic as before)\n",
    "            if isinstance(calibrator.base_estimator_, list):\n",
    "                 logging.warning(\"CalibratedClassifierCV returned a list of base estimators. Returning the first one.\")\n",
    "                 final_base_estimator = calibrator.base_estimator_[0]\n",
    "            else:\n",
    "                 final_base_estimator = calibrator.base_estimator_\n",
    "\n",
    "            logging.info(f\"--- {calibration_method.capitalize()} Scaling Training Complete ---\")\n",
    "            return final_base_estimator, calibrator\n",
    "\n",
    "        elif calibration_method == 'cvap':\n",
    "            logging.info(f\"Using Cross Venn-Abers Prediction (CVAP) with score_method='{score_method}'\")\n",
    "\n",
    "            # 1. Out-of-fold raw scores\n",
    "            cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "            oof_scores_list, oof_y_cal_list = [], []\n",
    "            for train_idx, val_idx in cv.split(X_train_np, y_train_np):\n",
    "                est = clone(base_estimator).fit(X_train_np[train_idx], y_train_np[train_idx])\n",
    "                scores = get_scores(est, X_train_np[val_idx], score_method)\n",
    "                oof_scores_list.append(scores)\n",
    "                oof_y_cal_list.append(y_train_np[val_idx])\n",
    "\n",
    "            # 2. Final base model on all data\n",
    "            final_base_estimator = clone(base_estimator).fit(X_train_np, y_train_np)\n",
    "\n",
    "            # 3. Build one RawVennAbers per fold\n",
    "            calibrators = [\n",
    "                RawVennAbers(precision=cvap_precision).fit(scores, y_cal)\n",
    "                for scores, y_cal in zip(oof_scores_list, oof_y_cal_list)\n",
    "            ]\n",
    "\n",
    "            # 4. Wrap them into CVAPPredictorRaw\n",
    "            cvap_predictor = CVAPPredictorRaw(\n",
    "                final_estimator_=final_base_estimator,\n",
    "                calibrators_=calibrators,\n",
    "                loss_=cvap_loss,\n",
    "                score_method_=score_method\n",
    "            )\n",
    "            logging.info(\"--- CVAP Training Complete ---\")\n",
    "            return final_base_estimator, cvap_predictor\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown calibration_method: {calibration_method}. Choose 'platt', 'isotonic', or 'cvap'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during {calibration_method} calibration: {e}\", exc_info=True)\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPROACH 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Helper Function for aggregation of the p0 and p1 values across folds ---\n",
    "# def geo_mean(a):\n",
    "#     \"\"\"Calculates geometric mean along axis 1.\"\"\"\n",
    "#     # Handle potential zeros or negative values before taking the product/root\n",
    "#     # If any value in a row is <= 0, the geometric mean is typically considered 0.\n",
    "#     if a.shape[1] == 0:\n",
    "#         return np.ones(a.shape[0]) # Geometric mean of empty set is 1? Or NaN? Let's return 1.\n",
    "\n",
    "#     # Check for non-positive values\n",
    "#     has_non_positive = np.any(a <= 1e-9, axis=1) # Use tolerance\n",
    "\n",
    "#     # Calculate product safely\n",
    "#     log_a = np.log(np.maximum(a, 1e-9)) # Avoid log(0)\n",
    "#     geo_mean_val = np.exp(np.mean(log_a, axis=1))\n",
    "\n",
    "#     # Set geo_mean to 0 for rows that had non-positive values\n",
    "#     geo_mean_val[has_non_positive] = 0.0\n",
    "\n",
    "#     return geo_mean_val\n",
    "\n",
    "# # --- Helper Function to Get Scores ---\n",
    "# def get_scores(estimator, X, score_method):\n",
    "#     \"\"\"Gets scores from an estimator based on the specified method.\"\"\"\n",
    "#     if score_method == 'decision_function':\n",
    "#         if hasattr(estimator, 'decision_function'):\n",
    "#             scores = estimator.decision_function(X)\n",
    "#             # Ensure scores are 1D for binary classification\n",
    "#             if scores.ndim == 2 and scores.shape[1] == 1:\n",
    "#                  scores = scores.flatten()\n",
    "#             elif scores.ndim > 1:\n",
    "#                  # For binary, decision_function should be 1D. If not, maybe multiclass? Raise error.\n",
    "#                  raise ValueError(f\"decision_function returned shape {scores.shape}, expected 1D for binary classification.\")\n",
    "#             return scores\n",
    "#         else:\n",
    "#             raise AttributeError(f\"{estimator.__class__.__name__} does not have 'decision_function' method.\")\n",
    "#     elif score_method == 'predict_proba':\n",
    "#         if hasattr(estimator, 'predict_proba'):\n",
    "#             # Return probability of the positive class (class 1)\n",
    "#             proba = estimator.predict_proba(X)\n",
    "#             if proba.shape[1] != 2:\n",
    "#                  raise ValueError(f\"predict_proba returned shape {proba.shape}, expected (n_samples, 2)\")\n",
    "#             return proba[:, 1]\n",
    "#         else:\n",
    "#             raise AttributeError(f\"{estimator.__class__.__name__} does not have 'predict_proba' method.\")\n",
    "#     elif score_method == 'raw_margin_xgb':\n",
    "#         # Check if it looks like an XGBoost model (basic check)\n",
    "#         if hasattr(estimator, 'predict') and 'output_margin' in estimator.predict.__code__.co_varnames:\n",
    "#              try:\n",
    "#                  # XGBoost convention: predict with output_margin=True gives raw scores\n",
    "#                  # For binary classification, this is usually a single value per instance\n",
    "#                  scores = estimator.predict(X, output_margin=True)\n",
    "#                  return scores.flatten() # Ensure 1D\n",
    "#              except TypeError as e:\n",
    "#                  raise TypeError(f\"Error calling predict with output_margin=True on {estimator.__class__.__name__}. Is it an XGBoost model? Original error: {e}\")\n",
    "#         else:\n",
    "#              raise AttributeError(f\"{estimator.__class__.__name__} might not be an XGBoost model supporting 'output_margin'.\")\n",
    "#     elif score_method == 'raw_score_lgbm':\n",
    "#          # Check if it looks like a LightGBM model (basic check)\n",
    "#         if hasattr(estimator, 'predict') and 'raw_score' in estimator.predict.__code__.co_varnames:\n",
    "#              try:\n",
    "#                  # LightGBM convention: predict with raw_score=True gives raw scores\n",
    "#                  # For binary classification, output shape might depend on objective.\n",
    "#                  # Often (n_samples,) or (n_samples, 1) for binary logloss/cross_entropy\n",
    "#                  scores = estimator.predict(X, raw_score=True)\n",
    "#                  # Handle potential (n_samples, 1) output for binary\n",
    "#                  if scores.ndim == 2 and scores.shape[1] == 1:\n",
    "#                      scores = scores.flatten()\n",
    "#                  elif scores.ndim != 1:\n",
    "#                       # If multiclass raw_score=True might return (n_samples, n_classes)\n",
    "#                       raise ValueError(f\"LightGBM raw_score returned shape {scores.shape}. Expected 1D for binary.\")\n",
    "#                  return scores\n",
    "#              except TypeError as e:\n",
    "#                  raise TypeError(f\"Error calling predict with raw_score=True on {estimator.__class__.__name__}. Is it a LightGBM model? Original error: {e}\")\n",
    "#         else:\n",
    "#              raise AttributeError(f\"{estimator.__class__.__name__} might not be a LightGBM model supporting 'raw_score'.\")\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported score_method: {score_method}. Choose 'decision_function', 'predict_proba', 'raw_margin_xgb', or 'raw_score_lgbm'.\")\n",
    "\n",
    "# # --- Helper Class for CVAP Prediction ---\n",
    "# class _CVAPPredictor(BaseEstimator, ClassifierMixin):\n",
    "#     \"\"\"Internal helper class to store CVAP results using raw scores and provide prediction.\"\"\"\n",
    "#     def __init__(self, final_base_estimator, oof_scores_list, oof_y_cal_list,\n",
    "#                  score_method, precision=None, loss='log'): # precision is now unused here, but kept for consistency\n",
    "#         self.final_base_estimator_ = final_base_estimator\n",
    "#         self.oof_scores_list_ = oof_scores_list # List of raw score arrays (n_fold_samples,) from each fold\n",
    "#         self.oof_y_cal_list_ = oof_y_cal_list # List of y_cal arrays from each fold\n",
    "#         self.score_method_ = score_method     # How scores were obtained\n",
    "#         # self.precision_ = precision # Precision was for the old VennAbers, not needed for IsotonicRegression\n",
    "#         self.loss_ = loss\n",
    "#         self.n_splits_ = len(oof_scores_list)\n",
    "#         self.classes_ = np.array([0, 1]) # Hardcoded for binary\n",
    "\n",
    "#         if len(self.oof_scores_list_) != len(self.oof_y_cal_list_):\n",
    "#              raise ValueError(\"Mismatch between number of OOF score folds and label folds.\")\n",
    "#         if self.n_splits_ == 0:\n",
    "#              raise ValueError(\"Cannot initialize _CVAPPredictor with zero folds.\")\n",
    "#         for i, scores in enumerate(self.oof_scores_list_):\n",
    "#              if scores.ndim != 1:\n",
    "#                   raise ValueError(f\"OOF scores for fold {i} must be 1D, but got shape {scores.shape}\")\n",
    "#              if len(scores) != len(self.oof_y_cal_list_[i]):\n",
    "#                   raise ValueError(f\"Mismatch between score length ({len(scores)}) and label length ({len(self.oof_y_cal_list_[i])}) in fold {i}\")\n",
    "\n",
    "#     def predict_proba(self, X):\n",
    "#         \"\"\"Generates CVAP calibrated probabilities for new data X using raw scores.\"\"\"\n",
    "#         if not hasattr(self, 'final_base_estimator_') or self.final_base_estimator_ is None:\n",
    "#              raise NotFittedError(\"The final base estimator for CVAP is not available.\")\n",
    "\n",
    "#         X = check_array(X, accept_sparse=True, force_all_finite=False)\n",
    "\n",
    "#         logging.debug(f\"Getting test scores using method: {self.score_method_}\")\n",
    "#         try:\n",
    "#             raw_test_scores = get_scores(self.final_base_estimator_, X, self.score_method_)\n",
    "#             if raw_test_scores.ndim != 1:\n",
    "#                  raise ValueError(f\"get_scores returned non-1D scores (shape {raw_test_scores.shape}) for test data.\")\n",
    "#             logging.debug(f\"Raw test scores sample: {raw_test_scores[:5]}\")\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error getting scores for test data using method {self.score_method_}: {e}\", exc_info=True)\n",
    "#             raise\n",
    "\n",
    "#         n_test_samples = X.shape[0]\n",
    "#         p0p1_test_folds = np.zeros((self.n_splits_, n_test_samples, 2)) # Store p0, p1 for each fold and sample\n",
    "\n",
    "#         # --- CORRECTED Calibration Loop ---\n",
    "#         logging.info(f\"Calculating CVAP probabilities using {self.n_splits_} folds...\")\n",
    "#         for i in tqdm(range(self.n_splits_), desc=\"CVAP Fold Calibration\", leave=False):\n",
    "#             cal_scores_fold = self.oof_scores_list_[i]\n",
    "#             cal_y_fold = self.oof_y_cal_list_[i]\n",
    "\n",
    "#             if len(cal_scores_fold) == 0:\n",
    "#                  logging.warning(f\"Fold {i} has empty calibration data. Assigning default 0.5 probabilities.\")\n",
    "#                  p0p1_test_folds[i, :, 0] = 0.5 # Default p0\n",
    "#                  p0p1_test_folds[i, :, 1] = 0.5 # Default p1\n",
    "#                  continue\n",
    "\n",
    "#             # Pre-allocate results for this fold\n",
    "#             p0_results_fold = np.zeros(n_test_samples)\n",
    "#             p1_results_fold = np.zeros(n_test_samples)\n",
    "\n",
    "#             # Fit Isotonic Regression twice for each test point (as per IVAP definition)\n",
    "#             # This is necessary because the isotonic fit depends on the test point's score\n",
    "#             # and its *hypothesized* label.\n",
    "#             for j in range(n_test_samples):\n",
    "#                 test_score = raw_test_scores[j]\n",
    "\n",
    "#                 # Calculate p0 (assuming test label is 0)\n",
    "#                 scores_aug_0 = np.append(cal_scores_fold, test_score)\n",
    "#                 y_aug_0 = np.append(cal_y_fold, 0)\n",
    "#                 ir_0 = IsotonicRegression(out_of_bounds='clip', y_min=0, y_max=1, increasing='auto')\n",
    "#                 try:\n",
    "#                     ir_0.fit(scores_aug_0, y_aug_0)\n",
    "#                     p0_results_fold[j] = ir_0.predict([test_score])[0]\n",
    "#                 except Exception as e:\n",
    "#                     logging.warning(f\"Isotonic fit for p0 failed fold {i}, sample {j}. Score: {test_score}. Error: {e}. Setting p0=0.5\")\n",
    "#                     p0_results_fold[j] = 0.5 # Fallback\n",
    "\n",
    "#                 # Calculate p1 (assuming test label is 1)\n",
    "#                 scores_aug_1 = np.append(cal_scores_fold, test_score)\n",
    "#                 y_aug_1 = np.append(cal_y_fold, 1)\n",
    "#                 ir_1 = IsotonicRegression(out_of_bounds='clip', y_min=0, y_max=1, increasing='auto')\n",
    "#                 try:\n",
    "#                     ir_1.fit(scores_aug_1, y_aug_1)\n",
    "#                     p1_results_fold[j] = ir_1.predict([test_score])[0]\n",
    "#                 except Exception as e:\n",
    "#                      logging.warning(f\"Isotonic fit for p1 failed fold {i}, sample {j}. Score: {test_score}. Error: {e}. Setting p1=0.5\")\n",
    "#                      p1_results_fold[j] = 0.5 # Fallback\n",
    "\n",
    "#             # Store results for this fold\n",
    "#             p0p1_test_folds[i, :, 0] = p0_results_fold\n",
    "#             p0p1_test_folds[i, :, 1] = p1_results_fold\n",
    "\n",
    "#             logging.debug(f\"Fold {i} p0 sample: {p0_results_fold[:5]}\")\n",
    "#             logging.debug(f\"Fold {i} p1 sample: {p1_results_fold[:5]}\")\n",
    "\n",
    "\n",
    "#         # 3. Aggregate p0, p1 probability bounds across folds\n",
    "#         # Reshape for aggregation: (n_test_samples, n_splits)\n",
    "#         p0_stack = p0p1_test_folds[:, :, 0].T\n",
    "#         p1_stack = p0p1_test_folds[:, :, 1].T\n",
    "#         logging.debug(f\"p0_stack shape: {p0_stack.shape}, p1_stack shape: {p1_stack.shape}\")\n",
    "\n",
    "#         # 4. Calculate final calibrated probability based on loss function\n",
    "#         p_prime = np.zeros((n_test_samples, 2))\n",
    "#         if self.loss_ == 'log':\n",
    "#             geo_mean_1_minus_p0 = geo_mean(1 - p0_stack)\n",
    "#             geo_mean_p1 = geo_mean(p1_stack)\n",
    "#             denominator = geo_mean_1_minus_p0 + geo_mean_p1\n",
    "#             valid_denom = denominator > 1e-9\n",
    "#             p_prime[valid_denom, 1] = geo_mean_p1[valid_denom] / denominator[valid_denom]\n",
    "#             p_prime[~valid_denom, 1] = 0.5 # Default if denominator is zero (p0=1, p1=0 for all folds)\n",
    "#         elif self.loss_ == 'brier':\n",
    "#              # Avoid potential nan if a fold had no results (though handled above)\n",
    "#              mean_p1 = np.nanmean(p1_stack, axis=1)\n",
    "#              mean_p0_sq = np.nanmean(p0_stack**2, axis=1)\n",
    "#              mean_p1_sq = np.nanmean(p1_stack**2, axis=1)\n",
    "#              p_prime[:, 1] = mean_p1 + 0.5 * mean_p0_sq - 0.5 * mean_p1_sq\n",
    "#              # Replace potential NaN with 0.5 if all folds failed for a sample\n",
    "#              p_prime[np.isnan(p_prime[:, 1]), 1] = 0.5\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unsupported loss function for CVAP aggregation: {self.loss_}\")\n",
    "\n",
    "#         # Ensure probabilities are valid\n",
    "#         p_prime[:, 1] = np.clip(p_prime[:, 1], 0, 1)\n",
    "#         p_prime[:, 0] = 1 - p_prime[:, 1]\n",
    "\n",
    "#         logging.debug(f\"Final calibrated probs sample: {p_prime[:5, 1]}\")\n",
    "#         return p_prime\n",
    "\n",
    "#     # --- predict and _more_tags remain unchanged ---\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"Predicts class labels.\"\"\"\n",
    "#         proba = self.predict_proba(X)\n",
    "#         return self.classes_[np.argmax(proba, axis=1)]\n",
    "\n",
    "#     def _more_tags(self):\n",
    "#         return {'binary_only': True}\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#          # Dummy fit method needed for sklearn compatibility if used directly\n",
    "#          check_X_y(X, y, accept_sparse=True, force_all_finite=False)\n",
    "#          if not hasattr(self, 'final_base_estimator_'):\n",
    "#               raise NotFittedError(\"Cannot call fit on _CVAPPredictor directly. It's fitted internally.\")\n",
    "#          self.classes_ = np.unique(y)\n",
    "#          if len(self.classes_) != 2:\n",
    "#               raise ValueError(\"CVAP Predictor internal error: Expected 2 classes.\")\n",
    "#          return self\n",
    "\n",
    "\n",
    "# # --- Main Unified Function ---\n",
    "# def train_calibrate_model(base_estimator_class, best_params, X_train, y_train,\n",
    "#                           calibration_method='platt', # 'platt', 'isotonic', 'cvap'\n",
    "#                           n_splits=5, random_state=None,\n",
    "#                           # CVAP specific params\n",
    "#                           score_method='decision_function', # 'decision_function', 'predict_proba', 'raw_margin_xgb', 'raw_score_lgbm'\n",
    "#                           cvap_loss='log', # 'log' or 'brier' for aggregation\n",
    "#                           cvap_precision=None, # Precision for rounding scores in VA fit\n",
    "#                           # Platt/Isotonic specific params (CalibratedClassifierCV handles score method)\n",
    "#                          ):\n",
    "#     \"\"\"\n",
    "#     Trains a base estimator and calibrates its outputs using the specified method.\n",
    "\n",
    "#     For Platt/Isotonic, uses sklearn's CalibratedClassifierCV.\n",
    "#     For CVAP, uses the provided VennAbers implementation with k-fold CV,\n",
    "#     operating on raw scores specified by `score_method`.\n",
    "\n",
    "#     Args:\n",
    "#         base_estimator_class: Class of the base estimator (e.g., SVC, RandomForestClassifier).\n",
    "#         best_params (dict): Dictionary of best hyperparameters for the base estimator.\n",
    "#         X_train (pd.DataFrame or np.ndarray): Training features.\n",
    "#         y_train (pd.Series or np.ndarray): Training labels (binary 0/1).\n",
    "#         calibration_method (str): 'platt', 'isotonic', or 'cvap'.\n",
    "#         n_splits (int): Number of folds for cross-validation (used by all methods).\n",
    "#         random_state (int): Random state for reproducibility.\n",
    "#         score_method (str): Method to get scores for CVAP calibration.\n",
    "#                             Options: 'decision_function', 'predict_proba',\n",
    "#                                      'raw_margin_xgb', 'raw_score_lgbm'.\n",
    "#                             Ignored for 'platt' and 'isotonic' methods.\n",
    "#         cvap_loss (str): Aggregation loss for CVAP ('log' or 'brier').\n",
    "#         cvap_precision (int, optional): Precision for rounding scores in CVAP's VennAbers fit.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: (fitted_base_estimator, fitted_calibrator_object)\n",
    "#                - fitted_base_estimator: The base estimator trained on the full training data.\n",
    "#                - fitted_calibrator_object: An object with a `predict_proba` method\n",
    "#                  that returns calibrated probabilities.\n",
    "#                  For Platt/Isotonic, this is a CalibratedClassifierCV instance.\n",
    "#                  For CVAP, this is the custom _CVAPPredictor instance.\n",
    "#                Returns (None, None) if an error occurs.\n",
    "#     \"\"\"\n",
    "#     logging.info(f\"--- Starting Model Training & Calibration ({calibration_method}) ---\")\n",
    "\n",
    "#     # Input Type Handling\n",
    "#     if isinstance(X_train, pd.DataFrame):\n",
    "#         X_train_np = X_train.values\n",
    "#     else:\n",
    "#         X_train_np = np.asarray(X_train)\n",
    "#     if isinstance(y_train, pd.Series):\n",
    "#         y_train_np = y_train.values\n",
    "#     else:\n",
    "#         y_train_np = np.asarray(y_train)\n",
    "#     if len(np.unique(y_train_np)) != 2:\n",
    "#         raise ValueError(f\"This function currently supports only binary classification. Found labels: {np.unique(y_train_np)}\")\n",
    "\n",
    "#     # Instantiate the base estimator\n",
    "#     try:\n",
    "#         # Special handling for SVC probability if needed by Platt/Isotonic *internal* logic\n",
    "#         # CalibratedClassifierCV might internally require probability=True for some base estimators\n",
    "#         # even if we don't explicitly use predict_proba. Let's ensure it's set if method needs it.\n",
    "#         current_params = best_params.copy()\n",
    "#         is_svc = issubclass(base_estimator_class, SVC)\n",
    "\n",
    "#         # Check if the chosen calibration method *might* rely on predict_proba internally\n",
    "#         needs_proba = False\n",
    "#         if calibration_method in ['platt', 'isotonic']:\n",
    "#              # CalibratedClassifierCV's default ('auto') tries decision_function first,\n",
    "#              # but might fall back to predict_proba. Safest to enable for SVC.\n",
    "#              if is_svc: needs_proba = True\n",
    "#         elif calibration_method == 'cvap' and score_method == 'predict_proba':\n",
    "#              needs_proba = True\n",
    "\n",
    "#         if needs_proba and is_svc and not current_params.get('probability', False):\n",
    "#              logging.warning(f\"Setting probability=True for SVC as required by calibration method '{calibration_method}' or score_method '{score_method}'.\")\n",
    "#              current_params['probability'] = True\n",
    "\n",
    "#         base_estimator = base_estimator_class(**current_params)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error instantiating base estimator {base_estimator_class.__name__} with params {current_params}: {e}\", exc_info=True)\n",
    "#         return None, None\n",
    "\n",
    "\n",
    "#     # --- Calibration Method Logic ---\n",
    "#     try:\n",
    "#         if calibration_method in ['platt', 'isotonic']:\n",
    "#             logging.info(f\"Using CalibratedClassifierCV with method='{'sigmoid' if calibration_method == 'platt' else 'isotonic'}'\")\n",
    "#             logging.info(f\"(Ignoring 'score_method' parameter '{score_method}' for CalibratedClassifierCV)\")\n",
    "\n",
    "#             cv_strategy = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "#             # Pass the potentially modified estimator (e.g., SVC with probability=True)\n",
    "#             calibrator = CalibratedClassifierCV(\n",
    "#                 base_estimator, # Use the instance created above\n",
    "#                 method='sigmoid' if calibration_method == 'platt' else 'isotonic',\n",
    "#                 cv=cv_strategy,\n",
    "#                 n_jobs=-1\n",
    "#             )\n",
    "\n",
    "#             logging.info(\"Fitting CalibratedClassifierCV...\")\n",
    "#             calibrator.fit(X_train_np, y_train_np)\n",
    "#             logging.info(\"CalibratedClassifierCV fitting complete.\")\n",
    "\n",
    "#             # Extract final estimator (same logic as before)\n",
    "#             if isinstance(calibrator.base_estimator_, list):\n",
    "#                  logging.warning(\"CalibratedClassifierCV returned a list of base estimators. Returning the first one.\")\n",
    "#                  final_base_estimator = calibrator.base_estimator_[0]\n",
    "#             else:\n",
    "#                  final_base_estimator = calibrator.base_estimator_\n",
    "\n",
    "#             logging.info(f\"--- {calibration_method.capitalize()} Scaling Training Complete ---\")\n",
    "#             return final_base_estimator, calibrator\n",
    "\n",
    "#         elif calibration_method == 'cvap':\n",
    "#             logging.info(f\"Using Cross Venn-Abers Prediction (CVAP) with score_method='{score_method}'\")\n",
    "\n",
    "#             # Check if the chosen score_method is available on a temp instance *before* CV\n",
    "#             # try:\n",
    "#             #      temp_estimator = clone(base_estimator)\n",
    "#             #      # Fit on a tiny subset just to enable score method call\n",
    "#             #      temp_estimator.fit(X_train_np[:2], y_train_np[:2])\n",
    "#             #      _ = get_scores(temp_estimator, X_train_np[:2], score_method)\n",
    "#             #      logging.info(f\"Score method '{score_method}' seems available on {base_estimator_class.__name__}.\")\n",
    "#             # except (AttributeError, TypeError, ValueError) as e:\n",
    "#             #      logging.error(f\"Score method '{score_method}' not available or failed for estimator {base_estimator_class.__name__}: {e}\", exc_info=True)\n",
    "#             #      raise AttributeError(f\"Estimator {base_estimator_class.__name__} does not support the required score_method '{score_method}'. Error: {e}\") from e\n",
    "\n",
    "\n",
    "#             # 1. Get out-of-fold *raw scores* using k-fold CV\n",
    "#             logging.info(f\"Performing {n_splits}-fold CV to get out-of-fold scores for CVAP...\")\n",
    "#             cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "#             oof_scores_list = [] # List to store raw score arrays (n_fold_samples,)\n",
    "#             oof_y_cal_list = [] # List to store y_cal arrays\n",
    "\n",
    "#             for fold, (train_idx, val_idx) in enumerate(tqdm(cv.split(X_train_np, y_train_np), total=n_splits, desc=f\"CVAP OOF Scores ({score_method})\", leave=False)):\n",
    "#                 X_train_fold, X_val_fold = X_train_np[train_idx], X_train_np[val_idx]\n",
    "#                 y_train_fold, y_val_fold = y_train_np[train_idx], y_train_np[val_idx]\n",
    "\n",
    "#                 estimator_fold = clone(base_estimator)\n",
    "#                 estimator_fold.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "#                 # Get RAW scores for the validation set using the specified method\n",
    "#                 scores_fold = get_scores(estimator_fold, X_val_fold, score_method)\n",
    "#                 if scores_fold.ndim != 1 or len(scores_fold) != len(y_val_fold):\n",
    "#                      raise ValueError(f\"Fold {fold} get_scores returned unexpected shape {scores_fold.shape} or length, expected ({len(y_val_fold)},)\")\n",
    "\n",
    "#                 oof_scores_list.append(scores_fold)\n",
    "#                 oof_y_cal_list.append(y_val_fold)\n",
    "\n",
    "#             logging.info(\"Out-of-fold scores collected.\")\n",
    "\n",
    "#             # 2. Train the final base model on the entire training set\n",
    "#             logging.info(\"Training final base model on full training data...\")\n",
    "#             final_base_estimator = clone(base_estimator)\n",
    "#             final_base_estimator.fit(X_train_np, y_train_np)\n",
    "#             logging.info(\"Final base model trained.\")\n",
    "\n",
    "#             # 3. Create the CVAP Predictor object using raw scores\n",
    "#             cvap_predictor = _CVAPPredictor(\n",
    "#                 final_base_estimator=final_base_estimator,\n",
    "#                 oof_scores_list=oof_scores_list, # Pass raw scores\n",
    "#                 oof_y_cal_list=oof_y_cal_list,\n",
    "#                 score_method=score_method,      # Pass score method\n",
    "#                 precision=cvap_precision,\n",
    "#                 loss=cvap_loss\n",
    "#             )\n",
    "#             logging.info(\"CVAP Predictor object created.\")\n",
    "\n",
    "#             logging.info(\"--- CVAP Training Complete ---\")\n",
    "#             return final_base_estimator, cvap_predictor # Return the predictor object\n",
    "\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unknown calibration_method: {calibration_method}. Choose 'platt', 'isotonic', or 'cvap'.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error during {calibration_method} calibration: {e}\", exc_info=True)\n",
    "#         return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mondrian Inductive Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Define the helper function\n",
    "def probs_to_alphas(p_mat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Transform calibrated probability matrix\n",
    "    (n_samples, n_classes) → non-conformity scores α\n",
    "    using  α = 1 − p.\n",
    "    \"\"\"\n",
    "    if not isinstance(p_mat, np.ndarray) or p_mat.ndim != 2:\n",
    "         raise ValueError(\"Input must be a 2D numpy probability matrix.\")\n",
    "    return 1.0 - p_mat\n",
    "\n",
    "# Option 2: If you imported `margin`(from crepes.extras import margin), you don't strictly need probs_to_alphas,\n",
    "#           you can call margin(probs_cal) and margin(probs_test) later.\n",
    "#           We'll use the defined function probs_to_alphas below for clarity.\n",
    "\n",
    "\n",
    "# Add the Mondrian ICP wrapper function. First the fit function that uses the calibration split.\n",
    "def fit_mondrian_classifier(probs_cal, bins_cal=None):\n",
    "    \"\"\"\n",
    "    Fits a crepes ConformalClassifier using calibration probabilities.\n",
    "\n",
    "    Args:\n",
    "        probs_cal (np.ndarray): Calibrated probabilities (n_cal, n_classes).\n",
    "        bins_cal (np.ndarray, optional): Mondrian bins for calibration set. Defaults to None (standard ICP).\n",
    "\n",
    "    Returns:\n",
    "        ConformalClassifier: The fitted crepes classifier object, or None if fitting fails.\n",
    "    \"\"\"\n",
    "    logging.info(\"--- Fitting Mondrian Conformal Classifier ---\")\n",
    "    if probs_cal is None or len(probs_cal) == 0:\n",
    "        logging.error(\"Calibration probabilities are empty or None. Cannot fit Mondrian classifier.\")\n",
    "        return None\n",
    "    try:\n",
    "        alphas_cal = probs_to_alphas(probs_cal) # Or use margin(probs_cal)\n",
    "        cc = ConformalClassifier()\n",
    "        cc.fit(alphas_cal, bins=bins_cal)\n",
    "        logging.info(\"--- Mondrian Conformal Classifier Fitted ---\")\n",
    "        return cc\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fitting Mondrian classifier: {e}\", exc_info=True)\n",
    "        return None\n",
    "    \n",
    "# The function to evaluate MICP in the test split\n",
    "def evaluate_mondrian_prediction(fitted_cc, probs_test, y_test_true, bins_test=None, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Evaluates the fitted Mondrian classifier on the test set, including class-specific coverage.\n",
    "\n",
    "    Args:\n",
    "        fitted_cc (ConformalClassifier): The pre-fitted crepes classifier object.\n",
    "        probs_test (np.ndarray): Calibrated probabilities for test set (n_test, n_classes).\n",
    "        y_test_true (np.ndarray): True labels for the test set.\n",
    "        bins_test (np.ndarray, optional): Mondrian bins for test set. Defaults to None.\n",
    "        alpha (float): The significance level (1 - confidence).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (coverage, avg_set_size, prediction_sets, class_coverage_dict) or (None, None, None, None) if evaluation fails.\n",
    "    \"\"\"\n",
    "    logging.info(f\"--- Evaluating Mondrian Prediction (alpha={alpha}) ---\")\n",
    "    if fitted_cc is None:\n",
    "        logging.error(\"Fitted classifier is None. Cannot evaluate.\")\n",
    "        return None, None, None, None\n",
    "    if probs_test is None or len(probs_test) == 0 or y_test_true is None or len(y_test_true) == 0:\n",
    "        logging.warning(\"Test probabilities or labels are empty/None. Skipping evaluation.\")\n",
    "        # Return values indicating skipped evaluation but not necessarily an error state\n",
    "        return 0.0, 0.0, np.array([[]]), {}\n",
    "\n",
    "    try:\n",
    "        alphas_test = probs_to_alphas(probs_test) # Or use margin(probs_test)\n",
    "        pred_sets = fitted_cc.predict_set(alphas_test,\n",
    "                                          bins=bins_test,\n",
    "                                          confidence=1 - alpha) # boolean array (n_test, n_classes)\n",
    "\n",
    "        # Evaluate Coverage and Size\n",
    "        y_test_true_np = np.asarray(y_test_true) # Ensure numpy array\n",
    "        n_test = len(y_test_true_np)\n",
    "        if n_test == 0:\n",
    "            return 0.0, 0.0, np.array([[]]), {}\n",
    "\n",
    "        # Check if true label is in the non-zero indices (predicted classes) of the set row\n",
    "        contains = np.array([y_test_true_np[i] in np.where(pred_sets[i])[0] for i in range(n_test)])\n",
    "\n",
    "        coverage = contains.mean() if n_test > 0 else 0.0\n",
    "        avg_size = pred_sets.sum(axis=1).mean() if n_test > 0 else 0.0\n",
    "\n",
    "        # Class-specific coverage\n",
    "        class_coverage_dict = {}\n",
    "        unique_classes = np.unique(y_test_true_np)\n",
    "        for cls in unique_classes:\n",
    "            idx = np.where(y_test_true_np == cls)[0]\n",
    "            if len(idx) == 0:\n",
    "                class_coverage = np.nan\n",
    "            else:\n",
    "                class_coverage = contains[idx].mean()\n",
    "            class_coverage_dict[cls] = class_coverage\n",
    "            logging.info(f\"Mondrian CP Coverage for class {cls}: {class_coverage:.4f}\")\n",
    "\n",
    "        # Log a warning if any conformal set is empty\n",
    "        empty_sets = np.where(pred_sets.sum(axis=1) == 0)[0]\n",
    "        n_empty = len(empty_sets)\n",
    "        if n_empty > 0:\n",
    "            logging.warning(f\"{n_empty} conformal prediction sets are empty out of {n_test} samples.\")\n",
    "\n",
    "        logging.info(f\"Mondrian CP Coverage: {coverage:.4f}\")\n",
    "        logging.info(f\"Mondrian CP Avg Set Size: {avg_size:.4f}\")\n",
    "        logging.info(\"--- Mondrian Prediction Evaluation Complete ---\")\n",
    "\n",
    "        return coverage, avg_size, pred_sets, class_coverage_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during Mondrian prediction evaluation: {e}\", exc_info=True)\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Comprehensive Metrics ---\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_proba, model_name=\"Model\", conf_mat = False):\n",
    "    \"\"\"\n",
    "    Calculates a comprehensive set of classification metrics.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Ground truth labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        y_proba (array-like): Predicted probabilities for the positive class (class 1).\n",
    "        model_name (str): Name of the model for logging.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing calculated metrics.\n",
    "              Returns None if input arrays are empty or invalid.\n",
    "    \"\"\"\n",
    "    if len(y_true) == 0 or len(y_pred) == 0 or len(y_proba) == 0:\n",
    "        logging.error(f\"[{model_name}] Empty input arrays provided for metric calculation.\")\n",
    "        return None\n",
    "    if len(y_true) != len(y_pred) or len(y_true) != len(y_proba):\n",
    "        logging.error(f\"[{model_name}] Mismatched lengths in input arrays for metric calculation.\")\n",
    "        return None\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # --- Threshold-based Metrics (using y_pred) ---\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['precision'] = precision\n",
    "    metrics['recall_tpr'] = recall # True Positive Rate (Sensitivity)\n",
    "    metrics['f1_score'] = f1\n",
    "\n",
    "    # Specificity (True Negative Rate)\n",
    "    metrics['specificity_tnr'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "    # Geometric Mean\n",
    "    metrics['g_mean'] = np.sqrt(metrics['recall_tpr'] * metrics['specificity_tnr'])\n",
    "\n",
    "    # Confusion Matrix\n",
    "    metrics['confusion_matrix'] = {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp}\n",
    "\n",
    "    # --- Ranking/Probabilistic Metrics (using y_proba) ---\n",
    "    try:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_true, y_proba)\n",
    "    except ValueError as e:\n",
    "        logging.warning(f\"[{model_name}] Could not calculate ROC AUC: {e}. Setting to 0.0.\")\n",
    "        metrics['roc_auc'] = 0.0 # Handle cases with only one class present\n",
    "\n",
    "    # PR AUC\n",
    "    pr_curve_precision, pr_curve_recall, _ = precision_recall_curve(y_true, y_proba)\n",
    "    metrics['pr_auc'] = auc(pr_curve_recall, pr_curve_precision) # Note order: recall is x, precision is y\n",
    "\n",
    "    # Brier Score\n",
    "    metrics['brier_score'] = brier_score_loss(y_true, y_proba)\n",
    "\n",
    "    logging.info(f\"--- {model_name} Metrics ---\")\n",
    "    logging.info(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    logging.info(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    logging.info(f\"Recall (TPR): {metrics['recall_tpr']:.4f}\")\n",
    "    logging.info(f\"Specificity (TNR): {metrics['specificity_tnr']:.4f}\")\n",
    "    logging.info(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    logging.info(f\"G-Mean: {metrics['g_mean']:.4f}\")\n",
    "    logging.info(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    logging.info(f\"PR AUC: {metrics['pr_auc']:.4f}\")\n",
    "    logging.info(f\"Brier Score: {metrics['brier_score']:.4f}\")\n",
    "    logging.info(f\"Confusion Matrix (TN, FP, FN, TP): ({tn}, {fp}, {fn}, {tp})\")\n",
    "\n",
    "    # Optional: Plot Confusion Matrix\n",
    "    if conf_mat:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap([[tn, fp], [fn, tp]], annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['Predicted Galaxy (0)', 'Predicted Star (1)'],\n",
    "                    yticklabels=['Actual Galaxy (0)', 'Actual Star (1)'])\n",
    "        plt.title(f'{model_name} Confusion Matrix')\n",
    "        plt.ylabel('Actual Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        cm_filename = os.path.join(MODEL_DIR, f\"{model_name}_confusion_matrix_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\")\n",
    "        plt.savefig(cm_filename)\n",
    "        plt.close()\n",
    "        logging.info(f\"Confusion matrix plot saved to {cm_filename}\")\n",
    "\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Scaling ---\n",
    "# Important for SVM, not used for the other models.\n",
    "# Fit scaler ONLY on training data, then transform all sets.\n",
    "\n",
    "# Check if training set and other datasets are non-empty before scaling\n",
    "def apply_feature_scaling(\n",
    "    X_train, X_val, X_test, X_cal,\n",
    "    TRAIN_SIZE, VAL_SIZE, TEST_SIZE, CAL_SIZE,\n",
    "    MODEL_DIR,\n",
    "    save_scaler=True,  # New param: whether to save the scaler to disk\n",
    "    group_name=None    # New param: optional, for unique scaler filename per group\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies StandardScaler to the provided datasets if training data is available.\n",
    "    Optionally saves the fitted scaler to disk.\n",
    "    Returns: X_train_scaled, X_val_scaled, X_test_scaled, X_cal_scaled, scaler (or None)\n",
    "    \"\"\"\n",
    "\n",
    "    scaler = None\n",
    "    if len(X_train) > 0 and TRAIN_SIZE > 0:\n",
    "        logging.info(\"Applying StandardScaler to features...\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "    else:\n",
    "        logging.info(\"Empty training set, NOT able to apply StandardScaler!\")\n",
    "        X_train_scaled = X_train\n",
    "\n",
    "    if len(X_val) > 0 and VAL_SIZE > 0 and scaler is not None:\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "    else:\n",
    "        X_val_scaled = X_val\n",
    "\n",
    "    if len(X_test) > 0 and TEST_SIZE > 0 and scaler is not None:\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_test_scaled = X_test\n",
    "\n",
    "    if len(X_cal) > 0 and CAL_SIZE > 0 and scaler is not None:\n",
    "        X_cal_scaled = scaler.transform(X_cal)\n",
    "    else:\n",
    "        X_cal_scaled = X_cal\n",
    "\n",
    "    # Save the scaler if it was fitted and requested\n",
    "    if scaler is not None and save_scaler:\n",
    "        if group_name is not None:\n",
    "            scaler_filename = os.path.join(\n",
    "                MODEL_DIR, f\"scaler_{group_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib\"\n",
    "            )\n",
    "        else:\n",
    "            scaler_filename = os.path.join(\n",
    "                MODEL_DIR, f\"scaler_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib\"\n",
    "            )\n",
    "        joblib.dump(scaler, scaler_filename)\n",
    "        logging.info(f\"Scaler saved to {scaler_filename}\")\n",
    "\n",
    "    logging.info(\"Feature scaling complete.\")\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, X_cal_scaled, scaler\n",
    "\n",
    "# Example usage:\n",
    "# X_train_scaled, X_val_scaled, X_test_scaled, X_cal_scaled, scaler = apply_feature_scaling(\n",
    "#     X_train, X_val, X_test, X_cal, TRAIN_SIZE, VAL_SIZE, TEST_SIZE, CAL_SIZE, MODEL_DIR,\n",
    "#     save_scaler=True, # Ensure scaler is saved\n",
    "#     group_name=group_name # Pass group name for potentially unique scaler filename\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {} # Dictionary to store metrics for each model\n",
    "\n",
    "ALPHA = 0.8 #Coverage, mean size set will tend to 1 + ALPHA\n",
    "\n",
    "CALIBRATOR = 'cvap'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for SVM (using data fraction)\n",
    "MAX_RESOURCE_SVM = 1.0  # Max data fraction\n",
    "MIN_RESOURCE_SVM = 0.1  # Min data fraction (adjust based on minority class size)\n",
    "ETA_SVM = 3\n",
    "RESOURCE_TYPE_SVM = 'data_fraction'\n",
    "model_name_svm = \"SVM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¡MODIFICADO PARA PRUEBA RÁPIDA!\n",
    "MAX_RESOURCE_SVM = 1.0  # Se mantiene en 1.0 para usar todos los datos al final\n",
    "MIN_RESOURCE_SVM = 0.5  # Aumentado para reducir s_max (menos brackets/configs)\n",
    "ETA_SVM = 4             # Aumentado para eliminar configuraciones más rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_hpo(\n",
    "    X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "    MAX_RESOURCE_SVM,\n",
    "    MIN_RESOURCE_SVM,\n",
    "    ETA_SVM,\n",
    "    RESOURCE_TYPE_SVM,\n",
    "    RANDOM_SEED,\n",
    "    SVC,\n",
    "    loguniform,\n",
    "    hyperband_hpo,\n",
    "    f1_score,\n",
    "    logging,\n",
    "    model_name_svm\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Hyperband HPO for SVM and return best parameters and best score.\n",
    "    \"\"\"\n",
    "    # --- SVM: Define Search Space and HPO Params ---\n",
    "    param_space_svm = {\n",
    "        'C': loguniform(1e-2, 1e3),\n",
    "        'gamma': loguniform(1e-4, 1e1),\n",
    "        'kernel': ['rbf'], # Example: Fixed RBF kernel\n",
    "        # 'kernel': ['rbf', 'linear'], # Example: If you want to search kernels\n",
    "        # class_weight is added automatically inside hyperband_hpo\n",
    "        # random_state is added automatically inside hyperband_hpo\n",
    "    }\n",
    "\n",
    "    logging.info(f\"--- [{model_name_svm}] Running Hyperband HPO ---\")\n",
    "    best_params_svm, best_score_hpo_svm = hyperband_hpo(\n",
    "        model_class=SVC,\n",
    "        param_space=param_space_svm,\n",
    "        X_train=X_train_scaled, # USE SCALED DATA\n",
    "        y_train=y_train,\n",
    "        X_val=X_val_scaled,     # USE SCALED DATA\n",
    "        y_val=y_val,\n",
    "        max_resource=MAX_RESOURCE_SVM,\n",
    "        eta=ETA_SVM,\n",
    "        resource_type=RESOURCE_TYPE_SVM,\n",
    "        min_resource=MIN_RESOURCE_SVM,\n",
    "        scoring_func=f1_score,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    return best_params_svm, best_score_hpo_svm\n",
    "\n",
    "def svm_workflow(\n",
    "    X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "    X_cal_scaled, y_cal, X_test_scaled, y_test,\n",
    "    MAX_RESOURCE_SVM, MIN_RESOURCE_SVM, ETA_SVM, RESOURCE_TYPE_SVM,\n",
    "    model_name_svm, CALIBRATOR, ALPHA, RANDOM_SEED,\n",
    "    SVC=SVC,\n",
    "    loguniform=loguniform,\n",
    "    hyperband_hpo=hyperband_hpo,\n",
    "    f1_score=f1_score,\n",
    "    logging=logging,\n",
    "    best_params_svm=None,  # <-- Allow passing best_params directly\n",
    "    best_score_hpo_svm=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete SVM workflow: HPO, calibration, Mondrian ICP, and evaluation.\n",
    "\n",
    "    This function performs the following steps for SVM:\n",
    "      1. Defines the hyperparameter search space.\n",
    "      2. Runs Hyperband HPO using the provided data splits.\n",
    "      3. Trains the final SVM model and calibrates it (Platt/Isotonic/CVAP).\n",
    "      4. Fits Mondrian Inductive Conformal Predictor (ICP) on the calibration set.\n",
    "      5. Evaluates the model and Mondrian ICP on the test set.\n",
    "      6. Stores results in the provided all_results dictionary.\n",
    "\n",
    "    All logic is preserved from the original notebook cell.\n",
    "\n",
    "    Args:\n",
    "        X_train_scaled, y_train: Training data (scaled, labels).\n",
    "        X_val_scaled, y_val: Validation data (scaled, labels).\n",
    "        X_cal_scaled, y_cal: Calibration data (scaled, labels).\n",
    "        X_test_scaled, y_test: Test data (scaled, labels).\n",
    "        \n",
    "        MAX_RESOURCE_SVM, MIN_RESOURCE_SVM, ETA_SVM, RESOURCE_TYPE_SVM: HPO settings.\n",
    "        model_name_svm: Name for the model (e.g., \"SVM\").\n",
    "        CALIBRATOR: Calibration method ('platt', 'isotonic', 'cvap').\n",
    "        ALPHA: Conformal prediction significance level.\n",
    "        RANDOM_SEED: Random seed for reproducibility.\n",
    "        best_params_svm: (Optional) If provided, skips HPO and uses these params\n",
    "        best_score_hpo_svm: (Optional) If provided, uses this as HPO score\n",
    "\n",
    "    Returns:\n",
    "        None. Results are stored in all_results[model_name_svm].\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from scipy.stats import loguniform\n",
    "    import logging\n",
    "\n",
    "    logging.info(f\"\\n\\n===== Starting Workflow for {model_name_svm} =====\")\n",
    "    timestamp_svm = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if best_params_svm is None:\n",
    "        hpo_start_time_svm = time.time()\n",
    "        best_params_svm, best_score_hpo_svm = svm_hpo(\n",
    "            X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "            MAX_RESOURCE_SVM,\n",
    "            MIN_RESOURCE_SVM,\n",
    "            ETA_SVM,\n",
    "            RESOURCE_TYPE_SVM,\n",
    "            RANDOM_SEED,\n",
    "            SVC,\n",
    "            loguniform,\n",
    "            hyperband_hpo,\n",
    "            f1_score,\n",
    "            logging,\n",
    "            model_name_svm\n",
    "        )\n",
    "        hpo_duration_svm = time.time() - hpo_start_time_svm\n",
    "        logging.info(f\"--- [{model_name_svm}] HPO finished in {hpo_duration_svm:.2f} seconds ---\")\n",
    "    else:\n",
    "        hpo_duration_svm = 0.0\n",
    "        logging.info(f\"--- [{model_name_svm}] Using provided hyperparameters, skipping HPO. ---\")\n",
    "\n",
    "    # --- 1.3 SVM: Train Final Model & Calibration (using Full Training Set) ---\n",
    "    fitted_svm_base = None\n",
    "    calibrator_svm = None\n",
    "    if best_params_svm:\n",
    "        logging.info(f\"--- [{model_name_svm}] Training final model and Platt scaler ---\")\n",
    "        calibration_start_time_svm = time.time()\n",
    "        # Ensure necessary fixed parameters are present for the final fit\n",
    "        best_params_svm['random_state'] = RANDOM_SEED\n",
    "        if 'class_weight' not in best_params_svm: best_params_svm['class_weight'] = 'balanced'\n",
    "        if 'probability' in best_params_svm: del best_params_svm['probability'] # Use decision_function\n",
    "\n",
    "        fitted_svm_base, calibrator_svm = train_calibrate_model(\n",
    "            base_estimator_class=SVC, # Pass the class\n",
    "            best_params=best_params_svm,\n",
    "            X_train=X_train_scaled,   # Use scaled training data\n",
    "            y_train=y_train,          # Use original y_train for CV indexing\n",
    "            calibration_method=CALIBRATOR,# Use choose calibration method\n",
    "            n_splits=5,               # Folds\n",
    "            random_state=RANDOM_SEED,\n",
    "            score_method='decision_function', # SVC supports decision_function\n",
    "            cvap_loss='log',          # Use log-loss aggregation for CVAP\n",
    "            cvap_precision=None       # Default precision\n",
    "        )\n",
    "        calibration_duration_svm = time.time() - calibration_start_time_svm\n",
    "        if fitted_svm_base and calibrator_svm:\n",
    "            logging.info(f\"--- [{model_name_svm}] Calibration with {CALIBRATOR} finished in {calibration_duration_svm:.2f} seconds ---\")\n",
    "            # Optional: Save models\n",
    "            # joblib.dump(...)\n",
    "        else:\n",
    "            logging.error(f\"[{model_name_svm}] Failed to train base model or the calibration.\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_svm}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "\n",
    "    # --- Section 1.4: Mondrian ICP Calibration ---\n",
    "    fitted_cc_svm = None # Initialize classifier variable\n",
    "    if fitted_svm_base and calibrator_svm:\n",
    "        if not y_cal.empty:\n",
    "            logging.info(f\"--- [{model_name_svm}] Calibrating Mondrian Conformal Prediction ---\")\n",
    "            mcp_cal_start_time_svm = time.time()\n",
    "\n",
    "            # Calculate probabilities needed for crepes on Calibration set\n",
    "            probs_cal_svm = calibrator_svm.predict_proba(X_cal_scaled) # (n_cal, 2)\n",
    "\n",
    "            # Define Mondrian Bins (Class-conditional example)\n",
    "            bins_cal_svm = y_cal.values # Assumes y_cal is pd.Series/np.array\n",
    "\n",
    "            # Fit the Mondrian classifier\n",
    "            fitted_cc_svm = fit_mondrian_classifier(probs_cal_svm, bins_cal=bins_cal_svm)\n",
    "\n",
    "            mcp_cal_duration_svm = time.time() - mcp_cal_start_time_svm\n",
    "            if fitted_cc_svm:\n",
    "                logging.info(f\"--- [{model_name_svm}] Mondrian CP calibration finished in {mcp_cal_duration_svm:.2f} seconds ---\")\n",
    "                # Optional: Save the fitted_cc_svm object\n",
    "                # cc_filename = ...\n",
    "                # joblib.dump(...)\n",
    "            else:\n",
    "                logging.error(f\"[{model_name_svm}] Failed to fit Mondrian classifier.\")\n",
    "        else:\n",
    "            logging.warning(f\"[{model_name_svm}] Calibration set is empty. Skipping Mondrian ICP calibration.\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_svm}] Base model or Calibrator not available. Skipping Mondrian ICP calibration.\")\n",
    "\n",
    "    # --- Section 1.5: Final Evaluation ---\n",
    "    if fitted_svm_base and calibrator_svm: # Check base model availability\n",
    "        logging.info(f\"--- [{model_name_svm}] Final Evaluation on Test Set ---\")\n",
    "        eval_start_time_svm = time.time()\n",
    "\n",
    "        # --- Calculate Base Metrics ---\n",
    "        probs_test_svm_full = calibrator_svm.predict_proba(X_test_scaled) # (n_test, 2)\n",
    "\n",
    "        # Derive probabilities and predictions from the calibrated output\n",
    "        y_proba_test_svm = probs_test_svm_full[:, 1] # Prob positive class\n",
    "        y_pred_test_svm = (y_proba_test_svm >= 0.5).astype(int) # Threshold calibrated probs\n",
    "        metrics_svm = calculate_metrics(y_test, y_pred_test_svm, y_proba_test_svm, model_name=model_name_svm)\n",
    "\n",
    "        # --- Mondrian Conformal Prediction Evaluation ---\n",
    "        cp_coverage_mond_svm, cp_avg_set_size_mond_svm = None, None # Initialize results\n",
    "        class_coverage_dict = None\n",
    "\n",
    "        if fitted_cc_svm is not None: # Check if Mondrian classifier was fitted successfully\n",
    "            mcp_eval_start_time_svm = time.time()\n",
    "            # Define Mondrian Bins for test set (Class-conditional example)\n",
    "            bins_test_svm = y_test.values if not y_test.empty else np.array([])\n",
    "            y_test_true_np = y_test.values if not y_test.empty else np.array([])\n",
    "\n",
    "            # Evaluate the fitted Mondrian classifier\n",
    "            cp_coverage_mond_svm, cp_avg_set_size_mond_svm, _, class_coverage_dict = evaluate_mondrian_prediction(\n",
    "                fitted_cc=fitted_cc_svm,          # Pass the fitted classifier\n",
    "                probs_test=probs_test_svm_full,   # Pass test probabilities (n_test, 2)\n",
    "                y_test_true=y_test_true_np,       # Pass true test labels\n",
    "                bins_test=bins_test_svm,          # Pass test bins\n",
    "                alpha=ALPHA\n",
    "            )\n",
    "            mcp_eval_duration_svm = time.time() - mcp_eval_start_time_svm\n",
    "            logging.info(f\"--- [{model_name_svm}] Mondrian CP evaluation finished in {mcp_eval_duration_svm:.2f} seconds ---\")\n",
    "        else:\n",
    "            logging.warning(f\"[{model_name_svm}] Skipping Mondrian CP evaluation: Classifier not fitted.\")\n",
    "\n",
    "        eval_duration_svm = time.time() - eval_start_time_svm # Total eval time\n",
    "        logging.info(f\"--- [{model_name_svm}] Total Evaluation finished in {eval_duration_svm:.2f} seconds ---\")\n",
    "\n",
    "        # --- Store results (Same as before, using the new variables) ---\n",
    "        all_results = {model_name_svm : {\n",
    "            'metrics': metrics_svm,\n",
    "            'cp_coverage_mond': cp_coverage_mond_svm,           # Store Mondrian coverage\n",
    "            'cp_class_coverage_dict': class_coverage_dict,\n",
    "            'cp_avg_set_size_mond': cp_avg_set_size_mond_svm,     # Store Mondrian avg set size\n",
    "            'best_hpo_params': best_params_svm,\n",
    "            'hpo_f1_score': best_score_hpo_svm,\n",
    "            'hpo_duration_s': hpo_duration_svm,\n",
    "        }}\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_svm}] Skipping final evaluation (Base model or Calibrator not available).\")\n",
    "\n",
    "    # Keep the final logging line:\n",
    "    logging.info(f\"===== Finished Workflow for {model_name_svm} =====\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why Log Loss with CVAP Can Be Overly Conservative (and How Brier Helps)\n",
    "\n",
    "When using **Cross Venn-Abers Predictors (CVAP)** with **log-loss aggregation**, the final probability is chosen to **minimize worst-case regret** under log-loss. This leads to **very conservative predictions**, especially when the individual calibration folds assign wide or low-confidence probability intervals. In imbalanced datasets or weak classifiers (like shallow decision trees), this can push all probabilities close to 0 or 1 — often collapsing to always predicting the majority class.\n",
    "\n",
    "**Brier-score aggregation**, on the other hand, minimizes expected squared error instead of log-loss. This encourages **less extreme probabilities**, making the final predictions more balanced and spread out. As a result, CVAP with Brier aggregation is less likely to ignore the minority class, improving its practical usability when log-loss calibration is too cautious.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for CART (using data fraction)\n",
    "MAX_RESOURCE_CART = 1.0\n",
    "MIN_RESOURCE_CART = 0.1 # Can start with smaller fraction for trees\n",
    "ETA_CART = 3\n",
    "RESOURCE_TYPE_CART = 'data_fraction'\n",
    "model_name_cart = \"CART\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¡MODIFICADO PARA PRUEBA RÁPIDA!\n",
    "MAX_RESOURCE_CART = 1.0\n",
    "MIN_RESOURCE_CART = 0.5 # Aumentado para reducir s_max\n",
    "ETA_CART = 4            # Aumentado para eliminar configuraciones más rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart_hpo(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    MAX_RESOURCE_CART,\n",
    "    MIN_RESOURCE_CART,\n",
    "    ETA_CART,\n",
    "    RESOURCE_TYPE_CART,\n",
    "    RANDOM_SEED,\n",
    "    DecisionTreeClassifier,\n",
    "    randint,\n",
    "    f1_score,\n",
    "    hyperband_hpo,\n",
    "    logging,\n",
    "    model_name_cart\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Hyperband HPO for CART (DecisionTreeClassifier) and return best parameters and best score.\n",
    "    \"\"\"\n",
    "    # --- CART: Define Search Space and HPO Params ---\n",
    "    param_space_cart = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': randint(3, 50),\n",
    "        'min_samples_split': randint(2, 100),\n",
    "        'min_samples_leaf': randint(1, 50),\n",
    "        # class_weight added automatically\n",
    "        # random_state added automatically\n",
    "    }\n",
    "\n",
    "    logging.info(f\"--- [{model_name_cart}] Running Hyperband HPO ---\")\n",
    "    best_params_cart, best_score_hpo_cart = hyperband_hpo(\n",
    "        model_class=DecisionTreeClassifier,\n",
    "        param_space=param_space_cart,\n",
    "        X_train=X_train, # USE UNSCALED DATA\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,     # USE UNSCALED DATA\n",
    "        y_val=y_val,\n",
    "        max_resource=MAX_RESOURCE_CART,\n",
    "        eta=ETA_CART,\n",
    "        resource_type=RESOURCE_TYPE_CART,\n",
    "        min_resource=MIN_RESOURCE_CART,\n",
    "        scoring_func=f1_score,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    return best_params_cart, best_score_hpo_cart\n",
    "\n",
    "def cart_workflow(\n",
    "    X_train, y_train, X_val, y_val, X_cal, y_cal, X_test, y_test,\n",
    "    model_name_cart,\n",
    "    MAX_RESOURCE_CART,\n",
    "    MIN_RESOURCE_CART,\n",
    "    ETA_CART,\n",
    "    RESOURCE_TYPE_CART,\n",
    "    RANDOM_SEED,\n",
    "    CALIBRATOR,\n",
    "    ALPHA,\n",
    "    DecisionTreeClassifier,\n",
    "    randint,\n",
    "    f1_score,\n",
    "    hyperband_hpo,\n",
    "    logging,\n",
    "    best_params_cart=None,  # <-- Allow passing best_params directly\n",
    "    best_score_hpo_cart=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete workflow for CART (DecisionTreeClassifier) model:\n",
    "    - Hyperparameter optimization using Hyperband\n",
    "    - Final model training and calibration\n",
    "    - Mondrian Inductive Conformal Prediction calibration\n",
    "    - Final evaluation on test set (including Mondrian CP evaluation)\n",
    "    - Results are stored in the provided all_results dictionary\n",
    "\n",
    "    All logic is preserved exactly as in the original notebook code.\n",
    "\n",
    "    Args:\n",
    "        X_train, y_train: Training data (unscaled)\n",
    "        X_val, y_val: Validation data (unscaled)\n",
    "        X_cal, y_cal: Calibration data (unscaled)\n",
    "        X_test, y_test: Test data (unscaled)\n",
    "        all_results: dict to store results\n",
    "        model_name_cart: str, name for the model (e.g. \"CART\")\n",
    "        MAX_RESOURCE_CART, MIN_RESOURCE_CART, ETA_CART, RESOURCE_TYPE_CART: HPO settings\n",
    "        RANDOM_SEED: int, random seed for reproducibility\n",
    "        CALIBRATOR: str, calibration method ('cvap', 'platt', etc.)\n",
    "        ALPHA: float, significance level for conformal prediction\n",
    "\n",
    "    Returns:\n",
    "        None (results are stored in all_results)\n",
    "    \"\"\"\n",
    "    logging.info(f\"\\n\\n===== Starting Workflow for {model_name_cart} =====\")\n",
    "    timestamp_cart = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if best_params_cart is None:\n",
    "        hpo_start_time_cart = time.time()\n",
    "        best_params_cart, best_score_hpo_cart = cart_hpo(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            MAX_RESOURCE_CART,\n",
    "            MIN_RESOURCE_CART,\n",
    "            ETA_CART,\n",
    "            RESOURCE_TYPE_CART,\n",
    "            RANDOM_SEED,\n",
    "            DecisionTreeClassifier,\n",
    "            randint,\n",
    "            f1_score,\n",
    "            hyperband_hpo,\n",
    "            logging,\n",
    "            model_name_cart\n",
    "        )\n",
    "        hpo_duration_cart = time.time() - hpo_start_time_cart\n",
    "        logging.info(f\"--- [{model_name_cart}] HPO finished in {hpo_duration_cart:.2f} seconds ---\")\n",
    "    else:\n",
    "        hpo_duration_cart = 0.0\n",
    "        logging.info(f\"--- [{model_name_cart}] Using provided hyperparameters, skipping HPO. ---\")\n",
    "\n",
    "    # --- 2.3 CART: Train Final Model & Calibration (using Full Training Set) ---\n",
    "    fitted_cart_base = None\n",
    "    calibrator_cart = None\n",
    "    if best_params_cart:\n",
    "        logging.info(f\"--- [{model_name_cart}] Training final model and calibrator ---\")\n",
    "        calibration_start_time_cart = time.time()\n",
    "        # Ensure necessary fixed parameters are present for the final fit\n",
    "        best_params_cart['random_state'] = RANDOM_SEED\n",
    "        if 'class_weight' not in best_params_cart: best_params_cart['class_weight'] = 'balanced'\n",
    "\n",
    "        fitted_cart_base, calibrator_cart = train_calibrate_model(\n",
    "            base_estimator_class=DecisionTreeClassifier,\n",
    "            best_params=best_params_cart,\n",
    "            X_train=X_train,  # Use UNSCALED training data\n",
    "            y_train=y_train,\n",
    "            calibration_method=CALIBRATOR,  # Use chosen calibration method (e.g., 'cvap', 'platt', etc.)\n",
    "            n_splits=5,\n",
    "            random_state=RANDOM_SEED,\n",
    "            score_method='predict_proba',  # For CART, use predict_proba\n",
    "            cvap_loss='brier',  # or 'brier' if desired\n",
    "            cvap_precision=None  # or set as needed\n",
    "        )\n",
    "        calibration_duration_cart = time.time() - calibration_start_time_cart\n",
    "        if fitted_cart_base and calibrator_cart:\n",
    "            logging.info(f\"--- [{model_name_cart}] Calibration with {CALIBRATOR} finished in {calibration_duration_cart:.2f} seconds ---\")\n",
    "            # Optional: Save models\n",
    "            # joblib.dump(...)\n",
    "        else:\n",
    "            logging.error(f\"[{model_name_cart}] Failed to train base model or the calibration.\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_cart}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "\n",
    "    # --- 2.4 CART: Mondrian ICP Calibration (using Calibration Set) ---\n",
    "    fitted_cc_cart = None  # Initialize Mondrian classifier variable\n",
    "    if fitted_cart_base and calibrator_cart:\n",
    "        if not y_cal.empty:\n",
    "            logging.info(f\"--- [{model_name_cart}] Calibrating Mondrian Conformal Prediction ---\")\n",
    "            mcp_cal_start_time_cart = time.time()\n",
    "\n",
    "            # Get calibrated probabilities from calibrator\n",
    "            calibrated_probs_cal_cart = calibrator_cart.predict_proba(X_cal)  # (n_cal, 2)\n",
    "\n",
    "            # Define Mondrian Bins (Class-conditional example)\n",
    "            bins_cal_cart = y_cal.values  # Assumes y_cal is pd.Series/np.array\n",
    "\n",
    "            # Fit the Mondrian classifier\n",
    "            fitted_cc_cart = fit_mondrian_classifier(calibrated_probs_cal_cart, bins_cal=bins_cal_cart)\n",
    "\n",
    "            mcp_cal_duration_cart = time.time() - mcp_cal_start_time_cart\n",
    "            if fitted_cc_cart:\n",
    "                logging.info(f\"--- [{model_name_cart}] Mondrian CP calibration finished in {mcp_cal_duration_cart:.2f} seconds ---\")\n",
    "                # Optional: Save the fitted_cc_cart object using joblib\n",
    "                # cc_filename = os.path.join(MODEL_DIR, f\"{model_name_cart}_mondrian_classifier_{timestamp_cart}.joblib\")\n",
    "                # joblib.dump(fitted_cc_cart, cc_filename)\n",
    "                # logging.info(f\"Mondrian classifier saved to {cc_filename}\")\n",
    "            else:\n",
    "                logging.error(f\"[{model_name_cart}] Failed to fit Mondrian classifier.\")\n",
    "        else:\n",
    "            logging.warning(f\"[{model_name_cart}] Calibration set is empty. Skipping Mondrian ICP calibration.\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_cart}] Base model or Calibrator not available. Skipping Mondrian ICP calibration.\")\n",
    "\n",
    "    # --- 2.5 CART: Final Evaluation (using Test Set) ---\n",
    "    if fitted_cart_base and calibrator_cart:\n",
    "        logging.info(f\"--- [{model_name_cart}] Final Evaluation on Test Set ---\")\n",
    "        eval_start_time_cart = time.time()\n",
    "\n",
    "        # Get calibrated probabilities from calibrator\n",
    "        calibrated_probs_test_cart = calibrator_cart.predict_proba(X_test)\n",
    "        y_proba_test_cart = calibrated_probs_test_cart[:, 1]  # Probability of positive class\n",
    "        logging.info(f\"[{model_name_cart}] Calibrated test probabilities for class 1 (sample): {y_proba_test_cart[:20]}\")\n",
    "        logging.info(f\"[{model_name_cart}] Calibrated test probabilities range: min={np.min(y_proba_test_cart):.4f}, max={np.max(y_proba_test_cart):.4f}, mean={np.mean(y_proba_test_cart):.4f}\")\n",
    "        y_pred_test_cart = (y_proba_test_cart >= 0.5).astype(int)  # Threshold calibrated probabilities\n",
    "\n",
    "        metrics_cart = calculate_metrics(y_test, y_pred_test_cart, y_proba_test_cart, model_name=model_name_cart)\n",
    "\n",
    "        # --- Mondrian Conformal Prediction Evaluation ---\n",
    "        cp_coverage_mond_cart, cp_avg_set_size_mond_cart = None, None  # Initialize results\n",
    "\n",
    "        if fitted_cc_cart is not None:  # Check if Mondrian classifier was fitted successfully\n",
    "            mcp_eval_start_time_cart = time.time()\n",
    "            # Define Mondrian Bins for test set (Class-conditional example)\n",
    "            bins_test_cart = y_test.values if not y_test.empty else np.array([])\n",
    "            y_test_true_np_cart = y_test.values if not y_test.empty else np.array([])\n",
    "\n",
    "            # Evaluate the fitted Mondrian classifier\n",
    "            cp_coverage_mond_cart, cp_avg_set_size_mond_cart, _, class_coverage_dict = evaluate_mondrian_prediction(\n",
    "                fitted_cc=fitted_cc_cart,                # Pass the fitted classifier\n",
    "                probs_test=calibrated_probs_test_cart,   # Pass test probabilities (n_test, 2)\n",
    "                y_test_true=y_test_true_np_cart,         # Pass true test labels\n",
    "                bins_test=bins_test_cart,                # Pass test bins\n",
    "                alpha=ALPHA\n",
    "            )\n",
    "            mcp_eval_duration_cart = time.time() - mcp_eval_start_time_cart\n",
    "            logging.info(f\"--- [{model_name_cart}] Mondrian CP evaluation finished in {mcp_eval_duration_cart:.2f} seconds ---\")\n",
    "        else:\n",
    "            logging.warning(f\"[{model_name_cart}] Skipping Mondrian CP evaluation: Classifier not fitted.\")\n",
    "\n",
    "        eval_duration_cart = time.time() - eval_start_time_cart\n",
    "        logging.info(f\"--- [{model_name_cart}] Total Evaluation finished in {eval_duration_cart:.2f} seconds ---\")\n",
    "\n",
    "        # Store results\n",
    "        all_results = {model_name_cart : {\n",
    "            'metrics': metrics_cart,\n",
    "            'cp_coverage_mond': cp_coverage_mond_cart,\n",
    "            'cp_class_coverage_dict': class_coverage_dict,\n",
    "            'cp_avg_set_size_mond': cp_avg_set_size_mond_cart,\n",
    "            'best_hpo_params': best_params_cart,\n",
    "            'hpo_f1_score': best_score_hpo_cart,\n",
    "            'hpo_duration_s': hpo_duration_cart,\n",
    "        }}\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_cart}] Skipping final evaluation (Base model or Calibrator not available).\")\n",
    "\n",
    "    logging.info(f\"===== Finished Workflow for {model_name_cart} =====\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for RF (using iterations)\n",
    "MAX_RESOURCE_RF = 300  # Max n_estimators\n",
    "MIN_RESOURCE_RF = 20   # Min n_estimators\n",
    "ETA_RF = 3\n",
    "RESOURCE_TYPE_RF = 'iterations'\n",
    "model_name_rf = \"Random_Forest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¡MODIFICADO PARA PRUEBA RÁPIDA!\n",
    "MAX_RESOURCE_RF = 20   # ¡Reducido drásticamente! (Antes 300)\n",
    "MIN_RESOURCE_RF = 5    # Mínimo bajo pero cercano a max para pocos brackets\n",
    "ETA_RF = 4             # Aumentado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_hpo(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    MAX_RESOURCE_RF,\n",
    "    MIN_RESOURCE_RF,\n",
    "    ETA_RF,\n",
    "    RESOURCE_TYPE_RF,\n",
    "    RANDOM_SEED,\n",
    "    RandomForestClassifier,\n",
    "    randint,\n",
    "    f1_score,\n",
    "    hyperband_hpo,\n",
    "    logging,\n",
    "    model_name_rf\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Hyperband HPO for Random Forest and return best parameters and best score.\n",
    "    \"\"\"\n",
    "    # --- RF: Define Search Space and HPO Params ---\n",
    "    param_space_rf = {\n",
    "        # n_estimators is controlled by resource\n",
    "        'max_depth': randint(5, 50),\n",
    "        'min_samples_split': randint(2, 50),\n",
    "        'min_samples_leaf': randint(1, 25),\n",
    "        'max_features': ['sqrt', 'log2', None], # None means max_features=n_features\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        # class_weight added automatically\n",
    "        # random_state added automatically\n",
    "    }\n",
    "\n",
    "    logging.info(f\"--- [{model_name_rf}] Running Hyperband HPO ---\")\n",
    "    best_params_rf, best_score_hpo_rf = hyperband_hpo(\n",
    "        model_class=RandomForestClassifier,\n",
    "        param_space=param_space_rf,\n",
    "        X_train=X_train, # USE UNSCALED DATA\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,     # USE UNSCALED DATA\n",
    "        y_val=y_val,\n",
    "        max_resource=MAX_RESOURCE_RF,\n",
    "        eta=ETA_RF,\n",
    "        resource_type=RESOURCE_TYPE_RF,\n",
    "        min_resource=MIN_RESOURCE_RF,\n",
    "        scoring_func=f1_score,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    return best_params_rf, best_score_hpo_rf\n",
    "\n",
    "def random_forest_workflow(\n",
    "    X_train, y_train, X_val, y_val, X_cal, y_cal, X_test, y_test,\n",
    "    model_name_rf,\n",
    "    MAX_RESOURCE_RF,\n",
    "    MIN_RESOURCE_RF,\n",
    "    ETA_RF,\n",
    "    RESOURCE_TYPE_RF,\n",
    "    CALIBRATOR,\n",
    "    ALPHA,\n",
    "    RANDOM_SEED,\n",
    "    RandomForestClassifier,\n",
    "    randint,\n",
    "    f1_score,\n",
    "    hyperband_hpo,\n",
    "    train_calibrate_model,\n",
    "    fit_mondrian_classifier,\n",
    "    evaluate_mondrian_prediction,\n",
    "    calculate_metrics,\n",
    "    logging,\n",
    "    np,\n",
    "    datetime,\n",
    "    best_params_rf=None,  # <-- Allow passing best_params directly\n",
    "    best_score_hpo_rf=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Full Random Forest workflow: HPO, training, calibration, Mondrian conformal prediction, and evaluation.\n",
    "    This function is a direct conversion of the notebook workflow to a callable function.\n",
    "    All logic is preserved exactly as in the original notebook cell.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train, y_train : Training data and labels\n",
    "    X_val, y_val     : Validation data and labels\n",
    "    X_cal, y_cal     : Calibration data and labels\n",
    "    X_test, y_test   : Test data and labels\n",
    "    model_name_rf    : Name of the model (string)\n",
    "    MAX_RESOURCE_RF, MIN_RESOURCE_RF, ETA_RF, RESOURCE_TYPE_RF : HPO params\n",
    "    CALIBRATOR       : Calibration method (e.g., 'cvap', 'platt', etc.)\n",
    "    ALPHA            : Conformal prediction significance level\n",
    "    RANDOM_SEED      : Random seed for reproducibility\n",
    "    RandomForestClassifier : RF class\n",
    "    randint          : Distribution for HPO\n",
    "    f1_score         : F1 scoring function\n",
    "    hyperband_hpo    : Hyperband HPO function\n",
    "    train_calibrate_model : Model training and calibration function\n",
    "    fit_mondrian_classifier : Mondrian conformal classifier fitting function\n",
    "    evaluate_mondrian_prediction : Mondrian conformal prediction evaluation function\n",
    "    calculate_metrics : Function to calculate metrics\n",
    "    logging          : Logging module\n",
    "    np               : Numpy module\n",
    "    datetime         : Datetime module\n",
    "    best_params_rf   : (Optional) If provided, skips HPO and uses these params\n",
    "    best_score_hpo_rf: (Optional) If provided, uses this as HPO score\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None (results are stored in all_results[model_name_rf])\n",
    "    \"\"\"\n",
    "    logging.info(f\"\\n\\n===== Starting Workflow for {model_name_rf} =====\")\n",
    "    timestamp_rf = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if best_params_rf is None:\n",
    "        hpo_start_time_rf = time.time()\n",
    "        best_params_rf, best_score_hpo_rf = rf_hpo(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            MAX_RESOURCE_RF,\n",
    "            MIN_RESOURCE_RF,\n",
    "            ETA_RF,\n",
    "            RESOURCE_TYPE_RF,\n",
    "            RANDOM_SEED,\n",
    "            RandomForestClassifier,\n",
    "            randint,\n",
    "            f1_score,\n",
    "            hyperband_hpo,\n",
    "            logging,\n",
    "            model_name_rf\n",
    "        )\n",
    "        hpo_duration_rf = time.time() - hpo_start_time_rf\n",
    "        logging.info(f\"--- [{model_name_rf}] HPO finished in {hpo_duration_rf:.2f} seconds ---\")\n",
    "    else:\n",
    "        hpo_duration_rf = 0.0\n",
    "        logging.info(f\"--- [{model_name_rf}] Using provided hyperparameters, skipping HPO. ---\")\n",
    "\n",
    "\n",
    "    # --- 3.3 RF: Train Final Model & Calibration (using Full Training Set) ---\n",
    "    fitted_rf_base = None\n",
    "    calibrator_rf = None\n",
    "    if best_params_rf:\n",
    "        logging.info(f\"--- [{model_name_rf}] Training final model and calibrator ---\")\n",
    "        calibration_start_time_rf = time.time()\n",
    "        # Ensure necessary fixed parameters are present for the final fit\n",
    "        best_params_rf['random_state'] = RANDOM_SEED\n",
    "        if 'class_weight' not in best_params_rf: best_params_rf['class_weight'] = 'balanced'\n",
    "        best_params_rf['n_jobs'] = -1 # Use all cores\n",
    "\n",
    "        fitted_rf_base, calibrator_rf = train_calibrate_model(\n",
    "            base_estimator_class=RandomForestClassifier,\n",
    "            best_params=best_params_rf,\n",
    "            X_train=X_train,  # Use UNSCALED training data\n",
    "            y_train=y_train,\n",
    "            calibration_method=CALIBRATOR,  # Use chosen calibration method (e.g., 'cvap', 'platt', etc.)\n",
    "            n_splits=5,\n",
    "            random_state=RANDOM_SEED,\n",
    "            score_method='predict_proba',  # For RF, use predict_proba\n",
    "            cvap_loss='log',  # or 'brier' if desired\n",
    "            cvap_precision=None  # or set as needed\n",
    "        )\n",
    "        calibration_duration_rf = time.time() - calibration_start_time_rf\n",
    "        if fitted_rf_base and calibrator_rf:\n",
    "            logging.info(f\"--- [{model_name_rf}] Calibration with {CALIBRATOR} finished in {calibration_duration_rf:.2f} seconds ---\")\n",
    "            # Optional: Save models\n",
    "            # joblib.dump(...)\n",
    "        else:\n",
    "            logging.error(f\"[{model_name_rf}] Failed to train base model or the calibration.\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_rf}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "\n",
    "    # --- 3.4 RF: Mondrian ICP Calibration (using Calibration Set) ---\n",
    "    fitted_cc_rf = None  # Initialize Mondrian classifier variable\n",
    "    if fitted_rf_base and calibrator_rf:\n",
    "        if not y_cal.empty:\n",
    "            logging.info(f\"--- [{model_name_rf}] Calibrating Mondrian Conformal Prediction ---\")\n",
    "            mcp_cal_start_time_rf = time.time()\n",
    "\n",
    "            # Get calibrated probabilities from calibrator\n",
    "            calibrated_probs_cal_rf = calibrator_rf.predict_proba(X_cal)  # (n_cal, 2)\n",
    "\n",
    "            # Define Mondrian Bins (Class-conditional example)\n",
    "            bins_cal_rf = y_cal.values  # Assumes y_cal is pd.Series/np.array\n",
    "\n",
    "            # Fit the Mondrian classifier\n",
    "            fitted_cc_rf = fit_mondrian_classifier(calibrated_probs_cal_rf, bins_cal=bins_cal_rf)\n",
    "\n",
    "            mcp_cal_duration_rf = time.time() - mcp_cal_start_time_rf\n",
    "            if fitted_cc_rf:\n",
    "                logging.info(f\"--- [{model_name_rf}] Mondrian CP calibration finished in {mcp_cal_duration_rf:.2f} seconds ---\")\n",
    "                # Optional: Save the fitted_cc_rf object using joblib\n",
    "                # cc_filename = os.path.join(MODEL_DIR, f\"{model_name_rf}_mondrian_classifier_{timestamp_rf}.joblib\")\n",
    "                # joblib.dump(fitted_cc_rf, cc_filename)\n",
    "                # logging.info(f\"Mondrian classifier saved to {cc_filename}\")\n",
    "            else:\n",
    "                logging.error(f\"[{model_name_rf}] Failed to fit Mondrian classifier.\")\n",
    "        else:\n",
    "            logging.warning(f\"[{model_name_rf}] Calibration set is empty. Skipping Mondrian ICP calibration.\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_rf}] Base model or Calibrator not available. Skipping Mondrian ICP calibration.\")\n",
    "\n",
    "    # --- 3.5 RF: Final Evaluation (using Test Set) ---\n",
    "    if fitted_rf_base and calibrator_rf:\n",
    "        logging.info(f\"--- [{model_name_rf}] Final Evaluation on Test Set ---\")\n",
    "        eval_start_time_rf = time.time()\n",
    "\n",
    "        # Get calibrated probabilities from calibrator\n",
    "        calibrated_probs_test_rf = calibrator_rf.predict_proba(X_test)\n",
    "        y_proba_test_rf = calibrated_probs_test_rf[:, 1]  # Probability of positive class\n",
    "        y_pred_test_rf = (y_proba_test_rf >= 0.5).astype(int)  # Threshold calibrated probabilities\n",
    "\n",
    "        metrics_rf = calculate_metrics(y_test, y_pred_test_rf, y_proba_test_rf, model_name=model_name_rf)\n",
    "\n",
    "        # --- Mondrian Conformal Prediction Evaluation ---\n",
    "        cp_coverage_mond_rf, cp_avg_set_size_mond_rf = None, None  # Initialize results\n",
    "\n",
    "        if fitted_cc_rf is not None:  # Check if Mondrian classifier was fitted successfully\n",
    "            mcp_eval_start_time_rf = time.time()\n",
    "            # Define Mondrian Bins for test set (Class-conditional example)\n",
    "            bins_test_rf = y_test.values if not y_test.empty else np.array([])\n",
    "            y_test_true_np_rf = y_test.values if not y_test.empty else np.array([])\n",
    "\n",
    "            # Evaluate the fitted Mondrian classifier\n",
    "            cp_coverage_mond_rf, cp_avg_set_size_mond_rf, _, class_coverage_dict = evaluate_mondrian_prediction(\n",
    "                fitted_cc=fitted_cc_rf,                # Pass the fitted classifier\n",
    "                probs_test=calibrated_probs_test_rf,   # Pass test probabilities (n_test, 2)\n",
    "                y_test_true=y_test_true_np_rf,         # Pass true test labels\n",
    "                bins_test=bins_test_rf,                # Pass test bins\n",
    "                alpha=ALPHA\n",
    "            )\n",
    "            mcp_eval_duration_rf = time.time() - mcp_eval_start_time_rf\n",
    "            logging.info(f\"--- [{model_name_rf}] Mondrian CP evaluation finished in {mcp_eval_duration_rf:.2f} seconds ---\")\n",
    "        else:\n",
    "            logging.warning(f\"[{model_name_rf}] Skipping Mondrian CP evaluation: Classifier not fitted.\")\n",
    "\n",
    "        eval_duration_rf = time.time() - eval_start_time_rf  # Total eval time\n",
    "        logging.info(f\"--- [{model_name_rf}] Total Evaluation finished in {eval_duration_rf:.2f} seconds ---\")\n",
    "\n",
    "        # Store results\n",
    "        all_results = {model_name_rf: {\n",
    "            'metrics': metrics_rf,\n",
    "            'cp_coverage_mond': cp_coverage_mond_rf,\n",
    "            'cp_class_coverage_dict': class_coverage_dict,\n",
    "            'cp_avg_set_size_mond': cp_avg_set_size_mond_rf,\n",
    "            'best_hpo_params': best_params_rf,\n",
    "            'hpo_f1_score': best_score_hpo_rf,\n",
    "            'hpo_duration_s': hpo_duration_rf,\n",
    "        }}\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_rf}] Skipping final evaluation (Base model or Calibrator not available).\")\n",
    "\n",
    "    logging.info(f\"===== Finished Workflow for {model_name_rf} =====\")\n",
    "    # Return best hyperparameters for the Random Forest model\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for XGB (using iterations)\n",
    "MAX_RESOURCE_XGB = 500 # Max n_estimators\n",
    "MIN_RESOURCE_XGB = 30  # Min n_estimators\n",
    "ETA_XGB = 3\n",
    "RESOURCE_TYPE_XGB = 'iterations'\n",
    "model_name_xgb = \"XGBoost\"\n",
    "ROUNDS = 20        # Number of rounds to wait for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¡MODIFICADO PARA PRUEBA RÁPIDA!\n",
    "MAX_RESOURCE_XGB = 30  # ¡Reducido drásticamente! (Antes 500)\n",
    "MIN_RESOURCE_XGB = 10  # Mínimo bajo pero cercano a max\n",
    "ETA_XGB = 4            # Aumentado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_hpo(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    MAX_RESOURCE_XGB,\n",
    "    MIN_RESOURCE_XGB,\n",
    "    ETA_XGB,\n",
    "    RESOURCE_TYPE_XGB,\n",
    "    RANDOM_SEED,\n",
    "    xgb,\n",
    "    loguniform,\n",
    "    randint,\n",
    "    uniform,\n",
    "    f1_score,\n",
    "    hyperband_hpo,\n",
    "    logging,\n",
    "    model_name_xgb\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Hyperband HPO for XGBoost and return best parameters and best score.\n",
    "    \"\"\"\n",
    "    # --- XGB: Define Search Space and HPO Params ---\n",
    "    param_space_xgb = {\n",
    "        # n_estimators controlled by resource\n",
    "        'learning_rate': loguniform(0.01, 0.3),\n",
    "        'max_depth': randint(3, 10),\n",
    "        'subsample': uniform(0.6, 0.4), # range [0.6, 1.0)\n",
    "        'colsample_bytree': uniform(0.6, 0.4),\n",
    "        'gamma': loguniform(1e-2, 1.0), # Min loss reduction\n",
    "        'reg_alpha': loguniform(1e-3, 1.0), # L1 reg\n",
    "        'reg_lambda': loguniform(1e-3, 1.0), # L2 reg\n",
    "        # scale_pos_weight added automatically\n",
    "        # random_state added automatically\n",
    "        'objective': ['binary:logistic'], # Fixed objective\n",
    "        'eval_metric': ['logloss'],        # Fixed eval metric for early stopping\n",
    "    }\n",
    "\n",
    "    logging.info(f\"--- [{model_name_xgb}] Running Hyperband HPO ---\")\n",
    "    best_params_xgb, best_score_hpo_xgb = hyperband_hpo(\n",
    "        model_class=xgb.XGBClassifier,\n",
    "        param_space=param_space_xgb,\n",
    "        X_train=X_train, # USE UNSCALED DATA\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,     # USE UNSCALED DATA\n",
    "        y_val=y_val,\n",
    "        max_resource=MAX_RESOURCE_XGB,\n",
    "        eta=ETA_XGB,\n",
    "        resource_type=RESOURCE_TYPE_XGB,\n",
    "        min_resource=MIN_RESOURCE_XGB,\n",
    "        scoring_func=f1_score,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    return best_params_xgb, best_score_hpo_xgb\n",
    "\n",
    "def xgb_workflow(\n",
    "    X_train, y_train, X_val, y_val, X_cal, y_cal, X_test, y_test,\n",
    "    all_results,\n",
    "    model_name_xgb,\n",
    "    MAX_RESOURCE_XGB,\n",
    "    MIN_RESOURCE_XGB,\n",
    "    ETA_XGB,\n",
    "    RESOURCE_TYPE_XGB,\n",
    "    ROUNDS,\n",
    "    CALIBRATOR,\n",
    "    ALPHA,\n",
    "    RANDOM_SEED,\n",
    "    loguniform,\n",
    "    randint,\n",
    "    uniform,\n",
    "    xgb,\n",
    "    hyperband_hpo,\n",
    "    f1_score,\n",
    "    EarlyStopping,\n",
    "    train_calibrate_model,\n",
    "    fit_mondrian_classifier,\n",
    "    evaluate_mondrian_prediction,\n",
    "    calculate_metrics,\n",
    "    logging,\n",
    "    np,\n",
    "    best_params_xgb=None,  # <-- Allow passing best_params directly\n",
    "    best_score_hpo_xgb=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete XGBoost workflow: HPO, calibration, Mondrian conformal prediction, and evaluation.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Defines the XGBoost hyperparameter search space.\n",
    "    2. Runs Hyperband HPO to find the best hyperparameters.\n",
    "    3. Determines the best number of boosting rounds using early stopping.\n",
    "    4. Trains the final XGBoost model and calibrator.\n",
    "    5. Fits a Mondrian conformal predictor on the calibration set.\n",
    "    6. Evaluates the model and conformal predictor on the test set.\n",
    "    7. Stores all results in the provided all_results dictionary.\n",
    "\n",
    "    All logic is preserved exactly as in the original notebook workflow.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train, y_train, X_val, y_val, X_cal, y_cal, X_test, y_test : pd.DataFrame/np.ndarray\n",
    "        Data splits for training, validation, calibration, and testing.\n",
    "    model_name_xgb : str\n",
    "        Name of the model (e.g., \"XGBoost\").\n",
    "    MAX_RESOURCE_XGB, MIN_RESOURCE_XGB, ETA_XGB, RESOURCE_TYPE_XGB, ROUNDS : int/str\n",
    "        Hyperband and early stopping parameters.\n",
    "    CALIBRATOR : str\n",
    "        Calibration method to use.\n",
    "    ALPHA : float\n",
    "        Significance level for conformal prediction.\n",
    "    RANDOM_SEED : int\n",
    "        Random seed for reproducibility.\n",
    "    loguniform, randint, uniform : scipy.stats distributions\n",
    "        Distributions for hyperparameter sampling.\n",
    "    xgb : module\n",
    "        XGBoost module.\n",
    "    hyperband_hpo : function\n",
    "        Function to run Hyperband HPO.\n",
    "    f1_score : function\n",
    "        F1 scoring function.\n",
    "    EarlyStopping : class\n",
    "        XGBoost early stopping callback.\n",
    "    train_calibrate_model : function\n",
    "        Function to train and calibrate the model.\n",
    "    fit_mondrian_classifier : function\n",
    "        Function to fit Mondrian conformal classifier.\n",
    "    evaluate_mondrian_prediction : function\n",
    "        Function to evaluate Mondrian conformal prediction.\n",
    "    calculate_metrics : function\n",
    "        Function to calculate base metrics.\n",
    "    logging : module\n",
    "        Logging module.\n",
    "    np : module\n",
    "        Numpy module.\n",
    "    best_params_xgb : (Optional) If provided, skips HPO and uses these params\n",
    "    best_score_hpo_xgb : (Optional) If provided, uses this as HPO score\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Results are stored in all_results[model_name_xgb].\n",
    "    \"\"\"\n",
    "    logging.info(f\"\\n\\n===== Starting Workflow for {model_name_xgb} =====\")\n",
    "    timestamp_xgb = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if best_params_xgb is None:\n",
    "        hpo_start_time_xgb = time.time()\n",
    "        best_params_xgb, best_score_hpo_xgb = xgb_hpo(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            MAX_RESOURCE_XGB,\n",
    "            MIN_RESOURCE_XGB,\n",
    "            ETA_XGB,\n",
    "            RESOURCE_TYPE_XGB,\n",
    "            RANDOM_SEED,\n",
    "            xgb,\n",
    "            loguniform,\n",
    "            randint,\n",
    "            uniform,\n",
    "            f1_score,\n",
    "            hyperband_hpo,\n",
    "            logging,\n",
    "            model_name_xgb\n",
    "        )\n",
    "        hpo_duration_xgb = time.time() - hpo_start_time_xgb\n",
    "        logging.info(f\"--- [{model_name_xgb}] HPO finished in {hpo_duration_xgb:.2f} seconds ---\")\n",
    "    else:\n",
    "        hpo_duration_xgb = 0.0\n",
    "        logging.info(f\"--- [{model_name_xgb}] Using provided hyperparameters, skipping HPO. ---\")\n",
    "\n",
    "    # --- 4.3 XGB: Train Final Model & Calibration (using Full Training Set) ---\n",
    "    fitted_xgb_base = None\n",
    "    calibrator_xgb = None\n",
    "    final_best_params_xgb = None  # Initialize\n",
    "\n",
    "    if best_params_xgb:\n",
    "        logging.info(f\"--- [{model_name_xgb}] Determining best iteration and training calibrator ---\")\n",
    "        calibration_start_time_xgb = time.time()\n",
    "\n",
    "        # 1. Determine best iteration using early stopping on validation set\n",
    "        temp_best_params_xgb = best_params_xgb.copy()  # Work with a copy\n",
    "        temp_best_params_xgb['random_state'] = RANDOM_SEED\n",
    "        if 'objective' not in temp_best_params_xgb: temp_best_params_xgb['objective'] = 'binary:logistic'\n",
    "        if 'eval_metric' not in temp_best_params_xgb: temp_best_params_xgb['eval_metric'] = 'logloss'\n",
    "        if 'n_jobs' not in temp_best_params_xgb: temp_best_params_xgb['n_jobs'] = -1\n",
    "        if 'scale_pos_weight' not in temp_best_params_xgb:\n",
    "            neg_count = (y_train == 0).sum()\n",
    "            pos_count = (y_train == 1).sum()\n",
    "            if pos_count > 0:\n",
    "                temp_best_params_xgb['scale_pos_weight'] = neg_count / pos_count\n",
    "        # Define callbacks for the fit that determines the best iteration\n",
    "        xgb_final_iteration_callbacks = [\n",
    "                EarlyStopping(rounds=ROUNDS,        # Number of rounds to wait for improvement\n",
    "                                save_best=True,   # Saves the model from the best iteration\n",
    "                                metric_name='logloss', # Explicitly state the metric to monitor (optional but good practice)\n",
    "                                maximize=False)    # We want to minimize logloss\n",
    "            ]\n",
    "\n",
    "        logging.info(\"Training temporary XGBoost with early stopping to find best iteration...\")\n",
    "        temp_xgb_model = xgb.XGBClassifier(**temp_best_params_xgb, callbacks=xgb_final_iteration_callbacks)\n",
    "        eval_set_final = [(X_val, y_val)]\n",
    "        temp_xgb_model.fit(X_train, y_train, eval_set=eval_set_final, verbose=False)\n",
    "\n",
    "        # Retrieve the best iteration\n",
    "        best_iteration = temp_xgb_model.best_iteration\n",
    "        if best_iteration is None or best_iteration <= 0:\n",
    "            logging.warning(f\"Early stopping did not trigger or returned invalid iteration ({best_iteration}). Using max_resource ({MAX_RESOURCE_XGB}) as n_estimators.\")\n",
    "            best_iteration = MAX_RESOURCE_XGB\n",
    "        logging.info(f\"Best iteration found: {best_iteration}\")\n",
    "\n",
    "        # Update best_params with the optimal number of estimators found\n",
    "        final_best_params_xgb = temp_best_params_xgb.copy()\n",
    "        final_best_params_xgb['n_estimators'] = best_iteration\n",
    "\n",
    "        # 2. Train final model and calibrator using train_calibrate_model (like CART)\n",
    "        logging.info(f\"--- [{model_name_xgb}] Training final model ({final_best_params_xgb['n_estimators']} est.) and calibrator ---\")\n",
    "        fitted_xgb_base, calibrator_xgb = train_calibrate_model(\n",
    "            base_estimator_class=xgb.XGBClassifier,\n",
    "            best_params=final_best_params_xgb,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            calibration_method=CALIBRATOR,  # Use chosen calibration method (e.g., 'cvap', 'platt', etc.)\n",
    "            n_splits=5,\n",
    "            random_state=RANDOM_SEED,\n",
    "            score_method='raw_margin_xgb',  # For XGB, use raw margin for Platt/CVAP\n",
    "            cvap_loss='log',                # or 'brier' if desired\n",
    "            cvap_precision=None             # or set as needed\n",
    "        )\n",
    "        calibration_duration_xgb = time.time() - calibration_start_time_xgb\n",
    "        if fitted_xgb_base and calibrator_xgb:\n",
    "            logging.info(f\"--- [{model_name_xgb}] Calibration with {CALIBRATOR} finished in {calibration_duration_xgb:.2f} seconds ---\")\n",
    "            # Optional: Save models\n",
    "            # joblib.dump(...)\n",
    "        else:\n",
    "            logging.error(f\"[{model_name_xgb}] Failed to train base model or the calibration.\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_xgb}] HPO did not find best parameters. Skipping subsequent steps.\")\n",
    "\n",
    "    # --- 4.4 XGB: Mondrian ICP Calibration (using Calibration Set) ---\n",
    "    fitted_cc_xgb = None  # Initialize Mondrian classifier variable\n",
    "    if fitted_xgb_base and calibrator_xgb:\n",
    "        if not y_cal.empty:\n",
    "            logging.info(f\"--- [{model_name_xgb}] Calibrating Mondrian Conformal Prediction ---\")\n",
    "            mcp_cal_start_time_xgb = time.time()\n",
    "\n",
    "            # Calculate probabilities needed for Mondrian ICP on Calibration set\n",
    "            probs_cal_xgb = calibrator_xgb.predict_proba(X_cal)  # Calibrated probs for BOTH classes\n",
    "\n",
    "            # Define Mondrian Bins (Class-conditional example)\n",
    "            bins_cal_xgb = y_cal.values  # Assumes y_cal is pd.Series/np.array\n",
    "\n",
    "            # Fit the Mondrian classifier\n",
    "            fitted_cc_xgb = fit_mondrian_classifier(probs_cal_xgb, bins_cal=bins_cal_xgb)\n",
    "\n",
    "            mcp_cal_duration_xgb = time.time() - mcp_cal_start_time_xgb\n",
    "            if fitted_cc_xgb:\n",
    "                logging.info(f\"--- [{model_name_xgb}] Mondrian CP calibration finished in {mcp_cal_duration_xgb:.2f} seconds ---\")\n",
    "                # Optional: Save the fitted_cc_xgb object using joblib alongside the base model and calibrator\n",
    "                # cc_filename = os.path.join(MODEL_DIR, f\"{model_name_xgb}_mondrian_classifier_{timestamp_xgb}.joblib\")\n",
    "                # joblib.dump(fitted_cc_xgb, cc_filename)\n",
    "                # logging.info(f\"Mondrian classifier saved to {cc_filename}\")\n",
    "            else:\n",
    "                logging.error(f\"[{model_name_xgb}] Failed to fit Mondrian classifier.\")\n",
    "        else:\n",
    "            logging.warning(f\"[{model_name_xgb}] Calibration set is empty. Skipping Mondrian ICP calibration.\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_xgb}] Base model or Calibrator not available. Skipping Mondrian ICP calibration.\")\n",
    "\n",
    "    # --- 4.5 XGB: Final Evaluation (using Test Set) ---\n",
    "    if fitted_xgb_base and calibrator_xgb:\n",
    "        logging.info(f\"--- [{model_name_xgb}] Final Evaluation on Test Set ---\")\n",
    "        eval_start_time_xgb = time.time()\n",
    "\n",
    "        # --- Calculate Base Metrics (Same as before) ---\n",
    "        probs_test_xgb_full = calibrator_xgb.predict_proba(X_test)  # Calibrated probs for BOTH classes\n",
    "        y_proba_test_xgb = probs_test_xgb_full[:, 1]\n",
    "        y_pred_test_xgb = (y_proba_test_xgb >= 0.5).astype(int)\n",
    "        metrics_xgb = calculate_metrics(y_test, y_pred_test_xgb, y_proba_test_xgb, model_name=model_name_xgb)\n",
    "\n",
    "        # --- Mondrian Conformal Prediction Evaluation ---\n",
    "        cp_coverage_mond_xgb, cp_avg_set_size_mond_xgb = None, None  # Initialize results\n",
    "\n",
    "        if fitted_cc_xgb is not None:  # Check if Mondrian classifier was fitted successfully\n",
    "            mcp_eval_start_time_xgb = time.time()\n",
    "            # Define Mondrian Bins for test set (Class-conditional example)\n",
    "            bins_test_xgb = y_test.values if not y_test.empty else np.array([])\n",
    "            y_test_true_np = y_test.values if not y_test.empty else np.array([])\n",
    "\n",
    "            # Evaluate the fitted Mondrian classifier\n",
    "            cp_coverage_mond_xgb, cp_avg_set_size_mond_xgb, _, class_coverage_dict = evaluate_mondrian_prediction(\n",
    "                fitted_cc=fitted_cc_xgb,            # Pass the fitted classifier\n",
    "                probs_test=probs_test_xgb_full,     # Pass test probabilities (n_test, 2)\n",
    "                y_test_true=y_test_true_np,         # Pass true test labels\n",
    "                bins_test=bins_test_xgb,            # Pass test bins\n",
    "                alpha=ALPHA\n",
    "            )\n",
    "            mcp_eval_duration_xgb = time.time() - mcp_eval_start_time_xgb\n",
    "            logging.info(f\"--- [{model_name_xgb}] Mondrian CP evaluation finished in {mcp_eval_duration_xgb:.2f} seconds ---\")\n",
    "        else:\n",
    "            logging.warning(f\"[{model_name_xgb}] Skipping Mondrian CP evaluation: Classifier not fitted.\")\n",
    "\n",
    "        eval_duration_xgb = time.time() - eval_start_time_xgb  # Total eval time\n",
    "        logging.info(f\"--- [{model_name_xgb}] Total Evaluation finished in {eval_duration_xgb:.2f} seconds ---\")\n",
    "\n",
    "        # --- Store results (Same as before, using the new variables) ---\n",
    "        all_results = {model_name_xgb : {\n",
    "            'metrics': metrics_xgb,\n",
    "            'cp_coverage_mond': cp_coverage_mond_xgb,           # Store Mondrian coverage\n",
    "            'cp_class_coverage_dict': class_coverage_dict,\n",
    "            'cp_avg_set_size_mond': cp_avg_set_size_mond_xgb,   # Store Mondrian avg set size\n",
    "            'best_hpo_params': best_params_xgb,\n",
    "            'final_n_estimators': final_best_params_xgb.get('n_estimators', None) if final_best_params_xgb else None,\n",
    "            'hpo_f1_score': best_score_hpo_xgb,\n",
    "            'hpo_duration_s': hpo_duration_xgb,\n",
    "        }}\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_xgb}] Skipping final evaluation (Base model or Calibrator not available).\")\n",
    "\n",
    "    logging.info(f\"===== Finished Workflow for {model_name_xgb} =====\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO Settings for LGBM (using iterations)\n",
    "MAX_RESOURCE_LGBM = 500 # Max n_estimators\n",
    "MIN_RESOURCE_LGBM = 30  # Min n_estimators\n",
    "ETA_LGBM = 3\n",
    "RESOURCE_TYPE_LGBM = 'iterations'\n",
    "model_name_lgbm = \"LightGBM\"\n",
    "ROUNDS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¡MODIFICADO PARA PRUEBA RÁPIDA!\n",
    "MAX_RESOURCE_LGBM = 30  # ¡Reducido drásticamente! (Antes 500)\n",
    "MIN_RESOURCE_LGBM = 10  # Mínimo bajo pero cercano a max\n",
    "ETA_LGBM = 4            # Aumentado\n",
    "RESOURCE_TYPE_LGBM = 'iterations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_hpo(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    MAX_RESOURCE_LGBM,\n",
    "    MIN_RESOURCE_LGBM,\n",
    "    ETA_LGBM,\n",
    "    RESOURCE_TYPE_LGBM,\n",
    "    RANDOM_SEED,\n",
    "    lgb,\n",
    "    loguniform,\n",
    "    randint,\n",
    "    uniform,\n",
    "    f1_score,\n",
    "    hyperband_hpo,\n",
    "    logging,\n",
    "    model_name_lgb\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Hyperband HPO for LightGBM and return best parameters and best score.\n",
    "    \"\"\"\n",
    "    # --- LGBM: Define Search Space and HPO Params ---\n",
    "    param_space_lgbm = {\n",
    "        # n_estimators controlled by resource\n",
    "        'learning_rate': loguniform(0.01, 0.3),\n",
    "        'num_leaves': randint(20, 100),\n",
    "        'max_depth': randint(3, 15), # Often kept lower than XGB depth\n",
    "        'subsample': uniform(0.6, 0.4), # Aliased as bagging_fraction\n",
    "        'colsample_bytree': uniform(0.6, 0.4), # Aliased as feature_fraction\n",
    "        'reg_alpha': loguniform(1e-3, 1.0), # L1\n",
    "        'reg_lambda': loguniform(1e-3, 1.0), # L2\n",
    "        # scale_pos_weight or is_unbalance=True added automatically\n",
    "        # random_state added automatically\n",
    "        'objective': ['binary'], # Fixed objective\n",
    "        'metric': ['logloss'],   # Fixed metric for early stopping\n",
    "        'verbose': [-1]          # Suppress LightGBM's internal verbosity if desired\n",
    "    }\n",
    "\n",
    "    logging.info(f\"--- [{model_name_lgbm}] Running Hyperband HPO ---\")\n",
    "    best_params_lgbm, best_score_hpo_lgbm = hyperband_hpo(\n",
    "        model_class=lgb.LGBMClassifier,\n",
    "        param_space=param_space_lgbm,\n",
    "        X_train=X_train, # USE UNSCALED DATA\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,     # USE UNSCALED DATA\n",
    "        y_val=y_val,\n",
    "        max_resource=MAX_RESOURCE_LGBM,\n",
    "        eta=ETA_LGBM,\n",
    "        resource_type=RESOURCE_TYPE_LGBM,\n",
    "        min_resource=MIN_RESOURCE_LGBM,\n",
    "        scoring_func=f1_score,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    return best_params_lgbm, best_score_hpo_lgbm\n",
    "\n",
    "def lgbm_workflow(\n",
    "    X_train, y_train, X_val, y_val, X_cal, y_cal, X_test, y_test,\n",
    "    model_name_lgbm,\n",
    "    MAX_RESOURCE_LGBM,\n",
    "    MIN_RESOURCE_LGBM,\n",
    "    ETA_LGBM,\n",
    "    RESOURCE_TYPE_LGBM,\n",
    "    ROUNDS,\n",
    "    CALIBRATOR,\n",
    "    ALPHA,\n",
    "    RANDOM_SEED,\n",
    "    lgb,\n",
    "    loguniform,\n",
    "    randint,\n",
    "    uniform,\n",
    "    f1_score,\n",
    "    hyperband_hpo,\n",
    "    early_stopping,\n",
    "    train_calibrate_model,\n",
    "    fit_mondrian_classifier,\n",
    "    evaluate_mondrian_prediction,\n",
    "    calculate_metrics,\n",
    "    logging,\n",
    "    np,\n",
    "    datetime,\n",
    "    best_params_lgbm=None,  # <-- Allow passing best_params directly\n",
    "    best_score_hpo_lgbm=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Full LightGBM workflow: HPO, training, calibration, Mondrian conformal prediction, and evaluation.\n",
    "    This function is a direct conversion of the notebook workflow to a callable function.\n",
    "    All logic is preserved exactly as in the original notebook cell.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train, y_train : Training data and labels\n",
    "    X_val, y_val     : Validation data and labels\n",
    "    X_cal, y_cal     : Calibration data and labels\n",
    "    X_test, y_test   : Test data and labels\n",
    "    model_name_lgbm  : Name of the model (string)\n",
    "    MAX_RESOURCE_LGBM, MIN_RESOURCE_LGBM, ETA_LGBM, RESOURCE_TYPE_LGBM, ROUNDS : HPO and early stopping params\n",
    "    CALIBRATOR       : Calibration method (e.g., 'cvap', 'platt', etc.)\n",
    "    ALPHA            : Conformal prediction significance level\n",
    "    RANDOM_SEED      : Random seed for reproducibility\n",
    "    lgb              : LightGBM module\n",
    "    loguniform, randint, uniform : Distributions for HPO\n",
    "    f1_score         : F1 scoring function\n",
    "    hyperband_hpo    : Hyperband HPO function\n",
    "    early_stopping   : LightGBM early stopping callback\n",
    "    train_calibrate_model : Model training and calibration function\n",
    "    fit_mondrian_classifier : Mondrian conformal classifier fitting function\n",
    "    evaluate_mondrian_prediction : Mondrian conformal prediction evaluation function\n",
    "    calculate_metrics : Function to calculate metrics\n",
    "    logging          : Logging module\n",
    "    np               : Numpy module\n",
    "    datetime         : Datetime module\n",
    "    best_params_lgbm : (Optional) If provided, skips HPO and uses these params\n",
    "    best_score_hpo_lgbm : (Optional) If provided, uses this as HPO score\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None (results are stored in all_results[model_name_lgbm])\n",
    "    \"\"\"\n",
    "    logging.info(f\"\\n\\n===== Starting Workflow for {model_name_lgbm} =====\")\n",
    "    timestamp_lgbm = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if best_params_lgbm is None:\n",
    "        hpo_start_time_lgbm = time.time()\n",
    "        best_params_lgbm, best_score_hpo_lgbm = lgbm_hpo(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            MAX_RESOURCE_LGBM,\n",
    "            MIN_RESOURCE_LGBM,\n",
    "            ETA_LGBM,\n",
    "            RESOURCE_TYPE_LGBM,\n",
    "            RANDOM_SEED,\n",
    "            lgb,\n",
    "            loguniform,\n",
    "            randint,\n",
    "            uniform,\n",
    "            f1_score,\n",
    "            hyperband_hpo,\n",
    "            logging,\n",
    "            model_name_lgbm\n",
    "        )\n",
    "        hpo_duration_lgbm = time.time() - hpo_start_time_lgbm\n",
    "        logging.info(f\"--- [{model_name_lgbm}] HPO finished in {hpo_duration_lgbm:.2f} seconds ---\")\n",
    "    else:\n",
    "        hpo_duration_lgbm = 0.0\n",
    "        logging.info(f\"--- [{model_name_lgbm}] Using provided hyperparameters, skipping HPO. ---\")\n",
    "\n",
    "    # --- 5.3 LGBM: Train Final Model & Calibration (using Full Training Set) ---\n",
    "    fitted_lgbm_base = None\n",
    "    calibrator_lgbm = None\n",
    "    final_best_params_lgbm = None  # Initialize\n",
    "\n",
    "    if best_params_lgbm:\n",
    "        logging.info(f\"--- [{model_name_lgbm}] Determining best iteration and training calibrator ---\")\n",
    "        calibration_start_time_lgbm = time.time()\n",
    "\n",
    "        # 1. Determine best iteration using early stopping on validation set\n",
    "        temp_best_params_lgbm = best_params_lgbm.copy()  # Work with a copy\n",
    "        temp_best_params_lgbm['random_state'] = RANDOM_SEED\n",
    "        if 'objective' not in temp_best_params_lgbm: temp_best_params_lgbm['objective'] = 'binary'\n",
    "        if 'metric' not in temp_best_params_lgbm: temp_best_params_lgbm['metric'] = 'logloss'\n",
    "        if 'n_jobs' not in temp_best_params_lgbm: temp_best_params_lgbm['n_jobs'] = -1\n",
    "        if 'verbose' not in temp_best_params_lgbm: temp_best_params_lgbm['verbose'] = -1  # Control verbosity\n",
    "        if 'scale_pos_weight' not in temp_best_params_lgbm:\n",
    "            neg_count = (y_train == 0).sum()\n",
    "            pos_count = (y_train == 1).sum()\n",
    "            if pos_count > 0:\n",
    "                temp_best_params_lgbm['scale_pos_weight'] = neg_count / pos_count\n",
    "                if 'is_unbalance' in temp_best_params_lgbm: del temp_best_params_lgbm['is_unbalance']\n",
    "            elif 'is_unbalance' not in temp_best_params_lgbm:\n",
    "                temp_best_params_lgbm['is_unbalance'] = True\n",
    "\n",
    "        # Define callbacks for the fit that determines the best iteration\n",
    "        callbacks_final = [\n",
    "            early_stopping(stopping_rounds=ROUNDS, verbose=False)\n",
    "        ]\n",
    "        metric_to_monitor = 'logloss'\n",
    "\n",
    "        logging.info(f\"Training temporary LightGBM with early stopping (monitoring '{metric_to_monitor}') to find best iteration...\")\n",
    "        temp_lgbm_model = lgb.LGBMClassifier(**temp_best_params_lgbm)\n",
    "        eval_set_final_lgbm = [(X_val, y_val)]\n",
    "\n",
    "        # Check if eval_set is valid before fitting\n",
    "        if not eval_set_final_lgbm or not isinstance(eval_set_final_lgbm, list) or not eval_set_final_lgbm[0]:\n",
    "            raise ValueError(\"eval_set_final_lgbm is not correctly defined before fitting.\")\n",
    "        if len(eval_set_final_lgbm[0]) != 2:\n",
    "            raise ValueError(\"Each element in eval_set must be a tuple (X, y).\")\n",
    "\n",
    "        try:\n",
    "            temp_lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set_final_lgbm,\n",
    "                eval_metric=metric_to_monitor,\n",
    "                callbacks=callbacks_final\n",
    "            )\n",
    "\n",
    "            # Retrieve the best iteration\n",
    "            best_iteration_lgbm = temp_lgbm_model.best_iteration_\n",
    "            if best_iteration_lgbm is None or best_iteration_lgbm <= 0:\n",
    "                logging.warning(f\"Early stopping did not trigger or returned invalid iteration ({best_iteration_lgbm}). Using max_resource ({MAX_RESOURCE_LGBM}) as n_estimators.\")\n",
    "                best_iteration_lgbm = MAX_RESOURCE_LGBM\n",
    "            logging.info(f\"Best iteration found: {best_iteration_lgbm}\")\n",
    "\n",
    "            # Update best_params with the optimal number of estimators found\n",
    "            final_best_params_lgbm = temp_best_params_lgbm.copy()\n",
    "            final_best_params_lgbm['n_estimators'] = best_iteration_lgbm\n",
    "\n",
    "            # 2. Train final model and calibrator using train_calibrate_model (like XGB)\n",
    "            logging.info(f\"--- [{model_name_lgbm}] Training final model ({final_best_params_lgbm['n_estimators']} est.) and calibrator ---\")\n",
    "            fitted_lgbm_base, calibrator_lgbm = train_calibrate_model(\n",
    "                base_estimator_class=lgb.LGBMClassifier,\n",
    "                best_params=final_best_params_lgbm,\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                calibration_method=CALIBRATOR,  # Use chosen calibration method (e.g., 'cvap', 'platt', etc.)\n",
    "                n_splits=5,\n",
    "                random_state=RANDOM_SEED,\n",
    "                score_method='raw_score_lgbm',  # Specify score method for LGBM if needed\n",
    "                cvap_loss='log',                # or 'brier' if desired\n",
    "                cvap_precision=None             # or set as needed\n",
    "            )\n",
    "            calibration_duration_lgbm = time.time() - calibration_start_time_lgbm\n",
    "            if fitted_lgbm_base and calibrator_lgbm:\n",
    "                logging.info(f\"--- [{model_name_lgbm}] Calibration finished in {calibration_duration_lgbm:.2f} seconds ---\")\n",
    "                # Optional: Save models\n",
    "                # joblib.dump(...)\n",
    "            else:\n",
    "                logging.error(f\"[{model_name_lgbm}] Failed to train base model or calibrator.\")\n",
    "        except ValueError as ve:\n",
    "            logging.error(f\"ValueError during temp_lgbm_model.fit: {ve}\")\n",
    "            logging.error(f\"Shapes: X_train={X_train.shape}, y_train={y_train.shape}, X_val={X_val.shape}, y_val={y_val.shape}\")\n",
    "            raise\n",
    "\n",
    "    # --- 5.4 LGBM: Mondrian ICP Calibration ---\n",
    "    fitted_cc_lgbm = None  # Initialize Mondrian classifier variable\n",
    "    if fitted_lgbm_base and calibrator_lgbm:\n",
    "        if not y_cal.empty:\n",
    "            logging.info(f\"--- [{model_name_lgbm}] Calibrating Mondrian Conformal Prediction ---\")\n",
    "            mcp_cal_start_time_lgbm = time.time()\n",
    "\n",
    "            # Calculate probabilities needed for Mondrian ICP on Calibration set\n",
    "            probs_cal_lgbm = calibrator_lgbm.predict_proba(X_cal)  # Calibrated probs for BOTH classes\n",
    "\n",
    "            # Define Mondrian Bins (Class-conditional example)\n",
    "            bins_cal_lgbm = y_cal.values  # Assumes y_cal is pd.Series/np.array\n",
    "\n",
    "            # Fit the Mondrian classifier\n",
    "            fitted_cc_lgbm = fit_mondrian_classifier(probs_cal_lgbm, bins_cal=bins_cal_lgbm)\n",
    "\n",
    "            mcp_cal_duration_lgbm = time.time() - mcp_cal_start_time_lgbm\n",
    "            if fitted_cc_lgbm:\n",
    "                logging.info(f\"--- [{model_name_lgbm}] Mondrian CP calibration finished in {mcp_cal_duration_lgbm:.2f} seconds ---\")\n",
    "                # Optional: Save the fitted_cc_lgbm object using joblib alongside the base model and calibrator\n",
    "                # cc_filename = os.path.join(MODEL_DIR, f\"{model_name_lgbm}_mondrian_classifier_{timestamp_lgbm}.joblib\")\n",
    "                # joblib.dump(fitted_cc_lgbm, cc_filename)\n",
    "                # logging.info(f\"Mondrian classifier saved to {cc_filename}\")\n",
    "            else:\n",
    "                logging.error(f\"[{model_name_lgbm}] Failed to fit Mondrian classifier.\")\n",
    "        else:\n",
    "            logging.warning(f\"[{model_name_lgbm}] Calibration set is empty. Skipping Mondrian ICP calibration.\")\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_lgbm}] Base model or Calibrator not available. Skipping Mondrian ICP calibration.\")\n",
    "\n",
    "    # --- 5.5 LGBM: Final Evaluation ---\n",
    "    if fitted_lgbm_base and calibrator_lgbm:\n",
    "        logging.info(f\"--- [{model_name_lgbm}] Final Evaluation on Test Set ---\")\n",
    "        eval_start_time_lgbm = time.time()\n",
    "\n",
    "        # --- Calculate Base Metrics (Same as before) ---\n",
    "        probs_test_lgbm_full = calibrator_lgbm.predict_proba(X_test)  # Calibrated probs for BOTH classes\n",
    "        y_proba_test_lgbm = probs_test_lgbm_full[:, 1]\n",
    "        y_pred_test_lgbm = (y_proba_test_lgbm >= 0.5).astype(int)\n",
    "        metrics_lgbm = calculate_metrics(y_test, y_pred_test_lgbm, y_proba_test_lgbm, model_name=model_name_lgbm)\n",
    "\n",
    "        # --- Mondrian Conformal Prediction Evaluation ---\n",
    "        cp_coverage_mond_lgbm, cp_avg_set_size_mond_lgbm = None, None  # Initialize results\n",
    "\n",
    "        if fitted_cc_lgbm is not None:  # Check if Mondrian classifier was fitted successfully\n",
    "            mcp_eval_start_time_lgbm = time.time()\n",
    "            # Define Mondrian Bins for test set (Class-conditional example)\n",
    "            bins_test_lgbm = y_test.values if not y_test.empty else np.array([])\n",
    "            y_test_true_np = y_test.values if not y_test.empty else np.array([])\n",
    "\n",
    "            # Evaluate the fitted Mondrian classifier\n",
    "            cp_coverage_mond_lgbm, cp_avg_set_size_mond_lgbm, _, class_coverage_dict = evaluate_mondrian_prediction(\n",
    "                fitted_cc=fitted_cc_lgbm,            # Pass the fitted classifier\n",
    "                probs_test=probs_test_lgbm_full,     # Pass test probabilities (n_test, 2)\n",
    "                y_test_true=y_test_true_np,          # Pass true test labels\n",
    "                bins_test=bins_test_lgbm,            # Pass test bins\n",
    "                alpha=ALPHA\n",
    "            )\n",
    "            mcp_eval_duration_lgbm = time.time() - mcp_eval_start_time_lgbm\n",
    "            logging.info(f\"--- [{model_name_lgbm}] Mondrian CP evaluation finished in {mcp_eval_duration_lgbm:.2f} seconds ---\")\n",
    "        else:\n",
    "            logging.warning(f\"[{model_name_lgbm}] Skipping Mondrian CP evaluation: Classifier not fitted.\")\n",
    "\n",
    "        eval_duration_lgbm = time.time() - eval_start_time_lgbm  # Total eval time\n",
    "        logging.info(f\"--- [{model_name_lgbm}] Total Evaluation finished in {eval_duration_lgbm:.2f} seconds ---\")\n",
    "\n",
    "        # --- Store results (Same as before, using the new variables) ---\n",
    "        all_results = {model_name_lgbm : {\n",
    "            'metrics': metrics_lgbm,\n",
    "            'cp_coverage_mond': cp_coverage_mond_lgbm,           # Store Mondrian coverage\n",
    "            'cp_class_coverage_dict': class_coverage_dict,\n",
    "            'cp_avg_set_size_mond': cp_avg_set_size_mond_lgbm,   # Store Mondrian avg set size\n",
    "            'best_hpo_params': best_params_lgbm, # Original HPO params\n",
    "            # Store actual used estimators if available\n",
    "            'final_n_estimators': final_best_params_lgbm.get('n_estimators', None) if final_best_params_lgbm else None,\n",
    "            'hpo_f1_score': best_score_hpo_lgbm,\n",
    "            'hpo_duration_s': hpo_duration_lgbm,\n",
    "        }}\n",
    "    else:\n",
    "        logging.warning(f\"[{model_name_lgbm}] Skipping final evaluation (Base model or Calibrator not available).\")\n",
    "    logging.info(f\"===== Finished Workflow for {model_name_lgbm} =====\")\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running HPO:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting feature set group 'group_1' with 11 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Bracket s=0 (n=1, r0=1.00): 100%|██████████| 1/1 [00:08<00:00,  8.43s/it, Best F1: 0.5257]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Bracket s=0 (n=1, r0=1.00): 100%|██████████| 1/1 [00:00<00:00,  4.54it/s, Best F1: 0.3817]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Bracket s=0 (n=2, r0=20.00): 100%|██████████| 2/2 [00:08<00:00,  4.16s/it, Best F1: 0.6610]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Bracket s=0 (n=1, r0=30.00): 100%|██████████| 1/1 [00:00<00:00,  7.01it/s, Best F1: 0.5339]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Bracket s=0 (n=1, r0=30.00): 100%|██████████| 1/1 [00:00<00:00,  8.97it/s, Best F1: 0.6061]\n",
      "Running HPO:  50%|█████     | 1/2 [00:17<00:17, 17.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting feature set group 'group_2' with 25 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Bracket s=0 (n=1, r0=1.00): 100%|██████████| 1/1 [00:12<00:00, 13.00s/it, Best F1: 0.3018]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Bracket s=0 (n=1, r0=1.00): 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, Best F1: 0.3863]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Bracket s=0 (n=2, r0=20.00): 100%|██████████| 2/2 [00:21<00:00, 10.52s/it, Best F1: 0.6389]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Bracket s=0 (n=1, r0=30.00): 100%|██████████| 1/1 [00:00<00:00,  4.94it/s, Best F1: 0.3857]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Bracket s=0 (n=1, r0=30.00): 100%|██████████| 1/1 [00:00<00:00,  6.35it/s, Best F1: 0.5409]\n",
      "Running HPO: 100%|██████████| 2/2 [00:52<00:00, 26.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HPO loop finished. Best parameters saved in 'best_params' directory.\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "# HPO Loop - Find and Save Best Hyperparameters #\n",
    "#################################################\n",
    "\n",
    "# --- Configuration ---\n",
    "BEST_PARAMS_DIR = \"best_params\"\n",
    "os.makedirs(BEST_PARAMS_DIR, exist_ok=True)\n",
    "\n",
    "# List of feature groups to process (keys from the 'groups' dictionary)\n",
    "# Adjust this list if you only want to run specific groups\n",
    "GROUP_NAMES_TO_PROCESS = list(groups.keys()) # Process all defined groups\n",
    "GROUP_NAMES_TO_PROCESS = ['group_1', 'group_2'] # Example: Process only specific groups\n",
    "\n",
    "# Ensure necessary variables are defined in the global scope before running this cell:\n",
    "# groups, df, TARGET_COLUMN, clean_data, split_data, apply_feature_scaling,\n",
    "# MAX_RESOURCE_*, MIN_RESOURCE_*, ETA_*, RESOURCE_TYPE_*, RANDOM_SEED, MODEL_DIR,\n",
    "# SVC, loguniform, hyperband_hpo, f1_score, model_name_svm,\n",
    "# DecisionTreeClassifier, randint, model_name_cart,\n",
    "# RandomForestClassifier, model_name_rf,\n",
    "# xgb, uniform, model_name_xgb,\n",
    "# lgb, model_name_lgbm,\n",
    "# svm_hpo, cart_hpo, rf_hpo, xgb_hpo, lgbm_hpo\n",
    "\n",
    "# Dictionary to store best params found (optional, mainly for logging/debugging here)\n",
    "all_best_params_found = {}\n",
    "\n",
    "logging.info(f\"Starting HPO loop for groups: {GROUP_NAMES_TO_PROCESS}\")\n",
    "hpo_loop_start_time = time.time()\n",
    "\n",
    "# Use tqdm for the outer loop to show progress over groups\n",
    "for group_name in tqdm(GROUP_NAMES_TO_PROCESS, desc=\"Running HPO\"):\n",
    "    logging.info(f\"\\n{'='*30} Running HPO for Group: {group_name} {'='*30}\")\n",
    "    group_hpo_start_time = time.time()\n",
    "    group_best_params = {} # Store best params for the current group\n",
    "\n",
    "    try:\n",
    "        # --- Data Preparation ---\n",
    "        logging.info(f\"[{group_name}] Cleaning data...\")\n",
    "        X, y, df_clean = clean_data(df, group_name, TARGET_COLUMN, logger=logging)\n",
    "        if X.empty or y.empty:\n",
    "            logging.warning(f\"[{group_name}] Skipping HPO due to insufficient data after cleaning.\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"[{group_name}] Splitting data (Train/Val only for HPO)...\")\n",
    "        # Only need train/val splits for Hyperparameter Optimization\n",
    "        X_train, y_train, X_val, y_val, _, _, _, _ = split_data(X, y)\n",
    "\n",
    "        if X_train.empty or y_train.empty or X_val.empty or y_val.empty:\n",
    "             logging.warning(f\"[{group_name}] Skipping HPO due to empty train or validation set after splitting.\")\n",
    "             continue\n",
    "\n",
    "        logging.info(f\"[{group_name}] Applying feature scaling (for SVM HPO)...\")\n",
    "        # Note: Only scale train/val needed for HPO. Don't save scaler yet.\n",
    "        X_train_scaled, X_val_scaled, _, _, _ = apply_feature_scaling(\n",
    "            X_train, X_val, pd.DataFrame(), pd.DataFrame(), # Pass empty DFs for test/cal\n",
    "            len(X_train) / len(X) if len(X) > 0 else 0,\n",
    "            len(X_val) / len(X) if len(X) > 0 else 0,\n",
    "            0, 0,\n",
    "            MODEL_DIR, save_scaler=False # Don't save scaler during HPO phase\n",
    "        )\n",
    "\n",
    "        # --- Run HPO for each model ---\n",
    "\n",
    "        # SVM HPO\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running SVM HPO ---\")\n",
    "            best_params_svm, best_score_hpo_svm = svm_hpo(\n",
    "                X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "                MAX_RESOURCE_SVM, MIN_RESOURCE_SVM, ETA_SVM, RESOURCE_TYPE_SVM,\n",
    "                RANDOM_SEED, SVC, loguniform, hyperband_hpo, f1_score, logging, model_name_svm\n",
    "            )\n",
    "            group_best_params[model_name_svm] = best_params_svm\n",
    "            logging.info(f\"[{group_name}] SVM Best Params: {best_params_svm}, Score: {best_score_hpo_svm:.4f}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] SVM HPO failed: {e}\", exc_info=False) # Set exc_info=True for full traceback\n",
    "            group_best_params[model_name_svm] = None # Indicate failure\n",
    "\n",
    "        # CART HPO\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running CART HPO ---\")\n",
    "            best_params_cart, best_score_hpo_cart = cart_hpo(\n",
    "                X_train, y_train, X_val, y_val,\n",
    "                MAX_RESOURCE_CART, MIN_RESOURCE_CART, ETA_CART, RESOURCE_TYPE_CART,\n",
    "                RANDOM_SEED, DecisionTreeClassifier, randint, f1_score, hyperband_hpo, logging, model_name_cart\n",
    "            )\n",
    "            group_best_params[model_name_cart] = best_params_cart\n",
    "            logging.info(f\"[{group_name}] CART Best Params: {best_params_cart}, Score: {best_score_hpo_cart:.4f}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] CART HPO failed: {e}\", exc_info=False)\n",
    "            group_best_params[model_name_cart] = None\n",
    "\n",
    "        # Random Forest HPO\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running Random Forest HPO ---\")\n",
    "            best_params_rf, best_score_hpo_rf = rf_hpo(\n",
    "                 X_train, y_train, X_val, y_val,\n",
    "                 MAX_RESOURCE_RF, MIN_RESOURCE_RF, ETA_RF, RESOURCE_TYPE_RF,\n",
    "                 RANDOM_SEED, RandomForestClassifier, randint, f1_score, hyperband_hpo, logging, model_name_rf\n",
    "            )\n",
    "            group_best_params[model_name_rf] = best_params_rf\n",
    "            logging.info(f\"[{group_name}] RF Best Params: {best_params_rf}, Score: {best_score_hpo_rf:.4f}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] Random Forest HPO failed: {e}\", exc_info=False)\n",
    "            group_best_params[model_name_rf] = None\n",
    "\n",
    "        # XGBoost HPO\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running XGBoost HPO ---\")\n",
    "            best_params_xgb, best_score_hpo_xgb = xgb_hpo(\n",
    "                X_train, y_train, X_val, y_val,\n",
    "                MAX_RESOURCE_XGB, MIN_RESOURCE_XGB, ETA_XGB, RESOURCE_TYPE_XGB,\n",
    "                RANDOM_SEED, xgb, loguniform, randint, uniform, f1_score, hyperband_hpo, logging, model_name_xgb\n",
    "            )\n",
    "            group_best_params[model_name_xgb] = best_params_xgb\n",
    "            logging.info(f\"[{group_name}] XGB Best Params: {best_params_xgb}, Score: {best_score_hpo_xgb:.4f}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] XGBoost HPO failed: {e}\", exc_info=False)\n",
    "            group_best_params[model_name_xgb] = None\n",
    "\n",
    "        # LightGBM HPO\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running LightGBM HPO ---\")\n",
    "            best_params_lgbm, best_score_hpo_lgbm = lgbm_hpo(\n",
    "                 X_train, y_train, X_val, y_val,\n",
    "                 MAX_RESOURCE_LGBM, MIN_RESOURCE_LGBM, ETA_LGBM, RESOURCE_TYPE_LGBM,\n",
    "                 RANDOM_SEED, lgb, loguniform, randint, uniform, f1_score, hyperband_hpo, logging, model_name_lgbm\n",
    "            )\n",
    "            group_best_params[model_name_lgbm] = best_params_lgbm\n",
    "            logging.info(f\"[{group_name}] LGBM Best Params: {best_params_lgbm}, Score: {best_score_hpo_lgbm:.4f}\")\n",
    "        except Exception as e:\n",
    "             logging.error(f\"[{group_name}] LightGBM HPO failed: {e}\", exc_info=False)\n",
    "             group_best_params[model_name_lgbm] = None\n",
    "\n",
    "        # --- Save Best Params for the Group ---\n",
    "        params_filepath = os.path.join(BEST_PARAMS_DIR, f\"{group_name}_best_params.json\")\n",
    "        try:\n",
    "            # Convert numpy types to standard Python types for JSON serialization\n",
    "            serializable_params = {}\n",
    "            for model, params in group_best_params.items():\n",
    "                if params is not None:\n",
    "                     serializable_params[model] = {k: (int(v) if isinstance(v, np.integer) else\n",
    "                                                      float(v) if isinstance(v, np.floating) else\n",
    "                                                      v)\n",
    "                                                 for k, v in params.items()}\n",
    "                else:\n",
    "                    serializable_params[model] = None # Keep None for failed HPO\n",
    "\n",
    "            with open(params_filepath, 'w') as f:\n",
    "                json.dump(serializable_params, f, indent=4)\n",
    "            logging.info(f\"[{group_name}] Successfully saved best parameters to {params_filepath}\")\n",
    "            all_best_params_found[group_name] = serializable_params # Store the saved params\n",
    "        except TypeError as e:\n",
    "             logging.error(f\"[{group_name}] Failed to serialize best parameters: {e}. Params: {group_best_params}\", exc_info=True)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] Failed to save best parameters to {params_filepath}: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "        group_hpo_duration = time.time() - group_hpo_start_time\n",
    "        logging.info(f\"--- Finished HPO for Group: {group_name} in {group_hpo_duration:.2f} seconds ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[{group_name}] Unhandled exception during HPO processing for group: {e}\", exc_info=True)\n",
    "        # Optionally mark this group as failed in some way if needed later\n",
    "\n",
    "# --- End of HPO Loop ---\n",
    "hpo_loop_duration = time.time() - hpo_loop_start_time\n",
    "logging.info(f\"\\n{'='*30} Finished HPO for all groups in {hpo_loop_duration:.2f} seconds {'='*30}\")\n",
    "print(f\"\\nHPO loop finished. Best parameters saved in '{BEST_PARAMS_DIR}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Feature Groups:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting feature set group 'group_1' with 11 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javym\\miniconda3\\envs\\Lab\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\javym\\miniconda3\\envs\\Lab\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\javym\\miniconda3\\envs\\Lab\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\javym\\miniconda3\\envs\\Lab\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "Processing Feature Groups:  50%|█████     | 1/2 [00:54<00:54, 54.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting feature set group 'group_2' with 25 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javym\\miniconda3\\envs\\Lab\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\javym\\miniconda3\\envs\\Lab\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\javym\\miniconda3\\envs\\Lab\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\javym\\miniconda3\\envs\\Lab\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "Processing Feature Groups: 100%|██████████| 2/2 [02:30<00:00, 75.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Workflow loop finished, but failed to save results to JSON.\n",
      "Results are available in the 'all_group_results' dictionary variable in this session.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# Workflow Loop - Run Models with Best Parameters and Evaluate #\n",
    "################################################################\n",
    "\n",
    "# --- Configuration ---\n",
    "BEST_PARAMS_DIR = \"best_params\" # Directory where HPO results are saved\n",
    "RESULTS_DIR = \"results\" # Directory to save final results JSON\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "RESULTS_FILENAME = os.path.join(RESULTS_DIR, f\"all_group_results_{datetime.now():%Y%m%d_%H%M%S}.json\") # Timestamped results file\n",
    "\n",
    "# List of feature groups to process (should match HPO loop or be a subset)\n",
    "GROUP_NAMES_TO_PROCESS = list(groups.keys()) # Process all defined groups\n",
    "GROUP_NAMES_TO_PROCESS = ['group_1', 'group_2'] # Example: Process only specific groups\n",
    "\n",
    "# Ensure necessary variables and functions are defined in the global scope:\n",
    "# groups, df, TARGET_COLUMN, clean_data, split_data, apply_feature_scaling,\n",
    "# CALIBRATOR, ALPHA, RANDOM_SEED, MODEL_DIR, ROUNDS (for XGB/LGBM),\n",
    "# svm_workflow, cart_workflow, random_forest_workflow, xgb_workflow, lgbm_workflow,\n",
    "# All model names (model_name_svm, etc.),\n",
    "# All HPO/Resource settings (MAX_RESOURCE_*, etc. - needed if best_params are missing),\n",
    "# All necessary modules/functions passed to workflows (SVC, DecisionTreeClassifier,\n",
    "# RandomForestClassifier, xgb, lgb, loguniform, randint, uniform, f1_score,\n",
    "# hyperband_hpo, EarlyStopping, early_stopping, train_calibrate_model,\n",
    "# fit_mondrian_classifier, evaluate_mondrian_prediction, calculate_metrics,\n",
    "# logging, np, datetime)\n",
    "\n",
    "# Dictionary to store results for all groups and models\n",
    "all_group_results = {}\n",
    "\n",
    "logging.info(f\"Starting main workflow loop for groups: {GROUP_NAMES_TO_PROCESS}\")\n",
    "outer_loop_start_time = time.time()\n",
    "\n",
    "# Use tqdm for the outer loop to show progress over groups\n",
    "for group_name in tqdm(GROUP_NAMES_TO_PROCESS, desc=\"Processing Feature Groups\"):\n",
    "    logging.info(f\"\\n{'='*30} Processing Feature Group: {group_name} {'='*30}\")\n",
    "    group_start_time = time.time()\n",
    "    group_results = {} # Store results for the current group\n",
    "    group_best_params = {} # To store loaded best params\n",
    "\n",
    "    # --- Load Best Params for the Group ---\n",
    "    params_filepath = os.path.join(BEST_PARAMS_DIR, f\"{group_name}_best_params.json\")\n",
    "    if os.path.exists(params_filepath):\n",
    "        try:\n",
    "            with open(params_filepath, 'r') as f:\n",
    "                group_best_params = json.load(f)\n",
    "            logging.info(f\"[{group_name}] Successfully loaded best parameters from {params_filepath}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] Failed to load best parameters from {params_filepath}: {e}. Proceeding without pre-tuned params.\", exc_info=True)\n",
    "            group_best_params = {} # Reset if loading fails\n",
    "    else:\n",
    "        logging.warning(f\"[{group_name}] Best parameters file not found: {params_filepath}. Workflows will run HPO internally.\")\n",
    "        group_best_params = {} # Ensure it's a dict\n",
    "\n",
    "    # Extract best params for each model, defaulting to None if not found/loaded\n",
    "    # The workflow functions are expected to handle None and run HPO if needed.\n",
    "    best_params_svm = group_best_params.get(model_name_svm, None)\n",
    "    best_params_cart = group_best_params.get(model_name_cart, None)\n",
    "    best_params_rf = group_best_params.get(model_name_rf, None)\n",
    "    best_params_xgb = group_best_params.get(model_name_xgb, None)\n",
    "    best_params_lgbm = group_best_params.get(model_name_lgbm, None)\n",
    "\n",
    "    try:\n",
    "        # --- 4.1 Data Preparation for the Current Group ---\n",
    "        logging.info(f\"[{group_name}] Cleaning data...\")\n",
    "        X, y, df_clean = clean_data(df, group_name, TARGET_COLUMN, logger=logging)\n",
    "\n",
    "        if X.empty or y.empty or df_clean.empty:\n",
    "            logging.warning(f\"[{group_name}] Skipping group due to insufficient data after cleaning (X: {X.shape}, y: {y.shape}).\")\n",
    "            all_group_results[group_name] = {'status': 'skipped_insufficient_data', 'results': group_results}\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"[{group_name}] Splitting data (Train/Val/Test/Cal)...\")\n",
    "        # Split into all necessary sets for training, validation (optional HPO and early stopping), calibration, and testing\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test, X_cal, y_cal = split_data(X, y)\n",
    "\n",
    "        if X_train.empty or y_train.empty or X_test.empty or y_test.empty:\n",
    "             logging.warning(f\"[{group_name}] Skipping group due to empty train or test set after splitting.\")\n",
    "             all_group_results[group_name] = {'status': 'skipped_empty_splits', 'results': group_results}\n",
    "             continue\n",
    "\n",
    "        # Check if calibration/validation sets are needed and non-empty\n",
    "        # Note: HPO might run internally if best_params were not loaded, requiring X_val/y_val\n",
    "        hpo_might_run = any(p is None for p in [best_params_svm, best_params_cart, best_params_rf, best_params_xgb, best_params_lgbm])\n",
    "        if (CALIBRATOR != 'none' and (X_cal.empty or y_cal.empty)):\n",
    "             logging.warning(f\"[{group_name}] Calibration set is empty, but CALIBRATOR is '{CALIBRATOR}'. Calibration will likely fail or be skipped.\")\n",
    "             # Decide whether to `continue` here based on strictness\n",
    "        if (hpo_might_run and (X_val.empty or y_val.empty)):\n",
    "             logging.warning(f\"[{group_name}] Validation set is empty, and HPO might run internally (missing best params). HPO will likely fail.\")\n",
    "             # Decide whether to `continue` here\n",
    "\n",
    "        logging.info(f\"[{group_name}] Applying feature scaling (for SVM)...\")\n",
    "        # Scale all necessary splits and save the scaler this time\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled, X_cal_scaled, scaler = apply_feature_scaling(\n",
    "            X_train, X_val, X_test, X_cal,\n",
    "            len(X_train) / len(X) if len(X) > 0 else 0, # Calculate actual proportions for logging inside the function\n",
    "            len(X_val) / len(X) if len(X) > 0 else 0,\n",
    "            len(X_test) / len(X) if len(X) > 0 else 0,\n",
    "            len(X_cal) / len(X) if len(X) > 0 else 0,\n",
    "            MODEL_DIR, # Pass MODEL_DIR for saving the scaler\n",
    "            save_scaler=True, # Ensure scaler is saved\n",
    "            group_name=group_name # Pass group name for potentially unique scaler filename\n",
    "        )\n",
    "\n",
    "        # --- 4.2 Run Workflows for the Current Group ---\n",
    "\n",
    "        # Workflow 1: SVM\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running SVM Workflow ---\")\n",
    "            svm_results = svm_workflow(\n",
    "                X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "                X_cal_scaled, y_cal, X_test_scaled, y_test,\n",
    "                MAX_RESOURCE_SVM, MIN_RESOURCE_SVM, ETA_SVM, RESOURCE_TYPE_SVM,\n",
    "                model_name_svm, CALIBRATOR, ALPHA, RANDOM_SEED,\n",
    "                SVC=SVC, loguniform=loguniform, hyperband_hpo=hyperband_hpo, # Pass dependencies\n",
    "                f1_score=f1_score, logging=logging, # Pass dependencies\n",
    "                best_params_svm=best_params_svm, # Pass loaded/None params\n",
    "                best_score_hpo_svm=None # HPO score not loaded, workflow calculates if needed\n",
    "                \n",
    "            )\n",
    "            group_results.update(svm_results if svm_results else {model_name_svm: {\"status\": \"failed\"}})\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] SVM Workflow failed: {e}\", exc_info=True)\n",
    "            group_results[model_name_svm] = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "        # Workflow 2: CART\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running CART Workflow ---\")\n",
    "            cart_results = cart_workflow(\n",
    "                X_train, y_train, X_val, y_val, X_cal, y_cal, X_test, y_test,\n",
    "                model_name_cart,\n",
    "                MAX_RESOURCE_CART, MIN_RESOURCE_CART, ETA_CART, RESOURCE_TYPE_CART,\n",
    "                RANDOM_SEED, CALIBRATOR, ALPHA,\n",
    "                DecisionTreeClassifier=DecisionTreeClassifier, randint=randint, # Pass dependencies\n",
    "                f1_score=f1_score, hyperband_hpo=hyperband_hpo, logging=logging, # Pass dependencies\n",
    "                best_params_cart=best_params_cart, # Pass loaded/None params\n",
    "                best_score_hpo_cart=None # HPO score not loaded\n",
    "                \n",
    "            )\n",
    "            group_results.update(cart_results if cart_results else {model_name_cart: {\"status\": \"failed\"}})\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] CART Workflow failed: {e}\", exc_info=True)\n",
    "            group_results[model_name_cart] = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "        # Workflow 3: Random Forest\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running Random Forest Workflow ---\")\n",
    "            # Ensure arguments match the function definition (file_context_9)\n",
    "            rf_results = random_forest_workflow(\n",
    "                X_train, y_train, X_val, y_val, X_cal, y_cal, X_test, y_test,\n",
    "                model_name_rf, # model_name first\n",
    "                MAX_RESOURCE_RF, MIN_RESOURCE_RF, ETA_RF, RESOURCE_TYPE_RF, # HPO settings\n",
    "                CALIBRATOR, ALPHA, RANDOM_SEED,\n",
    "                RandomForestClassifier=RandomForestClassifier, randint=randint, f1_score=f1_score, # Dependencies\n",
    "                hyperband_hpo=hyperband_hpo, train_calibrate_model=train_calibrate_model, # Dependencies\n",
    "                fit_mondrian_classifier=fit_mondrian_classifier, evaluate_mondrian_prediction=evaluate_mondrian_prediction, # Dependencies\n",
    "                calculate_metrics=calculate_metrics, logging=logging, np=np, datetime=datetime, # Dependencies\n",
    "                best_params_rf=best_params_rf, # Pass loaded/None params\n",
    "                best_score_hpo_rf=None # HPO score not loaded\n",
    "                \n",
    "            )\n",
    "            group_results.update(rf_results if rf_results else {model_name_rf: {\"status\": \"failed\"}})\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] Random Forest Workflow failed: {e}\", exc_info=True)\n",
    "            group_results[model_name_rf] = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "\n",
    "        # Workflow 4: XGBoost\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running XGBoost Workflow ---\")\n",
    "            # Ensure arguments match the function definition (file_context_4)\n",
    "            xgb_results = xgb_workflow(\n",
    "                X_train, y_train, X_val, y_val, X_cal, y_cal, X_test, y_test,\n",
    "                {}, # Pass empty dict for all_results initially, workflow should manage its own return\n",
    "                model_name_xgb,\n",
    "                MAX_RESOURCE_XGB, MIN_RESOURCE_XGB, ETA_XGB, RESOURCE_TYPE_XGB, ROUNDS,\n",
    "                CALIBRATOR, ALPHA, RANDOM_SEED,\n",
    "                loguniform=loguniform, randint=randint, uniform=uniform, xgb=xgb, # Dependencies\n",
    "                hyperband_hpo=hyperband_hpo, f1_score=f1_score, EarlyStopping=EarlyStopping, # Dependencies\n",
    "                train_calibrate_model=train_calibrate_model, fit_mondrian_classifier=fit_mondrian_classifier, # Dependencies\n",
    "                evaluate_mondrian_prediction=evaluate_mondrian_prediction, calculate_metrics=calculate_metrics, # Dependencies\n",
    "                logging=logging, np=np, # Dependencies\n",
    "                best_params_xgb=best_params_xgb, # Pass loaded/None params\n",
    "                best_score_hpo_xgb=None # HPO score not loaded\n",
    "                \n",
    "            )\n",
    "            group_results.update(xgb_results if xgb_results else {model_name_xgb: {\"status\": \"failed\"}})\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{group_name}] XGBoost Workflow failed: {e}\", exc_info=True)\n",
    "            group_results[model_name_xgb] = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "\n",
    "        # Workflow 5: LightGBM\n",
    "        try:\n",
    "            logging.info(f\"--- [{group_name}] Running LightGBM Workflow ---\")\n",
    "            # Ensure arguments match the function definition (file_context_2)\n",
    "            lgbm_results = lgbm_workflow(\n",
    "                 X_train, y_train, X_val, y_val, X_cal, y_cal, X_test, y_test,\n",
    "                 model_name_lgbm,\n",
    "                 MAX_RESOURCE_LGBM, MIN_RESOURCE_LGBM, ETA_LGBM, RESOURCE_TYPE_LGBM, ROUNDS,\n",
    "                 CALIBRATOR, ALPHA, RANDOM_SEED,\n",
    "                 lgb=lgb, loguniform=loguniform, randint=randint, uniform=uniform, f1_score=f1_score, # Dependencies\n",
    "                 hyperband_hpo=hyperband_hpo, early_stopping=early_stopping, train_calibrate_model=train_calibrate_model, # Dependencies\n",
    "                 fit_mondrian_classifier=fit_mondrian_classifier, evaluate_mondrian_prediction=evaluate_mondrian_prediction, # Dependencies\n",
    "                 calculate_metrics=calculate_metrics, logging=logging, np=np, datetime=datetime, # Dependencies\n",
    "                 best_params_lgbm=best_params_lgbm, # Pass loaded/None params\n",
    "                 best_score_hpo_lgbm=None # HPO score not loaded\n",
    "                 \n",
    "            )\n",
    "            group_results.update(lgbm_results if lgbm_results else {model_name_lgbm: {\"status\": \"failed\"}})\n",
    "        except Exception as e:\n",
    "             logging.error(f\"[{group_name}] LightGBM Workflow failed: {e}\", exc_info=True)\n",
    "             group_results[model_name_lgbm] = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "\n",
    "        # --- 4.3 Store Results for the Group ---\n",
    "        all_group_results[group_name] = {'status': 'completed', 'results': group_results}\n",
    "        group_duration = time.time() - group_start_time\n",
    "        logging.info(f\"--- Finished processing Feature Group: {group_name} in {group_duration:.2f} seconds ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[{group_name}] Unhandled exception during workflow processing for group: {e}\", exc_info=True)\n",
    "        all_group_results[group_name] = {'status': 'failed_outer', 'error': str(e), 'results': group_results}\n",
    "        # Optionally `continue` or `break` depending on desired behavior\n",
    "\n",
    "# --- End of Workflow Loop ---\n",
    "outer_loop_duration = time.time() - outer_loop_start_time\n",
    "logging.info(f\"\\n{'='*30} Finished processing all groups in {outer_loop_duration:.2f} seconds {'='*30}\")\n",
    "\n",
    "# --- Serialize Final Results ---\n",
    "try:\n",
    "    # Define a helper function to convert non-standard dictionary keys recursively\n",
    "    def convert_keys_to_standard_types(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            new_dict = {}\n",
    "            for k, v in obj.items():\n",
    "                new_key = k\n",
    "                # Convert numpy integer keys to standard Python int\n",
    "                if isinstance(k, np.integer):\n",
    "                    new_key = int(k)\n",
    "                # Add conversions for other non-standard key types if needed\n",
    "                # elif isinstance(k, np.floating): new_key = float(k)\n",
    "                # elif not isinstance(k, (str, int, float, bool, type(None))): new_key = str(k)\n",
    "                new_dict[new_key] = convert_keys_to_standard_types(v) # Recurse on value\n",
    "            return new_dict\n",
    "        elif isinstance(obj, list):\n",
    "            # Recursively process items in lists\n",
    "            return [convert_keys_to_standard_types(item) for item in obj]\n",
    "        else:\n",
    "            # Return non-dict/list items as is\n",
    "            return obj\n",
    "\n",
    "    # Define a helper function to make result VALUES JSON serializable\n",
    "    def default_serializer(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            # Convert numpy integers to Python int\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            # Convert numpy floats to Python float, handle NaN/Inf\n",
    "            if np.isnan(obj): return None # Represent NaN as null\n",
    "            if np.isinf(obj): return None # Represent Inf as null\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            # Convert numpy arrays to lists\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, (datetime, pd.Timestamp)):\n",
    "             # Convert datetime/timestamp objects to ISO format string\n",
    "             return obj.isoformat()\n",
    "        elif isinstance(obj, pd.DataFrame):\n",
    "            # Serialize DataFrames (example: to dict with 'split' orientation)\n",
    "            try:\n",
    "                return obj.to_dict(orient='split')\n",
    "            except Exception:\n",
    "                return f\"DataFrame (shape {obj.shape}) - Not serialized\"\n",
    "        elif isinstance(obj, Exception): # Serialize exception objects to string\n",
    "            return f\"Error: {str(obj)}\"\n",
    "        # Fallback for other types: try converting to string\n",
    "        try:\n",
    "            return str(obj)\n",
    "        except Exception:\n",
    "            # If string conversion fails, represent as unserializable type\n",
    "            return f\"Unserializable type: {type(obj)}\"\n",
    "\n",
    "    # Convert keys in the results dictionary BEFORE dumping to JSON\n",
    "    serializable_results = convert_keys_to_standard_types(all_group_results)\n",
    "\n",
    "    # Dump the processed dictionary to JSON\n",
    "    with open(RESULTS_FILENAME, 'w') as f:\n",
    "        # Use the default_serializer for values; keys are now standard types\n",
    "        json.dump(serializable_results, f, indent=4, default=default_serializer)\n",
    "\n",
    "    logging.info(f\"Successfully saved all group results to {RESULTS_FILENAME}\")\n",
    "    print(f\"\\nWorkflow loop finished. Results saved to '{RESULTS_FILENAME}'.\")\n",
    "except Exception as e:\n",
    "    # Log the error and inform the user\n",
    "    logging.error(f\"Failed to serialize final results to {RESULTS_FILENAME}: {e}\", exc_info=True)\n",
    "    print(\"\\nWorkflow loop finished, but failed to save results to JSON.\")\n",
    "    # Ensure the in-memory variable name matches what's used later if needed\n",
    "    print(\"Results might be available in the 'all_group_results' dictionary variable in this session (keys might not be standard types).\")\n",
    "    print(\"Processed, serializable results might be available in 'serializable_results'.\")\n",
    "\n",
    "\n",
    "# The 'all_group_results' dictionary (and the saved JSON file) now holds the results.\n",
    "# The next cell can load the JSON file or use the dictionary directly to create summary tables or plots.\n",
    "# Example: Access results for SVM in group_1\n",
    "# loaded_results = {}\n",
    "# try:\n",
    "#     with open(RESULTS_FILENAME, 'r') as f:\n",
    "#         loaded_results = json.load(f)\n",
    "#     print(loaded_results.get('group_1', {}).get('results', {}).get(model_name_svm))\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading results file: {e}\")\n",
    "#     print(\"Using in-memory results:\")\n",
    "#     print(all_group_results.get('group_1', {}).get('results', {}).get(model_name_svm))\n",
    "\n",
    "# The next cell should contain the code to parse 'all_group_results' (or loaded results)\n",
    "# and display the final summary DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Workflow loop finished. Results saved to 'results\\all_group_results_20250503_001415.json'.\n"
     ]
    }
   ],
   "source": [
    "# --- Serialize Final Results ---\n",
    "try:\n",
    "    # Define a helper function to convert non-standard dictionary keys recursively\n",
    "    def convert_keys_to_standard_types(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            new_dict = {}\n",
    "            for k, v in obj.items():\n",
    "                new_key = k\n",
    "                # Convert numpy integer keys to standard Python int\n",
    "                if isinstance(k, np.integer):\n",
    "                    new_key = int(k)\n",
    "                # Add conversions for other non-standard key types if needed\n",
    "                # elif isinstance(k, np.floating): new_key = float(k)\n",
    "                # elif not isinstance(k, (str, int, float, bool, type(None))): new_key = str(k)\n",
    "                new_dict[new_key] = convert_keys_to_standard_types(v) # Recurse on value\n",
    "            return new_dict\n",
    "        elif isinstance(obj, list):\n",
    "            # Recursively process items in lists\n",
    "            return [convert_keys_to_standard_types(item) for item in obj]\n",
    "        else:\n",
    "            # Return non-dict/list items as is\n",
    "            return obj\n",
    "\n",
    "    # Define a helper function to make result VALUES JSON serializable\n",
    "    def default_serializer(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            # Convert numpy integers to Python int\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            # Convert numpy floats to Python float, handle NaN/Inf\n",
    "            if np.isnan(obj): return None # Represent NaN as null\n",
    "            if np.isinf(obj): return None # Represent Inf as null\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            # Convert numpy arrays to lists\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, (datetime, pd.Timestamp)):\n",
    "             # Convert datetime/timestamp objects to ISO format string\n",
    "             return obj.isoformat()\n",
    "        elif isinstance(obj, pd.DataFrame):\n",
    "            # Serialize DataFrames (example: to dict with 'split' orientation)\n",
    "            try:\n",
    "                return obj.to_dict(orient='split')\n",
    "            except Exception:\n",
    "                return f\"DataFrame (shape {obj.shape}) - Not serialized\"\n",
    "        elif isinstance(obj, Exception): # Serialize exception objects to string\n",
    "            return f\"Error: {str(obj)}\"\n",
    "        # Fallback for other types: try converting to string\n",
    "        try:\n",
    "            return str(obj)\n",
    "        except Exception:\n",
    "            # If string conversion fails, represent as unserializable type\n",
    "            return f\"Unserializable type: {type(obj)}\"\n",
    "\n",
    "    # Convert keys in the results dictionary BEFORE dumping to JSON\n",
    "    serializable_results = convert_keys_to_standard_types(all_group_results)\n",
    "\n",
    "    # Dump the processed dictionary to JSON\n",
    "    with open(RESULTS_FILENAME, 'w') as f:\n",
    "        # Use the default_serializer for values; keys are now standard types\n",
    "        json.dump(serializable_results, f, indent=4, default=default_serializer)\n",
    "\n",
    "    logging.info(f\"Successfully saved all group results to {RESULTS_FILENAME}\")\n",
    "    print(f\"\\nWorkflow loop finished. Results saved to '{RESULTS_FILENAME}'.\")\n",
    "except Exception as e:\n",
    "    # Log the error and inform the user\n",
    "    logging.error(f\"Failed to serialize final results to {RESULTS_FILENAME}: {e}\", exc_info=True)\n",
    "    print(\"\\nWorkflow loop finished, but failed to save results to JSON.\")\n",
    "    # Ensure the in-memory variable name matches what's used later if needed\n",
    "    print(\"Results might be available in the 'all_group_results' dictionary variable in this session (keys might not be standard types).\")\n",
    "    print(\"Processed, serializable results might be available in 'serializable_results'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Performance Metrics Summary =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall_tpr</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>specificity_tnr</th>\n",
       "      <th>g_mean</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>brier_score</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>CP Coverage (Mondrian)</th>\n",
       "      <th>CP Avg Set Size (Mondrian)</th>\n",
       "      <th>CP Coverage (Mondrian, class 0)</th>\n",
       "      <th>CP Coverage (Mondrian, class 1)</th>\n",
       "      <th>HPO F1</th>\n",
       "      <th>HPO Duration (s)</th>\n",
       "      <th>Final Estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.9644</td>\n",
       "      <td>0.8626</td>\n",
       "      <td>0.5870</td>\n",
       "      <td>0.6986</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.7635</td>\n",
       "      <td>0.9365</td>\n",
       "      <td>0.7622</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>5057</td>\n",
       "      <td>36</td>\n",
       "      <td>159</td>\n",
       "      <td>226</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.5852</td>\n",
       "      <td>6.4635</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CART</td>\n",
       "      <td>0.9560</td>\n",
       "      <td>0.9737</td>\n",
       "      <td>0.3844</td>\n",
       "      <td>0.5512</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.6198</td>\n",
       "      <td>0.7106</td>\n",
       "      <td>0.5881</td>\n",
       "      <td>0.0654</td>\n",
       "      <td>5089</td>\n",
       "      <td>4</td>\n",
       "      <td>237</td>\n",
       "      <td>148</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4768</td>\n",
       "      <td>0.9327</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9438</td>\n",
       "      <td>0.6545</td>\n",
       "      <td>0.7730</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>0.8078</td>\n",
       "      <td>0.9483</td>\n",
       "      <td>0.8201</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>5078</td>\n",
       "      <td>15</td>\n",
       "      <td>133</td>\n",
       "      <td>252</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>33.8943</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.9708</td>\n",
       "      <td>0.9518</td>\n",
       "      <td>0.6156</td>\n",
       "      <td>0.7476</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.9496</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>5081</td>\n",
       "      <td>12</td>\n",
       "      <td>148</td>\n",
       "      <td>237</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.5753</td>\n",
       "      <td>0.4922</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.9717</td>\n",
       "      <td>0.9389</td>\n",
       "      <td>0.6390</td>\n",
       "      <td>0.7604</td>\n",
       "      <td>0.9969</td>\n",
       "      <td>0.7981</td>\n",
       "      <td>0.9525</td>\n",
       "      <td>0.8157</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>5077</td>\n",
       "      <td>16</td>\n",
       "      <td>139</td>\n",
       "      <td>246</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7034</td>\n",
       "      <td>0.1797</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  accuracy  precision  recall_tpr  f1_score  specificity_tnr  \\\n",
       "0            SVM    0.9644     0.8626      0.5870    0.6986           0.9929   \n",
       "1           CART    0.9560     0.9737      0.3844    0.5512           0.9992   \n",
       "2  Random_Forest    0.9730     0.9438      0.6545    0.7730           0.9971   \n",
       "3        XGBoost    0.9708     0.9518      0.6156    0.7476           0.9976   \n",
       "4       LightGBM    0.9717     0.9389      0.6390    0.7604           0.9969   \n",
       "\n",
       "   g_mean  roc_auc  pr_auc  brier_score    TN  FP   FN   TP  \\\n",
       "0  0.7635   0.9365  0.7622       0.0305  5057  36  159  226   \n",
       "1  0.6198   0.7106  0.5881       0.0654  5089   4  237  148   \n",
       "2  0.8078   0.9483  0.8201       0.0245  5078  15  133  252   \n",
       "3  0.7837   0.9496  0.7952       0.0287  5081  12  148  237   \n",
       "4  0.7981   0.9525  0.8157       0.0250  5077  16  139  246   \n",
       "\n",
       "  CP Coverage (Mondrian) CP Avg Set Size (Mondrian)  \\\n",
       "0                   None                       None   \n",
       "1                   None                       None   \n",
       "2                   None                       None   \n",
       "3                   None                       None   \n",
       "4                   None                       None   \n",
       "\n",
       "  CP Coverage (Mondrian, class 0) CP Coverage (Mondrian, class 1)  HPO F1  \\\n",
       "0                            None                            None  0.5852   \n",
       "1                            None                            None  0.4768   \n",
       "2                            None                            None  0.7362   \n",
       "3                            None                            None  0.5753   \n",
       "4                            None                            None  0.7034   \n",
       "\n",
       "   HPO Duration (s) Final Estimators  \n",
       "0            6.4635              N/A  \n",
       "1            0.9327              N/A  \n",
       "2           33.8943              N/A  \n",
       "3            0.4922               99  \n",
       "4            0.1797              100  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_summary = []\n",
    "for model_name, results_data in all_results.items():\n",
    "    summary = {'Model': model_name}\n",
    "    metrics = results_data.get('metrics')\n",
    "    if metrics:\n",
    "        summary.update(metrics)\n",
    "        cm = summary.pop('confusion_matrix', None)\n",
    "        if cm:\n",
    "            summary['TN'] = cm.get('tn')\n",
    "            summary['FP'] = cm.get('fp')\n",
    "            summary['FN'] = cm.get('fn')\n",
    "            summary['TP'] = cm.get('tp')\n",
    "\n",
    "    # --- Use the new Mondrian CP results ---\n",
    "    summary['CP Coverage (Mondrian)'] = results_data.get('cp_coverage_mond')\n",
    "    summary['CP Avg Set Size (Mondrian)'] = results_data.get('cp_avg_set_size_mond')\n",
    "\n",
    "    # --- Add per-class Mondrian CP coverage ---\n",
    "    class_coverage_dict = results_data.get('cp_class_coverage_dict') or {}\n",
    "    # Add columns for class 0 and class 1 coverage (use None if missing)\n",
    "    summary['CP Coverage (Mondrian, class 0)'] = class_coverage_dict.get(0, None)\n",
    "    summary['CP Coverage (Mondrian, class 1)'] = class_coverage_dict.get(1, None)\n",
    "\n",
    "    summary['HPO F1'] = results_data.get('hpo_f1_score')\n",
    "    summary['HPO Duration (s)'] = results_data.get('hpo_duration_s')\n",
    "    summary['Final Estimators'] = results_data.get('final_n_estimators', 'N/A') # Keep if relevant (not for SVM)\n",
    "    results_summary.append(summary)\n",
    "\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "\n",
    "# Set display options for float formatting\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}' if isinstance(x, float) else x)\n",
    "\n",
    "print(\"\\n===== Performance Metrics Summary =====\")\n",
    "from IPython.display import display # Make sure display is imported\n",
    "if not results_df.empty:\n",
    "    # Ensure all columns and all rows are displayed\n",
    "    with pd.option_context('display.max_columns', None, 'display.max_rows', None):\n",
    "        display(results_df)\n",
    "else:\n",
    "    print(\"No results to display.\")\n",
    "\n",
    "# --- Update CSV saving ---\n",
    "results_csv_path = os.path.join(MODEL_DIR, f\"model_comparison_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "try:\n",
    "    results_df.to_csv(results_csv_path, index=False)\n",
    "    logging.info(f\"Results summary DataFrame saved to {results_csv_path}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to save results summary CSV: {e}\")\n",
    "\n",
    "# --- Optional Plotting (Update or Remove) ---\n",
    "# The existing plotting code for 'CP Empty %' and 'CP Multi-Class %'\n",
    "# might need to be removed or adapted if you don't calculate these stats\n",
    "# with the new mondrian_icp function.\n",
    "# For now, let's comment it out as crepes doesn't directly return these counts easily.\n",
    "\n",
    "# if not results_df.empty and 'CP Empty Sets' in results_df.columns and 'CP Multi-Class Sets' in results_df.columns:\n",
    "#    ... (keep the existing plot code commented out or remove it) ...\n",
    "# else:\n",
    "#    logging.warning(\"Could not plot CP set types: Results DataFrame is empty or missing required columns.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
